[{"content":"In this blog post I wanted to talk about how I became a maintainer for the community.beszel Ansible Collection and my learnings.\nWhat is Beszel? Beszel is a simple, lightweight server monitoring software which can be self-hosted on your own infrastructure. I first heard about Beszel whilst reading the /r/selfhosted subreddit about a year ago, and a couple of months ago I had time to deploy it in my Home-Ops project.\nBeszel consists of two components: the hub and the agent. The hub provides a web interface for visualising the data sent from agents which run on your servers. My Beszel Hub is currently receiving data from 10 systems in my Homelab:\nIf you want to read more about my setup I\u0026rsquo;ve written a little about it here.\nI thought to myself, I want an automated way of deploying and managing the lifecycle of the Beszel agent on my Homelab devices, so I looked to Ansible to help me with this.\nHumble Beginnings - dbrennand.beszel Ansible Role I couldn\u0026rsquo;t find anything developed on GitHub for automated Beszel Agent deployment via Ansible, so I created an Ansible Role dbrennand.beszel on GitHub and published it to Ansible Galaxy. This standalone role worked really well and I knew the role could be useful for others, and boy was I right! \u0026#x1f601;\nIt wasn\u0026rsquo;t long before pull requests were coming in to implement features outside the original agent deployment scope of the role. It was clear to me that we were going into Ansible Collection territory. If you\u0026rsquo;re new to Ansible and want to learn more about collections then check out my previous blog post where I discuss them.\nBirth of community.beszel Initially I reached out to the Beszel maintainer to propose the idea of an Ansible Collection, but after some more research I found that there is a process for requesting Ansible community collections and this seemed like a great fit. The Beszel maintainer thought so too, so I requested the new collection be created via the Ansible forum.\nThe initial 0.1.0 release of community.beszel implemented the same Ansible Role as dbrennand.beszel but fast forward to today, we\u0026rsquo;re on version 0.3.0 and have Ansible Roles for deploying Beszel Hub on baremetal and Ansible Modules for managing Beszel systems using the Beszel Hub REST API. There is a lot more functionality we can add to community.beszel and the collection has started to receive contributions from the community which is awesome! \u0026#x1f603;\nLearnings from becoming an Ansible Collection Maintainer Becoming a maintainer for community.beszel has taught me a lot about creating and testing Ansible Modules. I learned how to use ansible-test to create and run integration, sanity and unit tests. I documented these steps in the collection\u0026rsquo;s CONTRUBUTING.md file. Furthermore, I recently gave some feedback on the Ansible forum about my new maintainer journey, things that I found really helpful and opportunities for improvement in documentation and tooling.\nConclusion Overall I\u0026rsquo;ve found it very satisfying seeing how my little Ansible role has turned into something bigger, and others are getting benefit from it too. I\u0026rsquo;ve enjoyed getting more involved in the Ansible community and I\u0026rsquo;ve learned a lot about maintaining and testing content in an Ansible collection. I\u0026rsquo;m sure there is much more still to learn too!\nIf you\u0026rsquo;ve made it this far then thanks for reading and I hope you have a great day!\n","permalink":"https://dbren.uk/blog/ansible-collections-community-beszel/","summary":"\u003cp\u003eIn this blog post I wanted to talk about how I became a maintainer for the \u003ca href=\"https://github.com/ansible-collections/community.beszel\"\u003e\u003ccode\u003ecommunity.beszel\u003c/code\u003e\u003c/a\u003e Ansible Collection and my learnings.\u003c/p\u003e\n\u003ch1 id=\"what-is-beszel\"\u003eWhat is Beszel?\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://beszel.dev/\"\u003eBeszel\u003c/a\u003e is a simple, lightweight server monitoring software which can be self-hosted on your own infrastructure. I first heard about Beszel whilst reading the \u003ca href=\"https://www.reddit.com/r/selfhosted/comments/1eb4bi5/i_just_released_beszel_a_server_monitoring_hub/\"\u003e\u003ccode\u003e/r/selfhosted\u003c/code\u003e\u003c/a\u003e subreddit about a year ago, and a couple of months ago I had time to deploy it in my \u003ca href=\"https://github.com/dbrennand/home-ops/tree/main/docker/beszel\"\u003eHome-Ops\u003c/a\u003e project.\u003c/p\u003e\n\u003cp\u003eBeszel consists of two components: the hub and the agent. The hub provides a web interface for visualising the data sent from agents which run on your servers. My Beszel Hub is currently receiving data from 10 systems in my Homelab:\u003c/p\u003e","title":"Becoming an Ansible Collection Maintainer: community.beszel"},{"content":"It was another great Ansible meetup in London on the 5th June 2025. It was hosted at Metro Bank\u0026rsquo;s offices near Holburn and the attendance was excellent!\nI gave a talk titled Ansible 101 - Getting Started on your Ansible Journey. The talk was aimed at members of the community who\u0026rsquo;re new to Ansible and just starting their Ansible journey. We covered key terminology and concepts such as: Inventories, Playbooks, Modules and Plugins, Roles, Collections and Ansible Vault.\nThe slide deck and the Ansible content I created for the demo are available on GitHub.\nThank you to everyone who asked questions in the Q\u0026amp;A and came up and gave their feedback after the talk \u0026#x1f642;. The community is amazing, everyone is so friendly and welcoming. I encourage anyone who is on the fence about submitting a talk to give it a go!\nFinally, it was a pleasure to present alongside James Freeman and a big thank you to Metro Bank and all the organisers who made the meetup possible - I\u0026rsquo;m looking forward to the next one! \u0026#x1f389;\n","permalink":"https://dbren.uk/blog/ansible-meetup-5-june/","summary":"\u003cp\u003eIt was another great \u003ca href=\"https://www.meetup.com/ansible-london/events/307305041/\"\u003eAnsible meetup\u003c/a\u003e in London on the 5th June 2025. It was hosted at Metro Bank\u0026rsquo;s offices near Holburn and the attendance was excellent!\u003c/p\u003e\n\u003cp\u003eI gave a talk titled \u003cem\u003eAnsible 101 - Getting Started on your Ansible Journey\u003c/em\u003e. The talk was aimed at members of the community who\u0026rsquo;re new to Ansible and just starting their Ansible journey. We covered key terminology and concepts such as: Inventories, Playbooks, Modules and Plugins, Roles, Collections and Ansible Vault.\u003c/p\u003e","title":"Ansible Meetup 5 June 2025"},{"content":"Hey there folks! 👋\nBack again with another blog post! This time I wanted to discuss some cool changes I\u0026rsquo;ve made in my Homelab to explore AWX and Ansible Execution Environments.\nWhat is AWX? AWX provides a web interface and REST API for managing and running Ansible content. AWX is the upstream project which Red Hat\u0026rsquo;s commerical offering Ansible Automation Platform\u0026rsquo;s Automation Controller is based on. AWX uses Execution Environments to run Ansible content and is deployed on Kubernetes using the awx-operator.\nWhat is an Execution Environment (EE)? An EE is an OCI compliant container image which contains Python, ansible-core, ansible-runner and other dependencies (Ansible content such as roles, collections and any dependent Python modules). Using EEs helps create a consistent environment every time we run our Ansible content.\nMotivations Before making these changes in my Homelab, I was aware of AWX and EEs but I hadn\u0026rsquo;t used either of them much, or built my own EE before. I\u0026rsquo;ve always wanted to use AWX and recent changes in my job have also motivated my use of AWX too.\nBuilding Execution Environments The Ansible development tooling ecosystem has a very handy tool for building EEs called ansible-builder. Ansible builder helps simplify the EE creation process by handling tasks such as including Python modules, Ansible roles and collections, and configuring a certain user and permissions; just to name a few.\nAnsible builder uses a definition file named execution-environment.yml to define the base container image, dependencies and additional build steps for the EE.\nMulti-Arch Execution Environment builds in GitHub Actions In my Home-Ops repository, I use ansible-builder in a GitHub Action Workflow to build my EE and upload it to the GitHub Container Registry. You can pull it now using the command:\ndocker pull ghcr.io/dbrennand/home-ops:latest The image is multi-arch supporting both linux/amd64 and linux/arm64 as I use my EE on devices with these CPU architectures. Getting the EE to build for multiple architectures inside a GitHub Action was very trial and error. I searched around online but couldn\u0026rsquo;t find anyone else building a multi-arch EE with GitHub Actions and ansible-builder.\nThe first error I encountered was:\nERROR: Multi-platform build is not supported for the docker driver. Switch to a different driver, or turn on the containerd image store, and try again. Learn more at https://docs.docker.com/go/build-multi-platform/ I got this error even though I had initialised the workflow with the docker/setup-qemu-action@v3 and docker/setup-buildx-action@v3 actions for multi-arch builds, which I\u0026rsquo;d done before without any issues. It turns out that I needed to tell ansible-builder to use the name of the builder context created by the docker/setup-buildx-action@v3 action. Furthermore, whilst testing there was another issue where the image built by ansible-builder wasn\u0026rsquo;t loaded into the Docker daemon.\nTo resolve both these issues I used the --extra-build-cli-args argument with ansible-builder like so:\nansible-builder ... --extra-build-cli-args \u0026#34;--load --builder ${{ steps.docker_buildx.outputs.name }} --platform linux/amd64,linux/arm64\u0026#34; Once I did the above, the next error I encountered was:\nERROR: docker exporter does not currently support exporting manifest lists From a recently active GitHub issue this wasn\u0026rsquo;t supported until very recently via an experimental containerd-snapshotter feature on the docker/setup-docker-action@v4 action; so I enabled it:\n- name: Set up Docker uses: docker/setup-docker-action@v4 with: daemon-config: | { \u0026#34;features\u0026#34;: { \u0026#34;containerd-snapshotter\u0026#34;: true } } Finally, I had multi-arch builds of my EE in GitHub Actions working! Woo! \u0026#x1f389; \u0026#x1f604;\nUsing the Execution Environment I\u0026rsquo;m using the EE on my Macbook with ansible-navigator and AWX. I\u0026rsquo;ve written a little more about this here.\nAs most of my playbooks use the community.general.onepassword lookup plugin to pull secrets from a 1Password vault, I\u0026rsquo;ve included the op CLI in my EE.\nTo start the EE and authenticate the op CLI I run the following commands in my Home-Ops repository:\ncd ansible export ONEPASSWORD_SERVICE_ACCOUNT_TOKEN=op://Vault/ServiceAccount/token op run -- ansible-navigator exec -- /bin/bash The 1Password application on my Macbook then prompts for my fingerprint via Touch ID to authorise retrieval of the Service Account token. Pretty \u0026#x1f192; \u0026#x1f60e;\nIn AWX I\u0026rsquo;ve registered the EE for use in Job Templates using an Ansible playbook. To authenticate the op CLI in the EE running on AWX, I also create a custom credential type which I then use in my Job Templates. I\u0026rsquo;ll probably make another blog post expanding on this a little more soon! So stay tuned for that one! \u0026#x1f642;\nThat\u0026rsquo;s all for this one folks! Until next time! \u0026#x1f44b;\n","permalink":"https://dbren.uk/blog/homeops-ansible-ee/","summary":"\u003cp\u003eHey there folks! 👋\u003c/p\u003e\n\u003cp\u003eBack again with another blog post! This time I wanted to discuss some cool changes I\u0026rsquo;ve made in my Homelab to explore AWX and Ansible Execution Environments.\u003c/p\u003e\n\u003ch2 id=\"what-is-awx\"\u003eWhat is AWX?\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/ansible/awx\"\u003eAWX\u003c/a\u003e provides a web interface and REST API for managing and running Ansible content. AWX is the upstream project which Red Hat\u0026rsquo;s commerical offering Ansible Automation Platform\u0026rsquo;s Automation Controller is based on. AWX uses Execution Environments to run Ansible content and is deployed on Kubernetes using the \u003ca href=\"https://github.com/ansible/awx-operator\"\u003eawx-operator\u003c/a\u003e.\u003c/p\u003e","title":"Home-Ops | AWX and Ansible Execution Environments"},{"content":"I had an awesome time at the Ansible Meetup in London on the 21st of November 2024. The meetup was attended by around 25 people and was hosted at the Dell Technologies offices in London. The meetup was sponsored by Red Hat and Krameff Solutions.\nI did my first ever talk at a meetup titled: Infrastructure as Code - VPS provisioning and configuration on Hetzner Cloud. Hetzner is a public cloud provider I\u0026rsquo;ve used for personal projects and in my Homelab. I\u0026rsquo;ve been really impressed with their service and the value for money they offer. As part of my talk, I did a live demo of deploying a Virtual Private Server (VPS) and configuring a NGINX web server on the VPS using Ansible. The talk went really well and there was a some great Q\u0026amp;A with the audience \u0026#x1f604;\nMy presentation and the Ansible content I wrote for the demo is available on GitHub.\nI really enjoyed giving the talk and I\u0026rsquo;m looking forward to doing more in the future. I\u0026rsquo;m also looking forward to attending more meetups as it was great to speak with others who are passionate about Ansible and automation \u0026#x1f604; The pizza and drinks were great too! \u0026#x1f355; \u0026#x1f37a;\nI\u0026rsquo;d like to say a big thank you to the organisers for making it happen and to everyone who attended. I\u0026rsquo;m looking forward to the next one! \u0026#x1f389;\n","permalink":"https://dbren.uk/blog/ansible-meetup-21-november/","summary":"\u003cp\u003eI had an awesome time at the Ansible Meetup in London on the 21st of November 2024. The meetup was attended by around 25 people and was hosted at the Dell Technologies offices in London. The meetup was sponsored by Red Hat and Krameff Solutions.\u003c/p\u003e\n\u003cp\u003eI did my first ever talk at a meetup titled: \u003cem\u003eInfrastructure as Code - VPS provisioning and configuration on Hetzner Cloud\u003c/em\u003e. Hetzner is a public cloud provider I\u0026rsquo;ve used for personal projects and in my Homelab. I\u0026rsquo;ve been really impressed with their service and the value for money they offer. As part of my talk, I did a live demo of deploying a Virtual Private Server (VPS) and configuring a NGINX web server on the VPS using Ansible. The talk went really well and there was a some great Q\u0026amp;A with the audience \u0026#x1f604;\u003c/p\u003e","title":"Ansible Meetup 21 November 2024"},{"content":"Right now, I\u0026rsquo;m writing this blog post sitting in a coffee shop, securely routing all my traffic through a device on my home network and have access to all my Homelab services and devices as if I was at home too. How? Tailscale! \u0026#x2764;\u0026#xfe0f;\nWhat is Tailscale? Tailscale is a secure and private, identity-based, infrastructure agnostic network with a flexible topology, resilient networking, and a streamlined setup.\n— Tailscale Inc1\nTailscale uses the Wireguard protocol under the hood, which is modern, fast and secure. Tailscale establishes lightweight encrypted tunnels between devices forming a peer-to-peer mesh network (known as a Tailnet).\nYou can sign up and login to Tailscale using several providers such as Google, GitHub, Microsoft and more. Once signed up, you can download Tailscale for a range of platforms including: Windows, MacOS, Linux, Android and iOS. It can run on desktops, laptops, servers, VMs, containers, and even your router!\nIf you want to learn more in depth about how Tailscale works, check out their documentation.\nWhy Tailscale? So why is Tailscale so great? Why not deploy a traditional VPN like OpenVPN or WireGuard?\nWell, I\u0026rsquo;ve deployed and used VPN servers like this in the past, one example which comes to mind is PiVPN. Personally, I don\u0026rsquo;t have the time to maintain a dedicated VPN server, manage configuration, users, keys etc. I just want something that\u0026rsquo;s low maintenance, easy to deploy and use. Tailscale ticks all these boxes ✅\nTailscale is a zero-config VPN, meaning you don\u0026rsquo;t need to worry about IP addresses, ports, or keys. It just works! I don\u0026rsquo;t need to open any ports on my firewall, and Tailscale provides fine grained access controls via ACLs, so only the devices that I want to talk to one another can do so. One other feature that I really like is MagicDNS, which allows you to resolve hostnames of devices on your Tailnet, so you don\u0026rsquo;t need to remember IP addresses! 🎉\nThey have a very generous personal (free) plan allowing up to 3 users and 100 devices. This is more than enough for me and most others.\nThe Coordination Server and Risk Mitigation One thing to note is that Tailscale relies on a central \u0026lsquo;coordination server\u0026rsquo; accessible at https://login.tailscale.com. This is how you sign in and manage your Tailnet. The coordination server is also responsible for exchanging Wireguard public keys, assigning IP addresses for devices, allowing machine sharing features and setting policies (ACLs) on your Tailnet. However, it\u0026rsquo;s important to stress that no traffic from your devices is routed through the coordination server.\nUnlike the Tailscale client which is open source on GitHub, the coordination server is not open source. Now for some people this might be a deal breaker, but there are some alternatives, such as Headscale which is an open source implementation of the coordination server.\nMoreover, part of my threat model was to minimise the impact of the Tailscale coordination server being compromised and malicious devices being added to my Tailnet. Luckily, Tailscale has a feature called Tailnet lock where new devices need to be signed by trusted nodes before they can join the Tailnet. If you\u0026rsquo;re using Tailscale I highly recommend enabling this feature.\nDeploying Tailscale Deploying Tailscale on devices is easy. With Tailscale lock enabled there is an extra step but it\u0026rsquo;s not difficult. Here\u0026rsquo;s how I deployed Tailscale on my devices.\nI use the artis3n Tailscale Ansible role. Below is an example playbook:\n--- - name: Tailscale | Install \u0026amp; Update hosts: tailscale roles: - role: artis3n.tailscale Role variables are managed in the inventory group_vars and host_vars because for some devices I override the defaults such as when deploying an exit node or subnet router. More on this later!\nYou can find the example inventory in my HomeOps repository. Once I\u0026rsquo;ve deployed Tailscale on the device, I login to the Tailscale admin console to get the tailscale lock sign command to sign the device using a trusted signing node. Alternatively, if you don\u0026rsquo;t use Ansible you can install Tailscale using the following command on Linux:\ncurl -fsSL https://tailscale.com/install.sh | sh You\u0026rsquo;ll then be directed to a URL to authenticate the device to your Tailnet. Easy as \u0026#x1f967;!\nSubnet Routers A subnet router is a device in your tailnet that you use as a gateway to advertise routes to other devices.\n— Tailscale Inc2\nIn my setup I have two subnet routers for resiliency. Both are running on Pi-hole devices. I advertise specific addresses on my Tailnet instead of the entire subnet. Furthermore, these Pi-hole devices are configured as DNS servers for my Tailnet so I can resolve internal DNS names from Tailscale and get Pi-hole ad-blocking too!\nExit Nodes The exit node feature lets you route all traffic through a specific device on your Tailscale network.\n— Tailscale Inc3\nAn exit node is the equivalent of using the default routes 0.0.0.0/0, ::/0 on a traditional VPN configuration. All public internet traffic is routed through the exit node, which is perfect for scenarios where you\u0026rsquo;re on an untrusted network (like the coffee shop Wi-Fi I\u0026rsquo;m using right now!) and want to route all traffic through a trusted device on your Tailnet. Furthermore, exit nodes are great for bypassing geo-restrictions, for example, if you\u0026rsquo;re travelling and want to watch content that\u0026rsquo;s only available in your home country, or avoid tripping security systems when accessing online banking whilst abroad.\nIn my setup I have two exit nodes running as unprivileged Proxmox LXC containers. There is a tiny piece of manual configuration needed for this.\nConclusion IMHO Tailscale is brilliant and it\u0026rsquo;s pretty mind-blowing how easy it is to set up and use. I\u0026rsquo;m honestly kind of shocked that I can use all these features for free too. Their free-tier is very generous and even if it did cost money in the future, I\u0026rsquo;d be happy to pay for it because it\u0026rsquo;s a great service.\nThat\u0026rsquo;s all for this one folks! Until next time! \u0026#x1f44b;\nWhy Tailscale?\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSubnet Routers\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nExit Nodes\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://dbren.uk/blog/tailscale/","summary":"\u003cp\u003eRight now, I\u0026rsquo;m writing this blog post sitting in a coffee shop, securely routing all my traffic through a device on my home network and have access to all my Homelab services and devices as if I was at home too. How? Tailscale! \u0026#x2764;\u0026#xfe0f;\u003c/p\u003e\n\u003ch2 id=\"what-is-tailscale\"\u003eWhat is Tailscale?\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003eTailscale is a secure and private, identity-based, infrastructure agnostic network with a flexible topology, resilient networking, and a streamlined setup.\u003c/p\u003e\n\u003cp\u003e— \u003c!-- raw HTML omitted --\u003eTailscale Inc\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e\u003c!-- raw HTML omitted --\u003e\u003c/p\u003e","title":"Remote access with Tailscale"},{"content":"Hello internet! \u0026#x1f44b; In this blog post I will cover how to get started with Visual Studio Code Dev Containers.\nWhat are Dev Containers and why use them? Dev Containers provide a containerised development environment within Visual Studio Code. With Dev Containers, you and your team can have a consistent development environment, regardless of the host OS. Moreover, it helps ease the onboarding process, getting new team members up and running quickly, and helps avoid common issues such as \u0026ldquo;it works on my machine\u0026rdquo; or \u0026ldquo;oh, you\u0026rsquo;ve got a different version of x\u0026rdquo;.\nSo without further ado, let\u0026rsquo;s get started! \u0026#x1f680;\nGetting Started To use Dev Containers, you will need the following:\nVisual Studio Code obviously \u0026#x1f609; Dev Containers extension A container runtime such as Docker, Podman or OrbStack (uses Docker engine under the hood - MacOS only). My container runtime of choice is OrbStack. I\u0026rsquo;ve been using it for a while now and it has been great! \u0026#x1f389; It\u0026rsquo;s fast and very lightweight. Podman is another great alternative, especially if you\u0026rsquo;re looking for a Docker alternative. Podman Desktop was released not long ago too, so definitely check it out if GUIs are more your thing \u0026#x1f642;\nCreating a Dev Container Dev Containers are configured using a devcontainer.json file which resides in a .devcontainer directory in your repository. This file defines the entire configuration for the containerised environment including the base image, extensions, settings and more.\nFor this blog post, I will create a Dev Container for developing Ansible content. We will need Ansible core, Python and the Ansible extension for Visual Studio Code.\nLet\u0026rsquo;s start by creating the needed directories for the Ansible project:\nmkdir -pv ansible-dev-containers-demo/.devcontainer Next, we will create the devcontainer.json file:\ntouch ansible-dev-containers-demo/.devcontainer/devcontainer.json Now, let\u0026rsquo;s add the following configuration to the devcontainer.json file:\n{ \u0026#34;name\u0026#34;: \u0026#34;Ansible Dev Containers Demo\u0026#34;, \u0026#34;build\u0026#34;: { \u0026#34;dockerfile\u0026#34;: \u0026#34;Dockerfile\u0026#34; }, \u0026#34;customizations\u0026#34;: { \u0026#34;vscode\u0026#34;: { \u0026#34;settings\u0026#34;: { \u0026#34;redhat.telemetry.enabled\u0026#34;: false }, \u0026#34;extensions\u0026#34;: [ \u0026#34;redhat.ansible\u0026#34; ] } } } Let\u0026rsquo;s break down the configuration. We will be building the base image using a simple Dockerfile. The Ansible extension will be installed in the Dev Container and Red Hat telemetry in the extension will be disabled.\nNext, we\u0026rsquo;ll create the Dockerfile with the following content:\ntouch ansible-dev-containers-demo/.devcontainer/Dockerfile FROM python:3.12-slim # Install ansible-core RUN pip install ansible-core Now, let\u0026rsquo;s open the project in Visual Studio Code and we will see a toast notification that a Dev Container configuration file has been detected:\nClick Reopen in Container to start building the Dev Container. Once built, open a terminal in Visual Studio Code, and you\u0026rsquo;ll see that we are running inside the Dev Container with Ansible and the extension installed:\nroot@b6c0de05320f:/workspaces/ansible-dev-containers-demo# ansible --version ansible [core 2.17.2] config file = None configured module search path = [\u0026#39;/root/.ansible/plugins/modules\u0026#39;, \u0026#39;/usr/share/ansible/plugins/modules\u0026#39;] ansible python module location = /usr/local/lib/python3.12/site-packages/ansible ansible collection location = /root/.ansible/collections:/usr/share/ansible/collections executable location = /usr/local/bin/ansible python version = 3.12.4 (main, Jul 2 2024, 20:57:30) [GCC 12.2.0] (/usr/local/bin/python) jinja version = 3.1.4 libyaml = True You now have a containerised environment for Ansible development! \u0026#x1f389;\nRebuilding the Dev Container If you need to rebuild the Dev Container, you can do so by opening the command palette (Ctrl|Command+Shift+P) and running the Dev Containers: Rebuild and Reopen in Container command.\nConclusion Dev Containers are a great way for you and your team to have a consistent development environment. We only scratched the surface of what\u0026rsquo;s possible with Dev Containers in this post. In the future, I\u0026rsquo;ll cover running commands or scripts pre and post startup, 1Password SSH integration, environment variables, port forwarding and more! \u0026#x1f642;\nIf you\u0026rsquo;d like to learn more about Dev Containers, check out the official documentation.\nUntil next time! \u0026#x1f44b;\n","permalink":"https://dbren.uk/blog/vscode-dev-containers/","summary":"\u003cp\u003eHello internet! \u0026#x1f44b; In this blog post I will cover how to get started with \u003ca href=\"https://code.visualstudio.com/docs/devcontainers/containers\"\u003eVisual Studio Code Dev Containers\u003c/a\u003e.\u003c/p\u003e\n\u003ch2 id=\"what-are-dev-containers-and-why-use-them\"\u003eWhat are Dev Containers and why use them?\u003c/h2\u003e\n\u003cp\u003eDev Containers provide a containerised development environment within Visual Studio Code. With Dev Containers, you and your team can have a consistent development environment, regardless of the host OS. Moreover, it helps ease the onboarding process, getting new team members up and running quickly, and helps avoid common issues such as \u0026ldquo;it works on my machine\u0026rdquo; or \u0026ldquo;oh, you\u0026rsquo;ve got a different version of x\u0026rdquo;.\u003c/p\u003e","title":"Getting started with Visual Studio Code Dev Containers"},{"content":"Towards the end of last year, I introduced Proxmox into my Homelab. I\u0026rsquo;m using it to host various services on VMs and LXC containers. In my setup, I have two external SSDs providing storage, these are connected to my Proxmox node via USB 3.0. However, recently I was experiencing issues with the SSDs suddenly disconnecting from the Proxmox node!\nThis was very frustrating because the VM filesystems that were on these SSDs were becoming corrupted! \u0026#x1f631; This issue was happening on Proxmox 8.1.3 and kernel version 6.5.11-7-pve.\nTroubleshooting On Proxmox, you can check the system logs via the GUI by navigating to the node, and under System is syslog. It\u0026rsquo;s also possible via the journalctl command on the Proxmox node itself.\nUpon inspecting the Proxmox node logs, I couldn\u0026rsquo;t see any indication as to why the disconnects were happening. I could see the disconnect event occurring but prior to that, everything was normal. Here\u0026rsquo;s an example of the kernel logs:\nFeb 09 22:11:04 proxmox01 kernel: usb 2-1: USB disconnect, device number 17 Feb 09 22:11:04 proxmox01 kernel: sd 4:0:0:0: [sdb] Synchronizing SCSI cache Feb 09 22:11:04 proxmox01 kernel: sd 4:0:0:0: [sdb] Synchronize Cache(10) failed: Result: hostbyte=DID_ERROR driverbyte=DRIVER_OK I checked the SMART status of the SSDs and they were healthy, I tried different USB ports on the Proxmox node, swapping the USB 3.0 to SATA cables, nothing seemed to help \u0026#x1f615;\nThe Fix After some research, I found that the issue may have been related to USB power saving settings. After a certain period of time, the OS places USB devices in a low-power state to save power, which was causing the USB drives to disconnect! \u0026#x1f92f;\nTo resolve the problem, with some help from our good friend ChatGPT, I permanently disabled the USB power saving settings by adding usbcore.autosuspend=-1 to the GRUB kernel boot options in /etc/default/grub:\nGRUB_CMDLINE_LINUX_DEFAULT=\u0026#34;quiet usbcore.autosuspend=-1\u0026#34; Then, applying the changes and rebooting the Proxmox node:\nupdate-grub reboot After making these changes, I haven\u0026rsquo;t experienced any further disconnects! \u0026#x1f389; 🥳\n","permalink":"https://dbren.uk/blog/proxmox-fix-usb-disconnect/","summary":"\u003cp\u003eTowards the end of last year, I introduced \u003ca href=\"https://www.proxmox.com/en/\"\u003eProxmox\u003c/a\u003e into my Homelab. I\u0026rsquo;m using it to host various services on VMs and LXC containers. In my setup, I have two external SSDs providing storage, these are connected to my Proxmox node via USB 3.0. However, recently I was experiencing issues with the SSDs suddenly disconnecting from the Proxmox node!\u003c/p\u003e\n\u003cp\u003eThis was very frustrating because the VM filesystems that were on these SSDs were becoming corrupted! \u0026#x1f631;\nThis issue was happening on Proxmox \u003ccode\u003e8.1.3\u003c/code\u003e and kernel version \u003ccode\u003e6.5.11-7-pve\u003c/code\u003e.\u003c/p\u003e","title":"Fixing Proxmox USB Disconnects"},{"content":"Why I\u0026rsquo;m Migrating from Google Workspace and Outlook to iCloud+ Google Workspace My Google Workspace subscription is becoming increasingly more expensive 💰. The current price is £8.28 for the Business Standard plan, and next month this will be increasing to £9.20. This has become too expensive for me, and I\u0026rsquo;ve been looking for a cheaper alternative.\nOutlook Mail I have an Outlook account which has been lying dormant for a long time now.\nCome on, don\u0026rsquo;t judge me! We all have that one email account that we don\u0026rsquo;t use anymore\u0026hellip; right?! 😅 🥵\nAnyways\u0026hellip; This account has lots of old emails that I\u0026rsquo;d like to keep for archival purposes. Whilst migrating my Google Workspace account to iCloud+, I\u0026rsquo;d like to migrate these old emails too.\niCloud+ I\u0026rsquo;m already in the Apple ecosystem, and I recently purchased iCloud+ for £0.99 per month. This provides 50GB of storage which is more than enough for my needs. I also noticed you can use your own domain for email (which was the main reason I was using Google Workspace), and you can add up to 5 custom domains with 3 email addresses each! So I\u0026rsquo;ve decided to migrate my emails to iCloud+ to save money and simplify my setup.\nI\u0026rsquo;d like to begin by thanking Myles Gray for his excellent blog post on his experience migrating from Google Workspace to iCloud+. I used this as a guide for my migration.\nMigrating Emails Similar to Myles, I used imapsync to migrate my emails to iCloud+. imapsync is a well established and battle tested tool for migrating emails using the IMAP protocol. Being actively maintained, free and open-source, with lots of documentation and examples, it seemed like the perfect tool for the job. I used the imapsync container image to migrate my emails. I\u0026rsquo;m now sponsoring the maintainer on GitHub too! :slight_smile:\nBefore getting started you must have done the following:\nCreated an iCloud Mail email address (@icloud.com). Completed steps 1 and 2 of adding your custom email domain to iCloud+. Generated an app password for your iCloud+ account. Generated an app password for your Google Workspace account. Generated an app password for your Outlook account. Enabled IMAP access in the Gmail settings. The app specific passwords are needed if your account has two-factor authentication enabled (It\u0026rsquo;s 2023 people! You should have this enabled! 😅)\nMigrating Google Workspace Emails to iCloud Mail Initially, I used the command below to migrate my emails from Google Workspace to iCloud Mail:\ndocker run gilleslamiral/imapsync imapsync --errorsmax 200 --user1 \u0026lt;Gmail Address\u0026gt; --password1 \u0026#34;\u0026lt;Gmail App Password\u0026gt;\u0026#34; --host2 imap.mail.me.com --ssl2 --user2 \u0026lt;iCloud Address\u0026gt; --password2 \u0026#34;\u0026lt;iCloud+ App Password\u0026gt;\u0026#34; --gmail1 However, once the sync finished I noticed that some of the emails were not in the right folders \u0026#x1f615; What I didn\u0026rsquo;t realise was that as well as my custom labels (which are just IMAP folders), Gmail also applies an INBOX label to all emails 🙄. The default behaviour of imapsync is:\nThe first label synced by imapsync goes to its corresponding folder but other labels are then ignored. By default imapsync syncs folders (Gmail labels) using the classical alphanumeric order.\nimapsync Gmail FAQ 1\nThis is why some of my emails were not in the right folders!\nSince I now had emails in the wrong folders, I had to delete them from iCloud Mail and start again 🫠:\ndocker run gilleslamiral/imapsync imapsync --errorsmax 200 --user1 \u0026lt;iCloud Address\u0026gt; --password1 \u0026#34;\u0026lt;iCloud+ App Password\u0026gt;\u0026#34; --host1 imap.mail.me.com -ssl1 --host2 imap.mail.me.com --ssl2 --user2 \u0026lt;iCloud Address\u0026gt; --password2 \u0026#34;\u0026lt;iCloud+ App Password\u0026gt;\u0026#34; --delete1 --noexpungeaftereach Now, I ran the sync again but this time using the --folderlast option to sync the INBOX label last:\ndocker run gilleslamiral/imapsync imapsync --errorsmax 200 --user1 \u0026lt;Gmail Address\u0026gt; --password1 \u0026#34;\u0026lt;Gmail App Password\u0026gt;\u0026#34; --host2 imap.mail.me.com --ssl2 --user2 \u0026lt;iCloud Address\u0026gt; --password2 \u0026#34;\u0026lt;iCloud+ App Password\u0026gt;\u0026#34; --gmail1 --folderlast \u0026#34;INBOX\u0026#34; --folderlast \u0026#34;[Gmail]/All Mail\u0026#34; In the end, I synced 1109 emails (390.7 MiB) which took 23 minutes and 51 seconds. This was the amount once imapsync skipped cross duplicates (emails labelled multiple times). I relied heavily on the --dry option to test the sync before running it for real.\nMigrating Outlook Emails to iCloud Mail Next, I migrated my Outlook emails to iCloud Mail. As the emails in this account were very old, I wanted to put them all into a single folder called Other/Outlook|Personal. I used the following command to do this:\ndocker run gilleslamiral/imapsync imapsync --errorsmax 200 --user1 \u0026lt;Outlook Address\u0026gt; --office1 --password1 \u0026#34;\u0026lt;Outlook App Password\u0026gt;\u0026#34; --host2 imap.mail.me.com --ssl2 --user2 \u0026lt;iCloud Address\u0026gt; --password2 \u0026#34;\u0026lt;iCloud+ App Password\u0026gt;\u0026#34; --regextrans2 \u0026#34;s/(.*)/Other\\/Outlook|Personal/\u0026#34; --folder \u0026#34;Inbox\u0026#34; This synced 422 emails which took only a couple of minutes to complete. This sync went a lot smoother than the last one! 😆 😅\nCorrecting DNS Records Now that I\u0026rsquo;d migrated my emails, I needed to update my DNS MX and TXT records to point to iCloud Mail. Returning to iCloud+ and on the Custom Email Domain setup, I completed step 3. I use Cloudflare for DNS and I was pleasantly surprised to find Apple provides integration with Cloudflare to automatically amend my DNS records for me! \u0026#x1f603; I just had to confirm the changes and wait for the changes to propagate.\nConclusion In the end, I\u0026rsquo;d successfully migrated 1531 emails from Google Workspace and Outlook to iCloud+, and I\u0026rsquo;m saving money! 💸 🎉\nReferences https://imapsync.lamiral.info/FAQ.d/FAQ.Gmail.txt\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://dbren.uk/blog/google-outlook-to-icloud+/","summary":"\u003ch1 id=\"why-im-migrating-from-google-workspace-and-outlook-to-icloud\"\u003eWhy I\u0026rsquo;m Migrating from Google Workspace and Outlook to iCloud+\u003c/h1\u003e\n\u003ch2 id=\"google-workspace\"\u003eGoogle Workspace\u003c/h2\u003e\n\u003cp\u003eMy Google Workspace subscription is becoming increasingly more expensive 💰. The current price is £8.28 for the Business Standard plan, and next month this will be increasing to £9.20. This has become too expensive for me, and I\u0026rsquo;ve been looking for a cheaper alternative.\u003c/p\u003e\n\u003ch2 id=\"outlook-mail\"\u003eOutlook Mail\u003c/h2\u003e\n\u003cp\u003eI have an Outlook account which has been lying dormant for a long time now.\u003c/p\u003e","title":"Migrating from Google Workspace and Outlook Mail to iCloud+"},{"content":"And we\u0026rsquo;re back! \u0026#x1f44b;\nIn this blog post we will be looking at building and publishing a container image to GitHub Packages using GitHub Actions!\nI recently revisited an old project of mine; now called speeder. It\u0026rsquo;s a Python script to monitor your internet speed and send the results to InfluxDB. The results can then be visualised in Grafana. I originally created this script during the Coronavirus lockdowns whilst internet usage was high and essential for work. Since that time, I\u0026rsquo;ve learned a lot about CI/CD and noticed I hadn\u0026rsquo;t automated the build and publish process of the container image! 😬 Time to fix that! 👷\nWhat is CI/CD? Before we get started, let\u0026rsquo;s take a quick look at what CI/CD is. According to ChatGPT:\nContinuous Integration (CI) is the practice of frequently and automatically integrating code changes from multiple developers, leading to early issue detection and smoother teamwork. Continuous Delivery (CD) extends CI by automating the deployment process, allowing for faster and more reliable software releases. The benefits of CI/CD include quicker development cycles, higher software quality, reduced errors, and more frequent and reliable software updates.\nThanks ChatGPT! \u0026#x1f916;\nIn short, CI/CD helps developers to automate the process of building, testing and deploying software. It reduces the time and effort required to complete these processes, and helps ensure software is built and tested consistently and reliably.\nBuild and Publish a Container Image using GitHub Actions Begin by creating the GitHub Actions workflow directory .github/workflows in your project:\nmkdir -pv .github/workflows Next, create a .github/workflows/build.yml file with the following content:\nname: Build on: push: tags: - \u0026#39;v*\u0026#39; env: REGISTRY: ghcr.io # https://docs.github.com/en/actions/learn-github-actions/contexts#github-context IMAGE_NAME: ${{ github.repository }} jobs: build-and-push-image: runs-on: ubuntu-latest # Sets the permissions granted to the `GITHUB_TOKEN` for the actions in this job permissions: contents: read packages: write steps: - name: Checkout repository uses: actions/checkout@v3 - name: Log in to the Container registry uses: docker/login-action@v2 with: registry: ${{ env.REGISTRY }} username: ${{ github.actor }} # https://docs.github.com/en/actions/security-guides/automatic-token-authentication password: ${{ secrets.GITHUB_TOKEN }} - name: Extract metadata (tags, labels) for Docker id: meta uses: docker/metadata-action@v4 with: images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }} - name: Build and push Docker image uses: docker/build-push-action@v4 with: context: . push: true tags: ${{ steps.meta.outputs.tags }} labels: ${{ steps.meta.outputs.labels }} The above GitHub Actions workflow will perform the following steps 📝:\nCheckout the repository. Login to the GitHub Packages (ghcr.io) container registry. Use git to extract repository metadata to be used for the container image tag(s) and labels. Build the container image and push it to the GitHub Packages container registry. Now, commit and push the build.yml workflow:\ngit add .github/workflows/build.yml git commit -m \u0026#34;chore: build and publish container image\u0026#34; git push The workflow will only trigger when a push event occurs for tags matching the pattern v*. To trigger the workflow, create a new tag for your repository following the semver convention:\n# Once PR is merged git checkout main git pull git tag v1.0.0 git push --tags And that\u0026rsquo;s it! 🎉 Once the tag is pushed the workflow will trigger! Here is an example from my speeder project :slight_smile:\nClosing Thoughts The best part about this workflow is the docker/metadata-action. It uses git metadata from the repository to create the container image\u0026rsquo;s labels according to the Open Container Initiative image-spec \u0026#x2764;\u0026#xfe0f;\n❯ skopeo inspect docker://ghcr.io/dbrennand/speeder | jq .Labels { \u0026#34;org.opencontainers.image.created\u0026#34;: \u0026#34;2023-09-07T16:55:29.525Z\u0026#34;, \u0026#34;org.opencontainers.image.description\u0026#34;: \u0026#34;Python script to monitor your internet speed! 🚀 Periodically run librespeed/speedtest-cli and send results to InfluxDB.\u0026#34;, \u0026#34;org.opencontainers.image.licenses\u0026#34;: \u0026#34;MIT\u0026#34;, \u0026#34;org.opencontainers.image.revision\u0026#34;: \u0026#34;1e9dc09c6a95554ecb74f736029ec09c3ba6910c\u0026#34;, \u0026#34;org.opencontainers.image.source\u0026#34;: \u0026#34;https://github.com/dbrennand/speeder\u0026#34;, \u0026#34;org.opencontainers.image.title\u0026#34;: \u0026#34;speeder\u0026#34;, \u0026#34;org.opencontainers.image.url\u0026#34;: \u0026#34;https://github.com/dbrennand/speeder\u0026#34;, \u0026#34;org.opencontainers.image.version\u0026#34;: \u0026#34;v1.1.0\u0026#34; } Overall, this GitHub Actions workflow helps save a lot of time and effort. Furthermore, it helps make sure the container image is built and published consistently and reliably every time! \u0026#x1f680;\nThat\u0026rsquo;s all folks! Thanks for reading! \u0026#x1f44b;\nReferenced Documentation https://docs.github.com/en/actions/publishing-packages/publishing-docker-images#publishing-images-to-github-packages https://github.com/docker/login-action https://github.com/docker/metadata-action https://github.com/docker/build-push-action ","permalink":"https://dbren.uk/blog/build-and-publish-container-image-gha/","summary":"\u003cp\u003eAnd we\u0026rsquo;re back! \u0026#x1f44b;\u003c/p\u003e\n\u003cp\u003eIn this blog post we will be looking at building and publishing a container image to GitHub Packages using GitHub Actions!\u003c/p\u003e\n\u003cp\u003eI recently revisited an old project of mine; now called \u003ca href=\"https://github.com/dbrennand/speeder\"\u003espeeder\u003c/a\u003e. It\u0026rsquo;s a Python script to monitor your internet speed and send the results to \u003ca href=\"https://www.influxdata.com/\"\u003eInfluxDB\u003c/a\u003e. The results can then be visualised in \u003ca href=\"https://github.com/dbrennand/speeder#docker-compose-stack---influxdb-and-grafana\"\u003eGrafana\u003c/a\u003e. I originally created this script during the Coronavirus lockdowns whilst internet usage was high and essential for work. Since that time, I\u0026rsquo;ve learned a lot about CI/CD and noticed I hadn\u0026rsquo;t automated the build and publish process of the container image! 😬 Time to fix that! 👷\u003c/p\u003e","title":"Build and Publish Container Images using GitHub Actions"},{"content":"Time for another blog post! \u0026#x1f680;\nIn this blog post, I will be discussing how to test Ansible content with Molecule 🧪\nWhat is Molecule? Molecule aids in the development and testing of Ansible content: collections, playbooks and roles.\ngithub.com/ansible-community/molecule1\nWhy do we need to test Ansible content? Testing is an integral part of the software development lifecycle. It helps identify and prevent bugs from reaching production and in some cases, helps identify performance issues. When creating Ansible content we need to ensure that it works as expected and that we are not introducing any undesired behaviour. This is where Molecule comes in.\nMolecule Terminology 📚 Molecule has several terms that are used throughout the documentation. Let\u0026rsquo;s go over them now.\nInstances \u0026amp; Drivers 🚗 Molecule instances are what your Ansible content is executed against. Instances are created using a driver. Molecule has several drivers for handling the creation and destruction of instances. The drivers are currently located in ansible-community/molecule-plugins repository.\nFor example, the default Docker driver can be used to create a container instance and the Vagrant driver can create a virtual machine instance.\nScenarios 📖 Molecule scenarios can be thought of as a test suite. Each scenario contains its own instances and configuration. For example, a scenario could be used to test an Ansible role against different distributions or test a specific configuration of a role.\nThere should always be a default scenario which is used to test Ansible content with its default configuration.\nMolecule Directory Structure 📁 A basic Molecule directory structure is:\nmolecule └── default ├── prepare.yml ├── converge.yml ├── verify.yml └── molecule.yml Within the molecule directory is the default scenario directory which contains the prepare.yml, converge.yml and verify.yml playbooks, as well as the molecule.yml configuration file.\nPrepare Playbook ▶️ The prepare.yml playbook defines the preparation tasks to run before the converge.yml playbook. For example, this could be used configure the instance.\nConverge Playbook ▶️ The converge.yml playbook defines the Ansible content to be tested. This can be a playbook, role or collection.\nVerify Playbook ▶️ The verify.yml playbook is executed after the converge.yml playbook. This playbook contains verification tasks to check the Ansible content has been applied correctly.\nConfiguration File molecule.yml 📝 The molecule.yml file contains configuration for each Molecule component2:\nComponent Description Dependency Manager Molecule uses Ansible Galaxy as the default manager for resolving role and collection dependencies. Driver Instances \u0026amp; Drivers. Platforms (Instances) Defines instances to be created by the driver for the scenario. Provisioner Molecule uses Ansible as the provisioner. The provisioner manages the life cycle of instances by communicating with the driver. Scenario Molecule\u0026rsquo;s default scenario configuration can be overridden for full control over each sequence. Verifier Molecule uses Ansible as the verifier. The verifier uses the verify.yml playbook to check the state of the instance. The example below shows a basic configuration using the Docker driver to create a Debian container instance.\n--- dependency: name: galaxy driver: name: docker platforms: - name: instance image: geerlingguy/docker-debian10-ansible:latest pre_build_image: true provisioner: name: ansible verifier: name: ansible Molecule Commands 📜 Molecule has several commands for performing different actions. The most common commands are:\nMolecule Command Description create Creates the instance defined in the molecule.yml file. destroy Destroys the instance defined in the molecule.yml file. login Spawns a shell inside the instance. Useful for troubleshooting. converge Performs the converge sequence which includes creating the instance and executing the prepare.yml and converge.yml playbooks. test Performs a full test scenario. This includes the converge and idempotency sequences, executing the verify.yml playbook and destroying the instance. Now that we have covered the basics, let\u0026rsquo;s test an Ansible playbook with Molecule! 🧪\nDemo 📺 I\u0026rsquo;ve created a demo repository which contains a simple playbook to install nginx on a Debian container instance. You\u0026rsquo;ll need to have Docker and Python installed to use the repository.\nBegin by cloning the repository:\ngit clone https://github.com/dbrennand/molecule-demo.git \u0026amp;\u0026amp; cd molecule-demo Inspect the files and notice the default molecule scenario with the playbooks and molecule.yml configuration file. The converge.yml playbook calls the main playbook.yml two directories above.\nNext, install Molecule and the Docker driver. I recommend using a virtual environment:\nmkdir -pv ~/.virtualenvs python3 -m venv ~/.virtualenvs/molecule-demo source ~/.virtualenvs/molecule-demo/bin/activate pip install -r requirements.txt Now we can run the molecule test command to perform a full test scenario:\nmolecule test The output should look similar to the following:\nINFO Running default \u0026gt; create PLAY [Create] ****************************************************************** TASK [Create molecule instance(s)] ********************************************* changed: [localhost] =\u0026gt; (item=instance) ... INFO Running default \u0026gt; prepare PLAY [Prepare] ***************************************************************** TASK [Gathering Facts] ********************************************************* ok: [instance] TASK [Update apt cache] ******************************************************** changed: [instance] ... INFO Running default \u0026gt; converge PLAY [Converge] **************************************************************** TASK [Gathering Facts] ********************************************************* ok: [instance] PLAY [Playbook | Install nginx] ************************************************ TASK [Gathering Facts] ********************************************************* ok: [instance] TASK [Install nginx] *********************************************************** changed: [instance] PLAY RECAP ********************************************************************* instance : ok=3 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 INFO Running default \u0026gt; idempotence PLAY [Converge] **************************************************************** TASK [Gathering Facts] ********************************************************* ok: [instance] PLAY [Playbook | Install nginx] ************************************************ TASK [Gathering Facts] ********************************************************* ok: [instance] TASK [Install nginx] *********************************************************** ok: [instance] PLAY RECAP ********************************************************************* instance : ok=3 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 INFO Idempotence completed successfully. INFO Running default \u0026gt; side_effect WARNING Skipping, side effect playbook not configured. INFO Running default \u0026gt; verify INFO Running Ansible Verifier PLAY [Verify] ****************************************************************** TASK [Gathering Facts] ********************************************************* ok: [instance] TASK [Gather package list] ***************************************************** ok: [instance] TASK [Verify nginx is installed] *********************************************** ok: [instance] =\u0026gt; { \u0026#34;changed\u0026#34;: false, \u0026#34;msg\u0026#34;: \u0026#34;All assertions passed\u0026#34; } PLAY RECAP ********************************************************************* instance : ok=3 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 INFO Verifier completed successfully. INFO Running default \u0026gt; cleanup WARNING Skipping, cleanup playbook not configured. INFO Running default \u0026gt; destroy PLAY [Destroy] ***************************************************************** TASK [Wait for instance(s) deletion to complete] ******************************* FAILED - RETRYING: [localhost]: Wait for instance(s) deletion to complete (300 retries left). changed: [localhost] =\u0026gt; (item=instance) PLAY RECAP ********************************************************************* localhost : ok=3 changed=2 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 Tada! You\u0026rsquo;ve just tested an Ansible playbook with Molecule! ✨🎉\nWe now have peace of mind that our playbook works as expected! :slight_smile:\nConclusion 📝 In this blog post, we covered the basics of using Molecule and how to test Ansible content. I recommend checking out the Molecule documentation for more information.\nI hope you found this post useful and if you have any questions or feedback, feel free to reach out to me on Twitter or via email.\nUntil next time, happy testing! 🧪 \u0026#x1f44b;\nReferences https://github.com/ansible-community/molecule\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://ansible.readthedocs.io/projects/molecule/getting-started/#the-scenario-layout\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://dbren.uk/blog/testing-ansible-content/","summary":"\u003cp\u003eTime for another blog post! \u0026#x1f680;\u003c/p\u003e\n\u003cp\u003eIn this blog post, I will be discussing how to test Ansible content with \u003ca href=\"https://github.com/ansible-community/molecule\"\u003eMolecule\u003c/a\u003e 🧪\u003c/p\u003e\n\u003ch2 id=\"what-is-molecule\"\u003eWhat is Molecule?\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003eMolecule aids in the development and testing of Ansible content: collections, playbooks and roles.\u003c/p\u003e\n\u003cp\u003e\u003c!-- raw HTML omitted --\u003egithub.com/ansible-community/molecule\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e\u003c!-- raw HTML omitted --\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003ch2 id=\"why-do-we-need-to-test-ansible-content\"\u003eWhy do we need to test Ansible content?\u003c/h2\u003e\n\u003cp\u003eTesting is an integral part of the software development lifecycle. It helps identify and prevent bugs from reaching production and in some cases, helps identify performance issues. When creating Ansible content we need to ensure that it works as expected and that we are not introducing any undesired behaviour. This is where Molecule comes in.\u003c/p\u003e","title":"Testing Ansible Content with Molecule"},{"content":"Hi there! 👋\nIn my previous blog post I covered how to get started with Ansible. In this blog post I\u0026rsquo;ll be continuing to write about Ansible and covering how to manage secrets in Ansible playbooks.\nReady\u0026hellip; set\u0026hellip; go! 🚀\nManaging Secrets in Ansible 🔑 As with most automation, we need to use credentials to authenticate to our servers and other applications. Examples of secrets include usernames and passwords, API keys, SSH keys, etc. When using these types of secrets in playbooks, we need to store them securely but also still allow Ansible to access them when needed.\nIntroducing Ansible Vault 🔐 Ansible Vault is a command line tool that is part of the ansible-core package and allows us to encrypt secrets in our playbooks, inventory and vars files. It can encrypt an entire file or a specific variable within a file. Furthermore, Ansible Vault uses the AES256 encryption algorithm to encrypt files and variables using a password provided by the user.\nUsing Ansible Vault Let\u0026rsquo;s take a look at how we can use Ansible Vault to encrypt a file.\nEncrypting a File 🔒 To encrypt a file use the ansible-vault encrypt command, providing the path for the file to be encrypted. When prompted enter a password to encrypt the file:\nansible-vault encrypt vars/main.yml New Vault password: Confirm New Vault password: Encryption successful Now we can see the file is encrypted:\ncat vars/main.yml $ANSIBLE_VAULT;1.1;AES256 64383033313638636464333937393463663432616532353763646137356664376533326261373265 3466643962633863613061646235333137623266666238650a356163653039616634653632383933 30316438663233316365383637643135633736303435666236356130356565363263353262306564 3931613035373031360a396431626666343564626264346636633266343766316535346266346537 62393334393030386365376237393565343637663766336261313862363936343161 Editing or Viewing an Encrypted File To edit or view the contents of an encrypted file, use the ansible-vault edit or ansible-vault view command respectively:\nansible-vault edit vars/main.yml Vault password: ansible-vault view vars/main.yml Vault password: Decrypting a File 🔓 To decrypt the file, use the ansible-vault decrypt command and as before, provide the path to the file and the password:\nansible-vault decrypt vars/main.yml Vault password: Next, let\u0026rsquo;s see how we can use Ansible Vault to encrypt a variable.\nEncrypting a Variable To encrypt a variable, use the ansible-vault encrypt_string command providing the name of the variable and the value to encrypt. When prompted, enter a password to encrypt the variable:\nansible-vault encrypt_string -n my_secret_var \u0026#34;\u0026lt;Secret to encrypt here\u0026gt;\u0026#34; New Vault password: Confirm New Vault password: Encryption successful my_secret_var: !vault | $ANSIBLE_VAULT;1.1;AES256 63396563373936313066626338313132343139613636353538633938303135666562326139373534 3962633831613839623663353938393130343036333936340a383830366436323261646137383036 36373738346433366333666632653966323630323237666466326538336535323330663030303762 3436336162363433310a396134663062323431363263393930356237366563306238363463383066 6633 You can now copy and paste the output into your playbook vars section, vars file or inventory.\nGreat! So we now know how to encrypt a file and a variable, but how about a real world example! 🌎\nReal World Example I sometimes use Hetzner Cloud to spin up virtual servers for testing. In this short playbook which I use to create the server, there is a variable called hetzner_api_key which contains my API key. I don\u0026rsquo;t want to store this API key in plain text in my playbook! \u0026#x1f631; 🙅‍♂️\n# playbook.yml --- - name: Provision Hetzner VPS hosts: all gather_facts: false vars: hetzner_api_key: \u0026#34;my-secret-api-key\u0026#34; tasks: - name: Create a basic server hetzner.hcloud.hcloud_server: api_token: \u0026#34;{{ hetzner_api_key }}\u0026#34; location: fsn1 name: my-server server_type: cx11 image: ubuntu-18.04 state: present Using Ansible Vault we can encrypt this variable in the playbook:\nansible-vault encrypt_string -n hetzner_api_key \u0026#34;my-secret-api-key\u0026#34; New Vault password: Confirm New Vault password: Encryption successful hetzner_api_key: !vault | $ANSIBLE_VAULT;1.1;AES256 30646134626337346138323439636637366233633261373339666233653735616364623533616539 3133376230313265326631343439383238653539373034650a633632613037653337626237653730 38636136613162386530363530393537323132303538653265633635326236336561633234306562 3061613964343037320a323261363731613130343932393439626133616163346663623933313562 35313765626338653133336436393332636635656361363730363033626335643261 With the variable safely encrypted, place the variable into the playbook:\n# playbook.yml --- - name: Provision Hetzner VPS hosts: all gather_facts: false vars: hetzner_api_key: !vault | $ANSIBLE_VAULT;1.1;AES256 30646134626337346138323439636637366233633261373339666233653735616364623533616539 3133376230313265326631343439383238653539373034650a633632613037653337626237653730 38636136613162386530363530393537323132303538653265633635326236336561633234306562 3061613964343037320a323261363731613130343932393439626133616163346663623933313562 35313765626338653133336436393332636635656361363730363033626335643261 tasks: - name: Create a basic server hetzner.hcloud.hcloud_server: api_token: \u0026#34;{{ hetzner_api_key }}\u0026#34; location: fsn1 name: my-server server_type: cx11 image: ubuntu-18.04 state: present Finally, when running the playbook use the --ask-vault-pass option and enter the password used to encrypt the variable:\nansible-playbook -c local -i localhost, --ask-vault-pass playbook.yml All done! \u0026#x1f389;\nPrompting for the Vault Password Non-Interactively The above example was interactively prompting for the vault password. So how do we do this non-interactively? Say we want to run the playbook in a CI/CD pipeline. We can\u0026rsquo;t prompt for the password in this type of scenario \u0026#x1f914;\nTo solve this, we can use a vault password file.\nFirst, add the password to a file called .vault_password and lock down the permissions::\n# Note the space at the beginning of the command! # This is needed to prevent the password from being stored in the shell history. echo \u0026#34;my-encryption-password\u0026#34; \u0026gt; .vault_password chmod 600 .vault_password Now, we run the playbook with the --vault-password-file option instead:\nansible-playbook -c local -i localhost, --vault-password-file .vault_password playbook.yml Conclusion In this post we have covered how to use Ansible Vault for safely storing secrets for use in our playbooks. We\u0026rsquo;ve covered encrypting files and variables and how to use a vault password file to prompt for the vault password non-interactively.\nI hope you found this post useful and if you have any questions or feedback, feel free to reach out to me on Twitter or via email.\nUntil next time, happy automating! \u0026#x1f680; \u0026#x1f44b;\n","permalink":"https://dbren.uk/blog/managing-secrets-in-ansible/","summary":"\u003cp\u003eHi there! 👋\u003c/p\u003e\n\u003cp\u003eIn my previous \u003ca href=\"https://danielbrennand.com/blog/getting-started-ansible/\"\u003eblog post\u003c/a\u003e I covered how to get started with Ansible. In this blog post I\u0026rsquo;ll be continuing to write about Ansible and covering how to manage secrets in Ansible playbooks.\u003c/p\u003e\n\u003cp\u003eReady\u0026hellip; set\u0026hellip; go! 🚀\u003c/p\u003e\n\u003ch1 id=\"managing-secrets-in-ansible-\"\u003eManaging Secrets in Ansible 🔑\u003c/h1\u003e\n\u003cp\u003eAs with most automation, we need to use credentials to authenticate to our servers and other applications. Examples of secrets include usernames and passwords, API keys, SSH keys, etc. When using these types of secrets in playbooks, we need to store them securely but also still allow Ansible to access them when needed.\u003c/p\u003e","title":"Managing Secrets in Ansible"},{"content":"Hey there! \u0026#x1f44b;\nIn this blog post I\u0026rsquo;ll be writing about Ansible. I\u0026rsquo;ve been using it for a little while now and I\u0026rsquo;d like to share my experience with it and share some knowledge for others starting out! :slight_smile:\nLet\u0026rsquo;s begin! \u0026#x1f680;\nWhat is Ansible? Ansible is an open-source automation tool which can be used to automate the management and configuration of servers and other devices. Written in Python, Ansible is agentless and connects over SSH. Some common use-cases for Ansible include (but are not limited to):\nOrchestrating large scale application deployments. Prevent configuration drift and enforce consistent configurations. Provisioning and deprovisioning tasks. Audit server configuration and compliance. What makes Ansible great is its simplicity and ease of use. By adopting a human friendly language such as YAML, it lowers the barrier to entry for new users and makes it easy to read and understand.\nAnsible Terminology 📚 Before we get started, lets go over some of the terminology used in Ansible.\nControl Node The control node is the machine that Ansible is installed and run from. In most cases this will be your local machine however, in some cases, it may be another machine. For example, an organisation may use a Rundeck server as the control node or use Ansible Automation Platform which has the Ansible Automation Controller as the control node.\nSee the Ansible control node requirements documentation for further information.\nInventory 📋 The inventory file contains a list of servers that Ansible will manage. Inventory can be in INI or YAML format. Furthermore, it can be static or dynamically created via a script or inventory plugin (more on this later). For example, dynamic inventory could be created from a CMDB or cloud provider API.\nStatic INI Inventory # inventory.ini [webservers] # Inventory group name web1.example.com web2.example.com [databases] db1.example.com Static YAML Inventory # inventory.yaml --- webservers: # Inventory group name hosts: web1.example.com: web2.example.com: databases: hosts: db1.example.com: See the Ansible inventory documentation for further information.\nPlaybooks ▶️ Playbooks define the tasks (plays) that Ansible will run against the servers in the inventory. Playbooks are written in YAML and executed using the ansible-playbook command.\nA simple playbook could look like this:\n--- # playbook.yaml - name: Install Nginx on webservers hosts: webservers tasks: - name: Install Nginx ansible.builtin.package: name: nginx state: present Which you can run like this:\nansible-playbook -i inventory.yaml playbook.yaml See the Ansible playbook documentation for further information.\nModules and Plugins 🔌 Modules are the building blocks for doing anything in Ansible. Each task in a playbook uses a module and there are many modules available for use right out of the box. Modules can perform a wide range of tasks such as installing packages, managing users, configuring services and more. The example playbook above uses the built-in package module to install Nginx on the webservers.\nFurthermore, most modules provide idempotency which means it will check whether the desired state is already present before making changes. This ensures that the state of the server is kept consistent and speeds up our playbook runs! \u0026#x1f680;\nPlugins allow us to extend Ansible\u0026rsquo;s core functionality. There are many types of plugins in Ansible. Some examples include:\nInventory plugins to create dynamic inventory. Callback plugins which change the output format. Become plugins for changing the privilege escalation method. The main difference between a module and a plugin is that modules are executed on the remote server whereas plugins are executed on the control node itself.\nSee the following documentation pages for more information on modules and plugins:\nhttps://docs.ansible.com/ansible/latest/module_plugin_guide/index.html https://docs.ansible.com/ansible/latest/module_plugin_guide/modules_intro.html https://docs.ansible.com/ansible/latest/plugins/module.html https://docs.ansible.com/ansible/latest/plugins/plugins.html Roles Roles contain tasks, files, variables, templates and more for performing repeatable actions. The main benefit of roles is that they can be shared and reused easily across playbooks. You may have to regularly install and configure Nginx on servers; but the configuration differs slightly each time. This would be the perfect use-case for a role.\nRoles can be installed using the ansible-galaxy command. For example, to install the geerlingguy.nginx role you would run the following command:\nansible-galaxy role install geerlingguy.nginx Then, you can use the role in your playbook like this:\n# playbook.yaml - name: Install and configure Nginx on webservers hosts: webservers roles: - geerlingguy.nginx This would use the role defaults but nevertheless, it\u0026rsquo;s far simpler and maintainable right?! No copying tasks from a playbook in one project to another! 🙅‍♂️ We need to stay sane after all 🤪\nCollections 📦 Collections are the newest distribution format for Ansible content. They can be used to package and distribute playbooks, roles, modules and plugins. Collections can also be installed using the ansible-galaxy command. For example, to install the community.docker collection you would run the following command:\nansible-galaxy collection install community.docker Installing Ansible ⚙️ My advice would be to install Ansible in a Python virtual environment. This is because more often than not, the version of Ansible provided by your package manager will be sorely outdated. Furthermore, this way the installation of Ansible won\u0026rsquo;t interfere with your system\u0026rsquo;s Python installation and avoiding the pains of dependency hell.\nTo install Ansible in a virtual environment you\u0026rsquo;ll need Python 3 installed (the newer the better) and the venv module.\nBegin by creating a directory to hold your virtual environment and initialise it:\nmkdir -pv ~/.venv/ansible \u0026amp;\u0026amp; python -m venv ~/.venv/ansible Next, activate the virtual environment and install Ansible:\nsource ~/.venv/ansible/bin/activate pip install ansible Now you\u0026rsquo;re ready to go: \u0026#x1f389;\nansible --version ansible [core 2.14.3] config file = None configured module search path = [\u0026#39;/Users/user/.ansible/plugins/modules\u0026#39;, \u0026#39;/usr/share/ansible/plugins/modules\u0026#39;] ansible python module location = /Users/user/.venv/ansible/lib/python3.11/site-packages/ansible ansible collection location = /Users/user/.ansible/collections:/usr/share/ansible/collections executable location = /Users/user/.venv/ansible/bin/ansible python version = 3.11.1 (main, Feb 8 2023, 19:50:54) [Clang 13.1.6 (clang-1316.0.21.2.5)] (/Users/user/.venv/ansible/bin/python) jinja version = 3.1.2 libyaml = True Ad-hoc Commands Ad-hoc commands are a perfect way for running one off commands quickly and easily against one or more servers. For example, if we wanted to check the storage space of block devices on our servers we could run:\nansible webservers -i inventory.yml -m ansible.builtin.command -a \u0026#34;df -hT\u0026#34; Yay! ✨ No more sshing into each server and running the command manually! 🤮 💪\nWhat am I using Ansible for? Whilst configuring the devices in my Homelab, I quickly realised that I was deploying Caddy as a container and setting up Autorestic (my personal backup tool of choice) over and over again. I decided to create a Caddy Docker and a Autorestic role and use them in my playbooks. This way I can easily deploy them to any new device I add to my Homelab! \u0026#x1f604;\nI\u0026rsquo;ve also been using Ansible in my Homelab to deploy my Raspberry Pi K3s cluster and configure my Intel NUC media server. These devices have several containerised services running on them such as Pi-hole, Jellyfin, the *arrs and more. It\u0026rsquo;s still a work in progress and I plan on releasing the playbooks publicly once I\u0026rsquo;ve finalised them; so keep an eye out for that! \u0026#x1f440;\nConclusion We\u0026rsquo;ve only just scratched the surface of Ansible in this blog post. There are many more features and capabilities that Ansible has to offer. I\u0026rsquo;m sure I\u0026rsquo;ll be writing more blog posts about Ansible in the future. If you have any questions or suggestions on what I should write next, feel free to reach out to me on Twitter or via email.\nFinally, if your thirst for knowledge isn\u0026rsquo;t quenched, I highly recommend checking out the following resources:\nJeff Geerling\u0026rsquo;s Ansible 101 YouTube series. Jeff Geerling\u0026rsquo;s Ansible for DevOps book. Ansible documentation. That\u0026rsquo;s all folks! \u0026#x1f44b;\n","permalink":"https://dbren.uk/blog/getting-started-ansible/","summary":"\u003cp\u003eHey there! \u0026#x1f44b;\u003c/p\u003e\n\u003cp\u003eIn this blog post I\u0026rsquo;ll be writing about Ansible. I\u0026rsquo;ve been using it for a little while now and I\u0026rsquo;d like to share my experience with it and share some knowledge for others starting out! :slight_smile:\u003c/p\u003e\n\u003cp\u003eLet\u0026rsquo;s begin! \u0026#x1f680;\u003c/p\u003e\n\u003ch1 id=\"what-is-ansible\"\u003eWhat is Ansible?\u003c/h1\u003e\n\u003cp\u003eAnsible is an open-source automation tool which can be used to automate the management and configuration of servers and other devices. Written in Python, Ansible is agentless and connects over SSH. Some common use-cases for Ansible include (but are not limited to):\u003c/p\u003e","title":"Getting started with Ansible"},{"content":"Recently I\u0026rsquo;ve been working on several PowerShell scripts that require credentials to access REST APIs. In this blog post, I will showcase two approaches for storing credentials securely for use in PowerShell scripts.\nEncrypted Password File 🔒 The encrypted password file leverages the Windows Data Protection API (DPAPI) to encrypt the password as a System.Security.SecureString:\n$Credentials = Get-Credential $Credentials.Password System.Security.SecureString $Credentials.Password | ConvertFrom-SecureString 01000000d08c9ddf0115d1118c7a00c04fc297eb01000000f5ab85d7ee9da048ae4ae797ee7eaf0a000000000200000000001066000000010000200000008c4a03d2f0731e0e7661d695fda8b441eaff31e75724931f31374a0c8292b636000000000e800000000200002000000028da885828bd627480178382ce9a1b477819e7703546ce41819d37f4e63d33ba20000000ab2c4401635ec24db9f20071e18dea0b79ce16ba38b5503ec9937b7fbc849dcf40000000155053a793c210998ef7317b0161e7344c2174b904b527c0cf24e7bbf2243b99e936df3ab67bc9e285a1be33aed37c7604fb07f5d0c44ceb7d6334ca30b0a610 By default DPAPI uses the current user context to generate an encryption key. This encryption key is then used to encrypt the PSCredential.Password property as a System.Security.SecureString (as shown above). It is possible to provide your own encryption key, but I won\u0026rsquo;t be covering that in this post. If you want to read more on this, check out Travis Gan\u0026rsquo;s blog 1.\nIt\u0026rsquo;s also worth noting there are several caveats to this approach:\nYou must encrypt the password file as the user which will be accessing it. DPAPI is specific to the device which you encrypt the password file on. You cannot decrypt the password file on another system with the same user. Generating and using the Encrypted Password File To store the password securely in a file, we can use the Export-Clixml cmdlet which will store the System.Management.Automation.PSCredential object in XML format:\n$Credentials = Get-Credential $Credentials | Export-Clixml -Path \u0026#34;$(pwd)\\EncryptedCreds.xml\u0026#34; Once created, the EncryptedCreds.xml file will contain the XML representation of the PSCredential object. As shown below, the Password property is stored as a SecureString:\n\u0026lt;Objs Version=\u0026#34;1.1.0.1\u0026#34; xmlns=\u0026#34;http://schemas.microsoft.com/powershell/2004/04\u0026#34;\u0026gt; \u0026lt;Obj RefId=\u0026#34;0\u0026#34;\u0026gt; \u0026lt;TN RefId=\u0026#34;0\u0026#34;\u0026gt; \u0026lt;T\u0026gt;System.Management.Automation.PSCredential\u0026lt;/T\u0026gt; \u0026lt;T\u0026gt;System.Object\u0026lt;/T\u0026gt; \u0026lt;/TN\u0026gt; \u0026lt;ToString\u0026gt;System.Management.Automation.PSCredential\u0026lt;/ToString\u0026gt; \u0026lt;Props\u0026gt; \u0026lt;S N=\u0026#34;UserName\u0026#34;\u0026gt;User\u0026lt;/S\u0026gt; \u0026lt;SS N=\u0026#34;Password\u0026#34;\u0026gt;01000000d08c9ddf0115d1118c7a00c04fc297eb01000000f5ab85d7ee9da048ae4ae797ee7eaf0a000000000200000000001066000000010000200000008c4a03d2f0731e0e7661d695fda8b441eaff31e75724931f31374a0c8292b636000000000e800000000200002000000028da885828bd627480178382ce9a1b477819e7703546ce41819d37f4e63d33ba20000000ab2c4401635ec24db9f20071e18dea0b79ce16ba38b5503ec9937b7fbc849dcf40000000155053a793c210998ef7317b0161e7344c2174b904b527c0cf24e7bbf2243b99e936df3ab67bc9e285a1be33aed37c7604fb07f5d0c44ceb7d6334ca30b0a610\u0026lt;/SS\u0026gt; \u0026lt;/Props\u0026gt; \u0026lt;/Obj\u0026gt; \u0026lt;/Objs\u0026gt; We can then re-create the PSCredential object for use in PowerShell scripts using the Import-Clixml cmdlet:\n$Credentials = Import-Clixml -Path \u0026#34;$(pwd)\\EncryptedCreds.xml\u0026#34; $Credentials UserName Password -------- -------- User System.Security.SecureString $Credentials.GetType() IsPublic IsSerial Name BaseType -------- -------- ---- -------- True True PSCredential System.Object Write-Output -InputObject \u0026#34;Username: \u0026#34;\u0026#34;$($Credential.UserName)\u0026#34;\u0026#34; - Password: \u0026#34;\u0026#34;$($Credential.GetNetworkCredential().Password)\u0026#34;\u0026#34;\u0026#34; Username: \u0026#34;User\u0026#34; - Password: \u0026#34;Password123!\u0026#34; Tada! ✨\nThis approach can be good for one off scripts but, can quickly become a maintainance burden when you\u0026rsquo;re doing this for many scripts. For this type of use-case, the next approach is probably more suitable.\nSecretManagement \u0026amp; SecretStore Modules 🔐 The SecretManagement and SecretStore modules are a great choice for storing credentials securely. To begin, install the modules:\nInstall-Module -Name \u0026#34;Microsoft.PowerShell.SecretManagement\u0026#34;, \u0026#34;Microsoft.PowerShell.SecretStore\u0026#34; -Verbose Creating a SecretVault Next, we need to create a SecretVault to store secrets using the Register-SecretVault cmdlet 🔐:\nRegister-SecretVault -Name \u0026#34;MySecretVault\u0026#34; -ModuleName \u0026#34;Microsoft.PowerShell.SecretStore\u0026#34; -DefaultVault -Description \u0026#34;Secret vault storing my secrets.\u0026#34; -PassThru Name ModuleName IsDefaultVault ---- ---------- -------------- MySecretVault Microsoft.PowerShell.SecretStore True Storing Secrets Nice! \u0026#x1f44d; Now lets store a PSCredential in the SecretVault with the Set-Secret cmdlet. When creating the first secret you will be prompted for a password which will be used to encrypt the SecretVault:\nℹ You can store a SecureString, HashTable, String or Byte[] in a secret as well!\n$RestAPICreds = Get-Credential Set-Secret -Name \u0026#34;REST API Creds\u0026#34; -Vault \u0026#34;MySecretVault\u0026#34; -Secret $RestAPICreds Creating a new MySecretVault vault. A password is required by the current store configuration. Enter password: ******** Enter password again for verification: ******** # Bonus! Store metadata alongside the secret itself! $Metadata = @{ SecretExpiration = ([DateTime]::New(2022, 12, 22)) } Set-Secret -Name \u0026#34;REST API Creds\u0026#34; -Vault \u0026#34;MySecretVault\u0026#34; -Secret $RestAPICreds -Metadata $Metadata Retrieving Secrets Now that the secret is stored securely, we can retrieve it in another script using the Get-Secret cmdlet:\n$RestAPICredentialsSecret = Get-Secret -Name \u0026#34;REST API Credentials\u0026#34; -Vault \u0026#34;MySecretVault\u0026#34; $RestAPICredentialsSecret UserName Password -------- -------- test System.Security.SecureString # Bonus! Get information about a secret too! # https://learn.microsoft.com/en-us/powershell/module/microsoft.powershell.secretmanagement/get-secretinfo?view=ps-modules Get-SecretInfo -Name \u0026#34;REST API Credentials\u0026#34; -Vault \u0026#34;MySecretVault\u0026#34; Name Type VaultName ---- ---- --------- REST API Credentials PSCredential MySecretVault # As it\u0026#39;s a PSCredential object, we can use the GetNetworkCredential method to finally get the password! $RestAPICredentialsSecret.GetNetworkCredential().Password Awesome! 😎 We\u0026rsquo;ve stored the first secret securely! ✨🚀\nHowever, if you use the Get-Secret cmdlet in a new session, or after a certain amount of time has elapsed, you may notice that you\u0026rsquo;re prompted for the password to decrypt the SecretVault again! \u0026#x1f631;\nGet-Secret -Name \u0026#34;REST API Credentials\u0026#34; -Vault \u0026#34;MySecretVault\u0026#34; Vault MySecretVault requires a password. Enter password: Obviously this is an issue if we want our scripts to work without any interaction!\nRetrieving Secrets Non-Interactively To fix this we can leverage the first approach and store our SecretVault decryption password in an encrypted password file and retrieve it later! So\u0026hellip; lets do it! \u0026#x1f604;\n$SecretVaultPassword = Get-Credential $SecretVaultPassword | Export-Clixml -Path \u0026#34;$(pwd)\\SecretVaultPassword.xml\u0026#34; Get-Content -Path \u0026#34;$(pwd)\\SecretVaultPassword.xml\u0026#34; ... \u0026lt;Objs Version=\u0026#34;1.1.0.1\u0026#34; xmlns=\u0026#34;http://schemas.microsoft.com/powershell/2004/04\u0026#34;\u0026gt; \u0026lt;Props\u0026gt; ... \u0026lt;SS N=\u0026#34;Password\u0026#34;\u0026gt;700061007300730077006f0072006400\u0026lt;/SS\u0026gt; \u0026lt;/Props\u0026gt; \u0026lt;/Obj\u0026gt; Once done, we can use the Unlock-SecretStore cmdlet in our scripts to unlock the SecretVault non-interactively:\n$SecretVaultPassword = (Import-Clixml -Path \u0026#34;$(pwd)\\SecretVaultPassword.xml\u0026#34;).Password Unlock-SecretStore -Password $SecretVaultPassword Automation FTW! ⚙ 💪\nModifying SecretVault Configuration Finally, you may also want to modify the configuration of a SecretVault. You can do this using the SecretStore cmdlets Get-SecretStoreConfiguration and Set-SecretStoreConfiguration.\nLet\u0026rsquo;s check the SecretVault configuration with the Get-SecretStoreConfiguration cmdlet:\nGet-SecretStoreConfiguration Scope Authentication PasswordTimeout Interaction ----- -------------- --------------- ----------- CurrentUser Password 900 Prompt We can see that the SecretVault configuration is set to prompt for a password, and the timeout before being prompted for the password again is 900 seconds (15 minutes) by default.\nThe timeout can be changed using the Set-SecretStoreConfiguration cmdlet:\nSet-SecretStoreConfiguration -PasswordTimeout 1800 -Confirm:$false # Check the configuration again Get-SecretStoreConfiguration Scope Authentication PasswordTimeout Interaction ----- -------------- --------------- ----------- CurrentUser Password 1800 Prompt Conclusion Finished! \u0026#x1f604; Hopefully you learned something new and found this post helpful! :slight_smile: There is a lot that I didn\u0026rsquo;t cover when it comes to the SecretManagement module. For example, extensions which create integrations with third-party secret management products like Azure KeyVault, KeePass, HashiCorp Vault and more!\nUntil the next one! 👋\nReferences https://www.travisgan.com/2015/06/powershell-password-encryption.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://dbren.uk/blog/powershell-storing-credentials/","summary":"\u003cp\u003eRecently I\u0026rsquo;ve been working on several PowerShell scripts that require credentials to access REST APIs. In this blog post, I will showcase two approaches for storing credentials securely for use in PowerShell scripts.\u003c/p\u003e\n\u003ch1 id=\"encrypted-password-file-\"\u003eEncrypted Password File 🔒\u003c/h1\u003e\n\u003cp\u003eThe encrypted password file leverages the \u003ca href=\"https://learn.microsoft.com/en-us/previous-versions/ms995355(v=msdn.10)\"\u003eWindows Data Protection API (DPAPI)\u003c/a\u003e to encrypt the password as a \u003ccode\u003eSystem.Security.SecureString\u003c/code\u003e:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-powershell\" data-lang=\"powershell\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e$Credentials = Get-Credential\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e$Credentials.Password\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eSystem.Security.SecureString\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e$Credentials.Password | ConvertFrom-SecureString\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e01000000d08c9ddf0115d1118c7a00c04fc297eb01000000f5ab85d7ee9da048ae4ae797ee7eaf0a000000000200000000001066000000010000200000008c4a03d2f0731e0e7661d695fda8b441eaff31e75724931f31374a0c8292b636000000000e800000000200002000000028da885828bd627480178382ce9a1b477819e7703546ce41819d37f4e63d33ba20000000ab2c4401635ec24db9f20071e18dea0b79ce16ba38b5503ec9937b7fbc849dcf40000000155053a793c210998ef7317b0161e7344c2174b904b527c0cf24e7bbf2243b99e936df3ab67bc9e285a1be33aed37c7604fb07f5d0c44ceb7d6334ca30b0a610\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eBy default DPAPI uses the current user context to generate an encryption key. This encryption key is then used to encrypt the \u003ccode\u003ePSCredential.Password\u003c/code\u003e property as a \u003ccode\u003eSystem.Security.SecureString\u003c/code\u003e (as shown above). It is possible to provide your own encryption key, but I won\u0026rsquo;t be covering that in this post. If you want to read more on this, check out \u003c!-- raw HTML omitted --\u003eTravis Gan\u0026rsquo;s blog \u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e\u003c!-- raw HTML omitted --\u003e.\u003c/p\u003e","title":"PowerShell: Storing Credentials Securely"},{"content":"Hi there! 👋\nRecently I discovered Git Hooks. Git Hooks provide a way of running custom scripts when a certain git action occurs. In this post, I want to share a pre-commit Git Hook I\u0026rsquo;ve written to lint PowerShell code using the PSScriptAnalyzer module.\nWhat is a Linter? 🕵️‍♂️ A linter analyses code to identify common errors, bugs and stylistic issues. Their aim is to improve code quality. Linters perform static analysis meaning they check code without executing it. Some well known linters for other languages include ESLint and Pylint.\nWhat is PSScriptAnalyzer? PSScriptAnalyzer is a linter for PowerShell modules and scripts. It runs a set of rules against PowerShell code. These rules are based on best practices identified by the PowerShell team at Microsoft and the community. The full set of rules can be found here.\nWhat is a pre-commit Git Hook? A pre-commit Git Hook executes when running git commit. The contents of the pre-commit file located at .git/hooks/pre-commit is executed and if the script has an exit code of 1 (job failed), then the commit is aborted. Pre-commit hooks provide an excellent use case for linting code changes before pushing to a remote repository.\nLinting PowerShell code using the pre-commit Git Hook 🔎 To use this pre-commit Git Hook you must have PowerShell 7, Git and the PSScriptAnalyzer module installed.\nInstall PSScriptAnalyzer:\nInstall-Module -Name \u0026#34;PSScriptAnalyzer\u0026#34; -Verbose Pre-commit Git Hook #!/usr/bin/env pwsh Import-Module -Name \u0026#34;PSScriptAnalyzer\u0026#34; $DiffNames = git --no-pager diff --name-only --staged --line-prefix=\u0026#34;$(git rev-parse --show-toplevel)/\u0026#34; $Results = @() foreach ($DiffName in $DiffNames) { Write-Output -InputObject \u0026#34;Analysing \u0026#34;\u0026#34;$($DiffName)\u0026#34;\u0026#34;\u0026#34; $Output = Invoke-ScriptAnalyzer -Path $DiffName $Results += $Output } if ($Results.Count -gt 0) { Write-Warning -Message \u0026#34;PSScriptAnalyzer identified one or more files with linting errors. Commit aborted. Fix them before committing or use \u0026#39;git commit --no-verify\u0026#39; to bypass this check.\u0026#34; foreach ($Result in $Results) { Write-Error -Message \u0026#34;$($Result.ScriptName) - Line $($Result.Line) - $($Result.Message)\u0026#34; } exit 1 } Installation Linux Create .git/hooks/pre-commit:\ntouch .git/hooks/pre-commit Paste the PowerShell snippet above into .git/hooks/pre-commit\nMake .git/hooks/pre-commit executable:\nchmod +x .git/hooks/pre-commit Windows Create .git/hooks/pre-commit.ps1 and .git/hooks/pre-commit:\nNew-Item -Path \u0026#34;.git\\hooks\\pre-commit.ps1\u0026#34;, \u0026#34;.git\\hooks\\pre-commit\u0026#34; -ItemType \u0026#34;File\u0026#34; Paste the PowerShell snippet above into .git/hooks/pre-commit.ps1\nPaste the snippet below into .git/hooks/pre-commit:\n#!/bin/sh pwsh -File \u0026#34;$(git rev-parse --show-toplevel)\\.git\\hooks\\pre-commit.ps1\u0026#34; Usage Now lets see if it\u0026rsquo;s working! \u0026#x1f604;\nCreate a directory and initialise a new repository:\nmkdir ~/git-hook-pwsh cd ~/git-hook-pwsh git init Follow the installation steps above depending on your environment.\nCreate a file called git-hook-pwsh.ps1 which uses Write-Host:\necho \u0026#39;Write-Host -Object \u0026#34;Oh no! Not the dreaded Write-Host!\u0026#34;\u0026#39; \u0026gt; git-hook-pwsh.ps1 Stage git-hook-pwsh.ps1 and attempt to commit it:\ngit add git-hook-pwsh.ps1; git commit -m \u0026#34;Is my pre-commit git hook working?\u0026#34; If all goes well you should see an output similar to below in your terminal:\n➜ git-hook-pwsh git:(master) ✗ git commit -m \u0026#34;Is my pre-commit git hook working?\u0026#34; Analysing \u0026#34;/home/dab/git-hook-pwsh/git-hook-pwsh.ps1\u0026#34; WARNING: PSScriptAnalyzer identified one or more files with linting errors. Commit aborted. Fix them before committing or use \u0026#39;git commit --no-verify\u0026#39; to bypass this check. Write-Error: git-hook-pwsh.ps1 - Line 1 - File \u0026#39;git-hook-pwsh.ps1\u0026#39; uses Write-Host. Avoid using Write-Host because it might not work in all hosts, does not work when there is no host, and (prior to PS 5.0) cannot be suppressed, captured, or redirected. Instead, use Write-Output, Write-Verbose, or Write-Information. Awesome! It worked! \u0026#x2b50;\nI\u0026rsquo;ve also posted this pre-commit Git Hook as a GitHub gist so others can find it! \u0026#x1f603;\nEnjoy! \u0026#x2728; Until next time! \u0026#x1f604;\n","permalink":"https://dbren.uk/blog/git-hook-powershell/","summary":"\u003cp\u003eHi there! 👋\u003c/p\u003e\n\u003cp\u003eRecently I discovered \u003ca href=\"https://git-scm.com/book/en/v2/Customizing-Git-Git-Hooks\"\u003eGit Hooks\u003c/a\u003e. Git Hooks provide a way of running custom scripts when a certain git action occurs. In this post, I want to share a pre-commit Git Hook I\u0026rsquo;ve written to lint PowerShell code using the \u003ca href=\"https://github.com/PowerShell/PSScriptAnalyzer\"\u003ePSScriptAnalyzer\u003c/a\u003e module.\u003c/p\u003e\n\u003ch1 id=\"what-is-a-linter-\"\u003eWhat is a Linter? 🕵️‍♂️\u003c/h1\u003e\n\u003cp\u003eA linter analyses code to identify common errors, bugs and stylistic issues. Their aim is to improve code quality. Linters perform static analysis meaning they check code without executing it. Some well known linters for other languages include ESLint and Pylint.\u003c/p\u003e","title":"Using Git Hooks to lint PowerShell"},{"content":"Hi there! \u0026#x1f44b;\nIn this blog post, I will show you how to download a streamed video from a website using youtube-dl.\nBackground Recently I needed to download a streamed video from a website for archival purposes. Full disclaimer, the service had already been paid for and I wanted to keep the video which was set to expire after a specific date.\nInvestigation \u0026#x1f50d; At first there was no obvious way to download the video. Investigating the HTTP traffic in the browser, I could see that the video was being streamed with each part being progressively downloaded.\nEach part of the video was split into Transport Stream (.ts) files, each lasting around 25 seconds. After some quick googling I came across this stack overflow post. The answers said to find the playlist file (.m3u8) associated with the Transport Stream.\nFinding the Transport Stream Playlist File URL \u0026#x1f517; To find the Transport Stream playlist file URL, follow the steps below:\nOpen your browser of choice.\nPress CTRL + SHIFT + I to bring up developer tools and select Network.\nGo to the web page containing the streamed video and use the filter to search the HTTP requests for playlist.m3u8.\nIn my example, the Transport Stream playlist file URL was similar to: https://x.cloudfront.net/x_vods/_definst_/xvods/x-vod/vb/11111_2021-11-22_12355F4.mp4/playlist.m3u8\nDownloading a Streamed Video using youtube-dl ⬇️ To download the streamed video I used mikenye\u0026rsquo;s youtube-dl container image.\nVerifying the Transport Stream Playlist File URL \u0026#x1f517; To verify that you have the correct playlist file URL, run youtube-dl with the -F option to list all available formats:\ndocker run \\ --rm -i \\ -e PGID=$(id -g) \\ -e PUID=$(id -u) \\ -v \u0026#34;$(pwd)\u0026#34;:/workdir:rw \\ mikenye/youtube-dl -F \\ https://x.cloudfront.net/x_vods/_definst_/xvods/x-vod/vb/11111_2021-11-22_12355F4.mp4/playlist.m3u8 This should output something similar to:\n[generic] playlist: Requesting header [generic] playlist: Downloading m3u8 information [info] Available formats for playlist: format code extension resolution note 556 mp4 640x360 556k , avc1.42c01e, mp4a.40.2 Awesome! 😎 Now you know you have the correct URL!\nDownloading the Streamed Video ⬇️ To download the streamed video, run the following command providing the format code from the output above:\ndocker run \\ --rm -i \\ -e PGID=$(id -g) \\ -e PUID=$(id -u) \\ -v \u0026#34;$(pwd)\u0026#34;:/workdir:rw \\ mikenye/youtube-dl --format 556 \\ https://x.cloudfront.net/x_vods/_definst_/xvods/x-vod/vb/11111_2021-11-22_12355F4.mp4/playlist.m3u8 You should see youtube-dl identify each .ts file in the playlist, read its contents and combine them into a single .mp4 file.\n[generic] playlist: Requesting header [generic] playlist: Downloading m3u8 information [download] Destination: playlist-playlist.mp4 ... [https @ 0x561d074eadc0] Opening \u0026#39;https://x.cloudfront.net/x_vods/_definst_/xvods/x-vod/vb/11111_2021-11-22_12355F4.mp4/media_w123456789_2.ts\u0026#39; for reading ... [https @ 0x561d074eadc0] Opening \u0026#39;https://x.cloudfront.net/x_vods/_definst_/xvods/x-vod/vb/11111_2021-11-22_12355F4.mp4/media_w123456789_308.ts\u0026#39; for reading ... frame=77069 fps=2670 q=-1.0 Lsize=84894kB time=00:51:22.79 bitrate=225.6kbits/s speed=107x video:35021kB audio:48855kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 1.213379% [ffmpeg] Downloaded 86931123 bytes [download] 100% of 82.90MiB in 00:30 Absolute magic! \u0026#x2728;\nI hope someone finds this blog post useful and it saves them a bit of time 🙂\n","permalink":"https://dbren.uk/blog/download-streamed-video/","summary":"\u003cp\u003eHi there! \u0026#x1f44b;\u003c/p\u003e\n\u003cp\u003eIn this blog post, I will show you how to download a streamed video from a website using \u003ca href=\"https://github.com/ytdl-org/youtube-dl\"\u003eyoutube-dl\u003c/a\u003e.\u003c/p\u003e\n\u003ch1 id=\"background\"\u003eBackground\u003c/h1\u003e\n\u003cp\u003eRecently I needed to download a streamed video from a website for archival purposes. Full disclaimer, the service had already been paid for and I wanted to keep the video which was set to expire after a specific date.\u003c/p\u003e\n\u003ch1 id=\"investigation-mag\"\u003eInvestigation \u0026#x1f50d;\u003c/h1\u003e\n\u003cp\u003eAt first there was no obvious way to download the video. Investigating the HTTP traffic in the browser, I could see that the video was being streamed with each part being progressively downloaded.\u003c/p\u003e","title":"Use youtube-dl to download a streamed video"},{"content":"Recently at work, a customer raised a ticket about programmatically interacting with UKCloud\u0026rsquo;s Cloud Storage service.\nUKCloud\u0026rsquo;s Cloud Storage service is an object storage solution based on Dell EMC Elastic Cloud Storage (ECS). Access is via a RESTful API, which also provides support for Amazon\u0026rsquo;s S3 API.\nUKCloud: Getting Started Guide for Cloud Storage 1\nThis ticket was interesting as the customer was using the @aws-sdk/client-s3 JavaScript package to upload images to the service. Prior to this ticket, I hadn\u0026rsquo;t used this package before or have much experience with S3 object storage and programmatically interacting with it. In this blog post, I want to share my learnings and provide a couple of examples in Node.js for interacting with UKCloud\u0026rsquo;s Cloud Storage service using this package.\n[NOTE]\nIt is worth noting that the code examples in this post should work with any S3 object storage provider, just modify the endpoint and region!\nS3 Terminology Before we get started, it\u0026rsquo;s important to understand certain terminology around Amazon\u0026rsquo;s S3 API.\nObjects An object is a file and any metadata that describes that file. Objects consist of object data and metadata. The metadata is a set of name-value pairs that describe the object. These pairs include some default metadata, such as the date last modified \u0026hellip; You can also specify custom metadata at the time that the object is stored.\nAmazon Web Services: What is Amazon S3? 2\nAmazon Web Services: Uploading, downloading, and working with objects in Amazon S3 3\nObject Keys 🔑 An object key (or key name) is a unique identifier of an object.\nAmazon Web Services: Creating object key names 4\nAn example of an object key could be Testing/Requirements.pdf or Accounting/Payslips.xls. Note that the object keys are prefixed with a directory.\nBuckets A bucket is a container for objects\u0026hellip; Every object is contained in a bucket.\nAmazon Web Services: What is Amazon S3? 2\nA bucket has a flat structure with no actual concept of directories however, you can prefix object keys with (sub)directories to create a directory structure 📁\nNow that\u0026rsquo;s out of the way, let\u0026rsquo;s begin interacting with the ECS S3 API using the @aws-sdk/client-s3 for JavaScript 😎\nPrerequisites Before starting, you will need the following:\nPrerequisite Optional Docker installed on your system. No The S3 API endpoint for the object storage provider of your choice. In the examples I will be using UKCloud\u0026rsquo;s Cloud Storage endpoint: https://cas.cor00005.ukcloud.com. No The object storage provider\u0026rsquo;s region name. In the examples I will be using: cor00005. No A bucket created with an object storage provider of your choice. Yes - You can create one via the API however, for the examples I already had a bucket. The object storage provider\u0026rsquo;s equivalent of an AccessKeyId and SecretAccessKey. No Node.js Examples 👨‍💻 The following packages are used in these examples:\n@aws-sdk/client-s3 - Documentation.\n@aws-sdk/s3-request-presigner - Documentation.\nfs\nBegin by creating two files: examples.mjs and dockerfile. Then, paste the snippet below into the dockerfile:\nFROM node:16-alpine RUN mkdir -p /usr/src/app \u0026amp;\u0026amp; chown -R node:node /usr/src/app WORKDIR /usr/src/app RUN npm install @aws-sdk/client-s3@3.43.0 @aws-sdk/s3-request-presigner@3.44.0 COPY --chown=node:node examples.mjs examples.mjs USER node CMD [\u0026#34;node\u0026#34;, \u0026#34;examples.mjs\u0026#34;] [NOTE] Place the Node.js examples below into examples.mjs.\nInitialise the S3Client Start by initialising the S3Client by importing the required package:\nimport { S3Client } from \u0026#39;@aws-sdk/client-s3\u0026#39;; // Retrieve constants from environment variables const S3_ENDPOINT = process.env.S3_ENDPOINT; const REGION = process.env.REGION; const BUCKET_NAME = process.env.BUCKET_NAME; // Initialise client const client = new S3Client({ endpoint: S3_ENDPOINT, region: REGION, }); Sending Commands with the S3Client Now, let\u0026rsquo;s use the S3Client to create an object in a bucket. For this type of action use the PutObjectCommand:\nCreate an Object import { S3Client, PutObjectCommand } from \u0026#39;@aws-sdk/client-s3\u0026#39;; // ... // Create an object (testObject.txt) prefixed with a directory (test-dir/) in a bucket try { await client.send( new PutObjectCommand({ Bucket: BUCKET_NAME, Key: \u0026#39;test-dir/testObject.txt\u0026#39;, Body: \u0026#39;Uploaded test object.\u0026#39; }) ); console.log(`Successfully created object \u0026#39;test-dir/testObject.txt\u0026#39; in the bucket \u0026#39;${BUCKET_NAME}\u0026#39;`); } catch (err) { console.error(`An error occurred creating object \u0026#39;test-dir/testObject.txt\u0026#39; in the bucket \u0026#39;${BUCKET_NAME}\u0026#39;: ${err}`); }; Let\u0026rsquo;s see if it works! \u0026#x1f604; Build and run the container image:\ndocker build -t aws-sdk-s3:nodejs . # The aws-sdk looks for the environment variables # AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY automatically docker run --rm --name aws-sdk-s3 \\ -e S3_ENDPOINT=\u0026#34;https://cas.cor00005.ukcloud.com\u0026#34; \\ -e REGION=\u0026#34;cor00005\u0026#34; \\ -e BUCKET_NAME=\u0026#34;db-bucket\u0026#34; \\ -e AWS_ACCESS_KEY_ID=\u0026#34;...\u0026#34; \\ -e AWS_SECRET_ACCESS_KEY=\u0026#34;...\u0026#34; \\ aws-sdk-s3:nodejs # Output Successfully created object \u0026#39;test-dir/testObject.txt\u0026#39; in the bucket \u0026#39;db-bucket\u0026#39; \u0026#x1f631; It worked! Congrats! You\u0026rsquo;ve just created your first object in a bucket! 😎\nList and Get Objects Another action is to list objects in a bucket and get them. Here is an example I wrote for that:\nimport { S3Client, PutObjectCommand, ListObjectsCommand, GetObjectCommand } from \u0026#39;@aws-sdk/client-s3\u0026#39;; // ... // List of all objects (max 1000) in a bucket try { var objects = await client.send( new ListObjectsCommand({ Bucket: BUCKET_NAME }) ); } catch (err) { console.error(`An error occurred listing objects for the bucket \u0026#39;${BUCKET_NAME}\u0026#39;: ${err}`); }; // Iterate over objects in the bucket and perform GetObjectCommand try { for (const [index, object] of objects.Contents.entries()) { console.log(`${index} - Getting object: ${object.Key}`); const resp = await client.send( new GetObjectCommand({ Bucket: BUCKET_NAME, Key: object.Key }) ); console.log(`${index} - Object: ${object.Key}, ETag: ${resp.ETag}`); }; } catch (err) { console.error(`An error occurred getting objects for the bucket \u0026#39;${BUCKET_NAME}\u0026#39;: ${err}`); }; Upload and Download Objects Uploading and downloading objects is done often, so below are examples I wrote for this:\n// ... import fs from \u0026#39;fs\u0026#39;; // ... // Create a file to be upload fs.writeFileSync(\u0026#39;/usr/src/app/uploadfile.txt\u0026#39;, \u0026#39;Upload me!\u0026#39;, (_) =\u0026gt; { console.log(\u0026#34;Successfully wrote file \u0026#39;/usr/src/app/uploadfile.txt\u0026#39;\u0026#34;); }); // Upload an object (file) to a bucket try { // Load the contents of \u0026#39;uploadfile.txt\u0026#39; fs.readFile(\u0026#39;/usr/src/app/uploadfile.txt\u0026#39;, \u0026#39;utf8\u0026#39;, async function (_, data) { await client.send( new PutObjectCommand({ Bucket: BUCKET_NAME, Key: \u0026#39;uploadfile.txt\u0026#39;, Body: data }) ) console.log(`Successfully uploaded object \u0026#39;/usr/src/app/uploadfile.txt\u0026#39; to the bucket \u0026#39;${BUCKET_NAME}\u0026#39;`); }); } catch (err) { console.error(`An error occurred uploading object \u0026#39;/usr/src/app/uploadfile.txt\u0026#39; to the bucket \u0026#39;${BUCKET_NAME}\u0026#39;: ${err}`); }; // Download an object (file) from a bucket try { const downloadFile = await client.send( new GetObjectCommand({ Bucket: BUCKET_NAME, Key: \u0026#39;test-dir/testObject.txt\u0026#39; }) ); // Create a write stream const writeStream = fs.createWriteStream(\u0026#39;/usr/src/app/testObject.txt\u0026#39;); // Pipe the object\u0026#39;s body to the write stream downloadFile.Body.pipe(writeStream); console.log(`Successfully downloaded object \u0026#39;testObject.txt\u0026#39; from the bucket \u0026#39;${BUCKET_NAME}\u0026#39;`); } catch (err) { console.error(`An error occurred downloading object \u0026#39;test-dir/testObject.txt\u0026#39; from the bucket \u0026#39;${BUCKET_NAME}\u0026#39;: ${err}`); }; Generate a Public URL for an Object The final example I want to share is generating a public URL for an object so it can be downloaded. The example below generates a URL valid for one hour:\n// ... import { getSignedUrl } from \u0026#34;@aws-sdk/s3-request-presigner\u0026#34;; /// ... // Get a public URL for an object in a bucket // Valid for 1 hour try { const command = new GetObjectCommand({ Bucket: BUCKET_NAME, Key: \u0026#39;uploadfile.txt\u0026#39; }); // Get public URL for the object const publicUrl = await getSignedUrl(client, command, { expiresIn: 3600 }); console.log(`Successfully generated public URL for object \u0026#39;test-dir/testObject.txt\u0026#39; in the bucket \u0026#39;${BUCKET_NAME}\u0026#39;: ${publicUrl}`); } catch (err) { console.error(`An error occurred generating public URL for object \u0026#39;test-dir/testObject.txt\u0026#39; in the bucket \u0026#39;${BUCKET_NAME}\u0026#39;: ${err}`); }; The examples above and others that I wrote can be found on GitHub here.\nI hope you learned something from this blog post! Until next time! \u0026#x1f604; \u0026#x1f44b;\nReferences https://docs.ukcloud.com/articles/cloud-storage/cs-gs.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/uploading-downloading-objects.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/object-keys.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://dbren.uk/blog/aws-sdk-s3/","summary":"\u003cp\u003eRecently at work, a customer raised a ticket about programmatically interacting with \u003ca href=\"https://docs.ukcloud.com/articles/cloud-storage/cs-gs.html\"\u003eUKCloud\u0026rsquo;s Cloud Storage service\u003c/a\u003e.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eUKCloud\u0026rsquo;s Cloud Storage service is an object storage solution based on Dell EMC Elastic Cloud Storage (ECS). Access is via a RESTful API, which also provides support for Amazon\u0026rsquo;s S3 API.\u003c/p\u003e\n\u003cp\u003e\u003c!-- raw HTML omitted --\u003eUKCloud: Getting Started Guide for Cloud Storage \u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e\u003c!-- raw HTML omitted --\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eThis ticket was interesting as the customer was using the \u003ca href=\"https://docs.aws.amazon.com/AWSJavaScriptSDK/v3/latest/clients/client-s3/index.html\"\u003e@aws-sdk/client-s3\u003c/a\u003e JavaScript package to upload images to the service. Prior to this ticket, I hadn\u0026rsquo;t used this package before or have much experience with S3 object storage and programmatically interacting with it. In this blog post, I want to share my learnings and provide a couple of examples in Node.js for interacting with UKCloud\u0026rsquo;s Cloud Storage service using this package.\u003c/p\u003e","title":"Interacting with the ECS S3 API using the aws-sdk/client-s3 package"},{"content":"Hey there! \u0026#x1f44b;\nIn this blog post, I will be showing you how to use Cloudflare as a Dynamic DNS (DDNS) provider.\nWhat is DDNS? Dynamic DNS (DDNS) is a service that keeps the Domain Name System (DNS) updated with a web property’s correct IP address.\nCloudflare glossary: Dynamic DNS 1\nEssentially, DDNS allows you to automatically update your domain\u0026rsquo;s DNS records when a change is detected to your home\u0026rsquo;s public IP address.\nDDNS Use Case Many Internet Service Providers (ISPs) do not provide a static IP address with their regular consumer plans. This often causes issues for those of us that enjoy self-hosting applications from home.\nSay you\u0026rsquo;re self-hosting some applications from home and have DNS records pointing to your home\u0026rsquo;s public IP address. What if your router suddenly rebooted due to a software or hardware issue?\nYour ISP may assign you with a new public IP address causing your applications to be inaccessible \u0026#x1f630; This is an example of where DDNS could help.\nWith DDNS, you can ensure your DNS records are automatically kept up to date when your home\u0026rsquo;s public IP address changes \u0026#x1f44d;\nOther DDNS providers Cloudflare is not the only option for DDNS. There are many others including:\nDuckDNS\nNoIP\nFreeDNS\nHowever, for this blog post, we are obviously focusing on using Cloudflare \u0026#x1f604;\nUsing Cloudflare as a DDNS provider We will be using joshuaavalon\u0026rsquo;s docker-cloudflare container to use Cloudflare as a DDNS provider.\nThere are many Cloudflare DDNS containers out there. The reason I use this one is because:\nIt\u0026rsquo;s multi-architecture meaning the container image can run on amd64, ARM/v6, ARM/v7 and ARM64 based devices.\nARM FTW! 🥧 It has a minimal configuration file supporting YAML, JSON or JavaScript. Configuration via environment variables is also supported but is considered \u0026ldquo;legacy\u0026rdquo;.\nIt provides advanced configuration options such as using an IPv(4|6) lookup service of your choice, and Webhooks to notify when DNS record updates run, succeed or fail.\nPrerequisites A Cloudflare account and domain added to Cloudflare.\nDocker installed on the device which can reach the Cloudflare API: https://api.cloudflare.com/client/v4/\nA Cloudflare API token. Please follow the instructions here.\nMake sure you generate an API Token and not a Global API Key!\nUsing the docker-cloudflare container The minimal configuration file required for the docker-cloudflare container is the following:\n# config.yaml auth: # Provide your API token here! scopedToken: QPExdfoNLwndJPDbt4nK9-yF1z_srC8D0m6-Gv_h domains: # Remember to change this to your domain! - name: ddns.yourdomain.com # The type of record that is created type: A # Determines the proxy status for the record proxied: true # If the record does not exist, create it create: true # zoneId could also be yourdomain.com if the Cloudflare API token is granted #zone:read permissions zoneId: JBFRZWzhTKtRFWgu3X7f4YLX The container will update your DNS record with your public IP address every 5 minutes.\nOnce you have your configuration file, use the following command to start the docker-cloudflare container:\ndocker run --name cloudflare-ddns -d -v ./config.yaml:/app/config.yaml joshava/cloudflare-ddns\nOnce started, you should see something similar to the following when running docker logs cloudflare-ddns:\n[cont-init.d] executing container initialization scripts... [cont-init.d] 10-adduser: executing... usermod: no changes Initializing container User uid: 1001 User gid: 1001 [cont-init.d] 10-adduser: exited 0. [cont-init.d] 11-cron: executing... Setting crontab to */5 * * * * [cont-init.d] 11-cron: exited 0. [cont-init.d] done. [services.d] starting services [services.d] done. 2021-09-16T18:02:57.903Z [info] Cloudflare DDNS start 2021-09-16T18:03:00.026Z [info] Skipped updating. 2021-09-16T18:03:00.027Z [info] Updated ddns.yourdomain.com with \u0026lt;your public IP\u0026gt; 2021-09-16T18:03:00.028Z [info] Cloudflare DDNS end Congratulations, you\u0026rsquo;re now using Cloudflare as a DDNS provider! \u0026#x1f604; \u0026#x2b50;\nBonus - Using a Discord Webhook for updates A few months ago I looked into using Discord webhooks to receive updates for when my DNS records were updated by the container.\nI submitted a PR because I wanted to send a specific message along with the webhook payload. After discussion with the author, a webhook formatter was added which I tested using a Discord webhook.\nFirstly, you need to create a Discord webhook for a channel on your server. Once created, provide the webhook URL in the config below.\nTo use the formatter, you need to use a JavaScript configuration file similar to the following:\n// config.js const formatter = (status, data) =\u0026gt; { if (status === \u0026#34;run\u0026#34;) { return { content: \u0026#34;Updating DNS record.\u0026#34; }; } else { return { content: JSON.stringify(data) }; } }; const config = { auth: { scopedToken: \u0026#34;QPExdfoNLwndJPDbt4nK9-yF1z_srC8D0m6-Gv_h\u0026#34; }, domains: [ { // Remember to change to your domain! name: \u0026#34;ddns.yourdomain.com\u0026#34;, type: \u0026#34;A\u0026#34;, proxied: true, create: true, // Remember to change the zone ID! zoneId: \u0026#34;JBFRZWzhTKtRFWgu3X7f4YLX\u0026#34;, webhook: { // Make sure you edit the webhook URL below to your own! run: \u0026#34;https://discord.com/api/webhooks/111111233445566678/py-D6zAc4IolXBoA7gslLAJc0WKO3KPU1eOxSNzX6qlkCBsqIP8EGILj-ALraivIbs6n\u0026#34;, success: \u0026#34;https://discord.com/api/webhooks/111111233445566678/py-D6zAc4IolXBoA7gslLAJc0WKO3KPU1eOxSNzX6qlkCBsqIP8EGILj-ALraivIbs6n\u0026#34;, failure: \u0026#34;https://discord.com/api/webhooks/111111233445566678/py-D6zAc4IolXBoA7gslLAJc0WKO3KPU1eOxSNzX6qlkCBsqIP8EGILj-ALraivIbs6n\u0026#34;, formatter } } ] }; module.exports = config; As the configuration file has changed from YAML to JavaScript, the docker run command used before is slightly different: docker run --name cloudflare-ddns -d -v ./config.js:/app/config.js joshava/cloudflare-ddns\nOnce running, you should see messages appearing in the Discord channel when a DNS record update occurs!\nI hope you found this blog post useful! Until next time! \u0026#x1f604;\nReferences https://www.cloudflare.com/learning/dns/glossary/dynamic-dns/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://dbren.uk/blog/cloudflare-ddns/","summary":"\u003cp\u003eHey there! \u0026#x1f44b;\u003c/p\u003e\n\u003cp\u003eIn this blog post, I will be showing you how to use Cloudflare as a Dynamic DNS (DDNS) provider.\u003c/p\u003e\n\u003ch2 id=\"what-is-ddns\"\u003eWhat is DDNS?\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003eDynamic DNS (DDNS) is a service that keeps the Domain Name System (DNS) updated with a web property’s correct IP address.\u003c/p\u003e\n\u003cp\u003e\u003c!-- raw HTML omitted --\u003eCloudflare glossary: Dynamic DNS \u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e\u003c!-- raw HTML omitted --\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eEssentially, DDNS allows you to automatically update your domain\u0026rsquo;s DNS records when a change is detected to your home\u0026rsquo;s public IP address.\u003c/p\u003e","title":"Using Cloudflare as a Dynamic DNS (DDNS) provider"},{"content":"Welcome \u0026#x1f44b;\nIn this blog post, I will be showing you how to create a blog for free using GitHub Pages and Hugo.\nI will walk you through the process of creating the GitHub repository (where your blog will live), creating your Hugo site, adding a theme to your blog \u0026#x1f3a8;, creating your first blog post and automating the publishing process!\nWhat is GitHub Pages and Hugo? GitHub Pages allows you to create a website which is hosted directly from a repository on GitHub.\nHugo is a fast and highly customisable static site generator.\nPrerequisites You\u0026rsquo;re going to need a couple of things before you start creating your blog:\nA GitHub account.\nGit.\nFollow the instructions for installing Git, setting up your username, commit email address. Hugo.\nI recommend installing the extended version of Hugo as some themes require it.\nFollow the instructions for Windows or Linux.\nMost likely your operating system will be 64-bit architecture. So you would download: hugo_extended_{version}_Windows-64bit.zip\nIf you have choco installed, run the following command to install Hugo: choco install hugo-extended -y\nStep 1 - Creating and cloning the GitHub repository Create a new public GitHub repository named username.github.io. Where username is your GitHub username. For example, if your GitHub username was bumblebee, then you would enter bumblebee.github.io.\nEnter the following command in a terminal to clone the repository to your machine (providing your GitHub username instead of username). Enter your GitHub credentials when prompted: git clone https://github.com/username/username.github.io.git\nNow, change directory and change the default branch to source using the following command: cd username.github.io; git branch -M source\nFor the rest of this post, substitute username for your GitHub username.\nStep 2 - Initalising your Hugo site Run the following command to initalise your site: hugo new site . -f yml --force\nYou should see the following output:\nCongratulations! Your new Hugo site is created in /path/to/your/hugo/site/username.github.io.\nStep 3 - Adding and configuring a site theme 🎨 Next, go to https://themes.gohugo.io/ and browse the list of themes that are available. There are a lot\u0026hellip; \u0026#x1f604;\nMany themes provide a demo so you can see for yourself whether you like a theme. Furthermore, many themes have their own configuration so make sure you read the documentation.\nFor this blog post, I\u0026rsquo;m going to use the Tania theme.\nFollowing the installation instructions, run the command: git submodule add https://github.com/WingLim/hugo-tania themes/hugo-tania to install the theme.\nIn the site\u0026rsquo;s root directory (username.github.io), open the config.yml file and paste the Tania theme\u0026rsquo;s configuration. Edit it to your liking and save the file.\nNOTE: Make sure you edit baseurl: \u0026quot;https://example.com\u0026quot; to baseurl: \u0026quot;https://username.github.io\u0026quot; - Remembering to substitute username for your GitHub username!\nAdditional configuration for the Tania theme can be found here.\nThe Tania theme also requires an articles.md file to be created if you want to have an archive page of blog posts as shown on the demo site.\nCreate the articles.md file inside the site\u0026rsquo;s content directory (username.github.io/content), paste the following block into it and save the file:\n--- title: Articles # Edit subtitle and date! subtitle: Posts, tutorials, snippets, musings, and everything else. date: 2020-11-26 type: section layout: \u0026#34;archives\u0026#34; --- Step 4 - Creating your first blog post \u0026#x270f;\u0026#xfe0f; Now it\u0026rsquo;s time to create your first blog post!\nRun the following command to create the blog post file (changing your-blog-post to the name of the post): hugo new post/your-blog-post.md\nYou should see the following output verifying that a markdown file was created for the post: username.github.io/content/post/your-blog-post.md created.\nNext, open the file, write your post\u0026rsquo;s content and save the file.\nTo see your site rendered run the command: hugo server -D and paste the following URL into your browser: http://localhost:1313/\nNOTE: Once you have finished writing your post, set draft: true at the top of the file to draft: false\nStep 5 - Automating the publishing process ⚙️ To automate the publishing process of your blog, create a gh-pages.yml file located at: username.github.io/.github/workflows/gh-pages.yml, paste the following block into the file and save the file:\nname: Github Pages on: push: branches: - source jobs: deploy: runs-on: ubuntu-20.04 concurrency: group: ${{ github.workflow }}-${{ github.ref }} steps: - name: Checkout uses: actions/checkout@v2 with: submodules: true # Fetch Hugo themes (true OR recursive) fetch-depth: 0 # Fetch all history for .GitInfo and .Lastmod - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: \u0026#39;0.85.0\u0026#39; extended: true - name: Build run: hugo --minify - name: Deploy uses: peaceiris/actions-gh-pages@v3 if: ${{ github.ref == \u0026#39;refs/heads/source\u0026#39; }} with: github_token: ${{ secrets.GITHUB_TOKEN }} publish_branch: main publish_dir: ./public Step 6 - Push the site\u0026rsquo;s content to the GitHub repository Finally, you need to push all the site\u0026rsquo;s content to the GitHub repository. To do this, run the following commands from the site\u0026rsquo;s root directory (username.github.io):\nStage all of the site\u0026rsquo;s content ready to commit: git add -A\nCreate a commit message to go along with the site\u0026rsquo;s content: git commit -m \u0026quot;Publishing my first blog post.\u0026quot;\nPush the commit to the source branch: git push -u origin source\nStep 7 - Configuring GitHub Pages for your GitHub repository Go to https://github.com/username/username.github.io/settings/pages and perform the following steps:\nSet the source branch to main.\nSet the source folder to / (root).\nPress Save.\nEnable Enforce HTTPS so your blog is served over HTTPS.\nIf all goes well the automated workflow will trigger and your site will be published at: https://username.github.io \u0026#x1f389;\nEnjoy! \u0026#x1f604;\n","permalink":"https://dbren.uk/blog/blog-github-pages-hugo/","summary":"\u003cp\u003eWelcome \u0026#x1f44b;\u003c/p\u003e\n\u003cp\u003eIn this blog post, I will be showing you how to create a blog for \u003cstrong\u003efree\u003c/strong\u003e using GitHub Pages and Hugo.\u003c/p\u003e\n\u003cp\u003eI will walk you through the process of creating the GitHub repository (where your blog will live), creating your Hugo site, adding a theme to your blog \u0026#x1f3a8;, creating your first blog post and automating the publishing process!\u003c/p\u003e\n\u003ch1 id=\"what-is-github-pages-and-hugo\"\u003eWhat is GitHub Pages and Hugo?\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://pages.github.com/\"\u003eGitHub Pages\u003c/a\u003e allows you to create a website which is hosted directly from a repository on GitHub.\u003c/p\u003e","title":"Create a blog for free using GitHub Pages and Hugo"},{"content":"Hi! \u0026#x1f44b;\nI had some free time recently so I decided to develop something which was relevant to current worldwide situation. The ongoing Coronavirus (COVID-19) pandemic.\nThis is going to be a short blog post about the application, its aims and the process of deploying it to Azure App Service.\nAims I had three aims that I wanted to achieve:\nI wanted to develop an application which was simple.\nI wanted to show data about how the COVID-19 pandemic is developing in the UK.\nI wanted to show relevant news related to COVID-19 in the UK.\nUK-COVID-19-Stats Python application The application is open source on Github and developed in Python. Check it out here! \u0026#x1f604;\nThe application uses a number of libraries including:\nFlask: I have had some previous experience with this web application framework for Python. It is well documented and this project is relatively small, so it made sense to pick Flask for this application.\nUK-COVID-19: This library is maintained by the folks over at Public Health England (PHE) and allows my application to retrieve UK based data about the COVID-19 outbreak. More on this below.\nLoguru: This is personal preference but I like how this library makes Python logging hassle free and is very easy to integrate into my application.\nFeedparser: This library allows my application to parse BBC\u0026rsquo;s health news RSS feed and show COVID-19 related articles in my application. This can be seen on this line in app.py.\nApplication datasource I began looking for a potential datasource for my application to retrieve UK based data for the COVID-19 outbreak.\nI came across the UK COVID-19 dashboard which provides an application programming interface (API) and Python software development kit (SDK) (mentioned above) developed by PHE. The API is free (Thanks PHE! \u0026#x2764;\u0026#xfe0f;) and provides loads of data including (but not limited to):\nNew cases.\nHospital cases.\nAdmissions.\nSubstantial testing data (including data for each pillar).\nDeaths.\nObviously, it was a perfect choice for the project \u0026#x1f604;\nFor more information on the API, you can look at the documentation yourself here.\nRunning locally You can run the application locally and test it out for yourself if you have docker installed.\nFirst, build the image from the dockerfile in the repository and then run the image. You can access the app at http://localhost:5000.\nMore detailed instructions on how to do this can be found here.\nDeploying to Azure App Service I wanted to host the application somewhere for free. Luckily for me, Microsoft\u0026rsquo;s Azure App Service has a free tier instance (SKU: F1) which allowed me to host my application free of charge! \u0026#x2764;\u0026#xfe0f; \u0026#x1f604;\nI deployed the application onto Azure App Service by using the Azure command-line interface (CLI). It was super easy and involved the following steps:\nInstall the Azure CLI.\nI\u0026rsquo;m using Windows so I used chocolatey to install it using the following command: choco install azure-cli -y Log in to my Azure account using the command: az login.\nFrom my project repository, run the following command: az webapp up --sku F1 --name \u0026lt;my-app-name\u0026gt;.\nThats it! The Azure CLI did all of the heavy lifting creating a resource group for my application in Azure, creating an App Service plan and the app service object, zipping up my application and deploying it! \u0026#x1f604;\nYou can access a live version of the application by heading to https://uk-covid-19-stats.azurewebsites.net.\nFor more information on the steps above, see the following Microsoft documentation.\nOverall, this was a fun little project which I believe met the aims I set out to achieve. I also ended up learning how to deploy a Python application to Azure App Service \u0026#x1f603;\nThanks for reading and I hope you enjoyed reading this short blog post. Until next time! \u0026#x1f44b;\n","permalink":"https://dbren.uk/blog/covid-19-app/","summary":"\u003cp\u003eHi! \u0026#x1f44b;\u003c/p\u003e\n\u003cp\u003eI had some free time recently so I decided to develop something which was relevant to current worldwide situation. The ongoing Coronavirus (COVID-19) pandemic.\u003c/p\u003e\n\u003cp\u003eThis is going to be a short blog post about the application, its aims and the process of deploying it to Azure App Service.\u003c/p\u003e\n\u003ch1 id=\"aims\"\u003eAims\u003c/h1\u003e\n\u003cp\u003eI had three aims that I wanted to achieve:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eI wanted to develop an application which was simple.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eI wanted to show data about how the COVID-19 pandemic is developing in the UK.\u003c/p\u003e","title":"UK COVID-19 stats application"},{"content":"Caddy version 2.0.0 released on May 4th 2020. Caddy brands itself as \u0026ldquo;The Ultimate Server\u0026rdquo; with functionality including a web server, reverse proxy, automatic certificate renewal, automatic HTTPS and more!\nI have to say, it\u0026rsquo;s pretty awesome! \u0026#x1f44d;\nThis blog post will show you how to use the Caddy v2 reverse proxy feature with a domain managed with Cloudflare. So, lets jump in! \u0026#x1f604;\nCaddyfile Caddy uses a Caddyfile for it\u0026rsquo;s configuration.\nTwo main Caddyfile concepts to understand are blocks and directives.\nBlocks A Caddyfile block contains configuration for a site. Blocks are declared using curly braces:\nexample.com { ... } One unique block worth mentioning is the global options block. This block must be defined at the top of the Caddyfile and allows you to modify options which apply globally to Caddy. Two usage examples for this block are altering the acme_ca (ACME CA\u0026rsquo;s directory) to the Let\u0026rsquo;s Encrypt staging endpoint or the email option which is used when creating the ACME account.\nTo use the global options block along with a site block, your Caddyfile would look similar to the following:\n{ email example@example.com # This is a valid comment in a Caddyfile :-) # This acme_ca option tells Caddy to use the Let\u0026#39;s Encrypt staging endpoint # Remove when transitioning to a production environment acme_ca https://acme-staging-v02.api.letsencrypt.org/directory } # Site block below example.com { ... } Directives and Subdirectives Caddyfile directives customise how a site is served by Caddy and must be declared within a site block. A subdirective provides additional configuration for a directive. See the Caddy documentation for a full list of directives and their respective subdirectives.\nThe directive we are interested in this blog post is the reverse_proxy directive.\nAdding to the previous example, usage of the reverse_proxy directive is as follows:\n{ email example@example.com # This acme_ca option tells Caddy to use the Let\u0026#39;s Encrypt staging endpoint # Remove when transitioning to a production environment acme_ca https://acme-staging-v02.api.letsencrypt.org/directory } # Site block below example.com { # Using the reverse_proxy directive within a site block reverse_proxy example:80 } Using the Caddy v2 Docker container with a Cloudflare managed domain Right, enough of the boring theory! Onto an actual example.\nI have created a Github repository which uses docker-compose to deploy the Caddy v2 container (including the Cloudflare module) and freshrss as an example application.\nWe will be using the DNS-01 challenge type to request a certificate for the subdomain freshrss under your Cloudflare managed apex domain.\nPrerequisites A domain managed with Cloudflare\nSSL/TLS encryption mode set the Full (strict)\nThis ensures that clients don\u0026rsquo;t encounter infinite redirects, they connect via HTTPS and ensures the Let\u0026rsquo;s Encrypt certificate on the server is valid. A server with ports 80 and 443 accessible to the internet\nNOTE: The DNS-01 challenge type doesn\u0026rsquo;t require any open ports however, in order to access any applications from the internet, we need these ports exposed. A server with Docker, docker-compose and git installed\nAn A record configured in Cloudflare with the following values:\nType: A\nName: freshrss\nContent: The IPv4 address of your server\nTTL: Auto\nProxy Status: Proxied\nA Cloudflare API token (NOT an API key!) with the following permissions:\nZone / Zone / Read\nZone / DNS / Edit\nDeployment Clone the caddy-cloudflare-docker-compose repository and change directory: git clone https://github.com/dbrennand/caddy-cloudflare-docker-compose.git; cd caddy-cloudflare-docker-compose.\nMake a copy of the ExampleCaddyfile called Caddyfile: cp ExampleCaddyfile Caddyfile.\nModify the following lines in the Caddyfile:\nemail example@example.com: Alter example@example.com to your email address.\nsubdomain.example.com: Modify this to be your domain. For example: freshrss.yourdomain.com.\nNOTE: I have provided some snippets and comments in the ExampleCaddyfile which I hope you will find useful.\nModify the CLOUDFLARE_API_TOKEN environment variable in the docker-compose.yaml file with your Cloudflare API token: environment: CLOUDFLARE_API_TOKEN: \u0026#34;Insert your Cloudflare API token here.\u0026#34; Modify the PUID, PGID and TZ environment variables in the docker-compose.yaml file (if required).\nStart the Caddy and freshrss containers using: docker-compose up -d.\nView the Caddy container logs using: docker logs caddy. If everything is configured correctly, you will see something similar to the following output in the Caddy container logs:\n[INFO] [freshrss.yourdomain.com] acme: use dns-01 solver [INFO] [freshrss.yourdomain.com] acme: Preparing to solve DNS-01 [INFO] [freshrss.yourdomain.com] acme: Trying to solve DNS-01 [INFO] [freshrss.yourdomain.com] acme: Checking DNS record propagation using [127.0.0.11:53] [INFO] Wait for propagation [timeout: 1m0s, interval: 2s] [INFO] [freshrss.yourdomain.com] acme: Waiting for DNS record propagation. [INFO] [freshrss.yourdomain.com] The server validated our request [INFO] [freshrss.yourdomain.com] acme: Cleaning DNS-01 challenge [INFO] [freshrss.yourdomain.com] acme: Validations succeeded; requesting certificates [INFO] [freshrss.yourdomain.com] Server responded with a certificate. [INFO][freshrss.yourdomain.com] Certificate obtained successfully [INFO][freshrss.yourdomain.com] Obtain: Releasing lock \u0026#x2b50; Congratulations! You have successfully configured Caddy to obtain a valid certificate from Let\u0026rsquo;s Encrypt for the subdomain freshrss under your Cloudflare apex domain \u0026#x1f604;\nNavigate to freshrss.yourdomain.com in your browser and you will see the setup wizard for freshrss!\nIn my next blog post, I will cover how you can use Cloudflare as a Dynamic DNS (DDNS) provider.\n","permalink":"https://dbren.uk/blog/caddy-cloudflare/","summary":"\u003cp\u003e\u003ca href=\"https://caddyserver.com/\"\u003eCaddy\u003c/a\u003e version \u003ca href=\"https://github.com/caddyserver/caddy/releases/tag/v2.0.0\"\u003e2.0.0\u003c/a\u003e released on May 4th 2020. Caddy brands itself as \u0026ldquo;The Ultimate Server\u0026rdquo; with \u003ca href=\"https://caddyserver.com/docs/\"\u003efunctionality\u003c/a\u003e including a web server, reverse proxy, automatic certificate renewal, automatic HTTPS and more!\u003c/p\u003e\n\u003cp\u003eI have to say, it\u0026rsquo;s pretty awesome! \u0026#x1f44d;\u003c/p\u003e\n\u003cp\u003eThis blog post will show you how to use the Caddy v2 reverse proxy feature with a domain managed with Cloudflare. So, lets jump in! \u0026#x1f604;\u003c/p\u003e\n\u003ch2 id=\"caddyfile\"\u003eCaddyfile\u003c/h2\u003e\n\u003cp\u003eCaddy uses a \u003ca href=\"https://caddyserver.com/docs/caddyfile\"\u003eCaddyfile\u003c/a\u003e for it\u0026rsquo;s configuration.\u003c/p\u003e\n\u003cp\u003eTwo main Caddyfile concepts to understand are \u003ca href=\"https://caddyserver.com/docs/caddyfile/concepts#blocks\"\u003e\u003cstrong\u003eblocks\u003c/strong\u003e\u003c/a\u003e and \u003ca href=\"https://caddyserver.com/docs/caddyfile/concepts#directives\"\u003e\u003cstrong\u003edirectives\u003c/strong\u003e\u003c/a\u003e.\u003c/p\u003e","title":"Using Caddy v2's reverse proxy feature with a domain managed with Cloudflare!"}]