[{"content":"And we\u0026rsquo;re back! üëã\nIn this blog post we will be looking at building and publishing a container image to GitHub Packages using GitHub Actions!\nI recently revisited an old project of mine; now called speeder. It\u0026rsquo;s a Python script to monitor your internet speed and send the results to InfluxDB. The results can then be visualised in Grafana. I originally created this script during the Coronavirus lockdowns whilst internet usage was high and essential for work. Since that time, I\u0026rsquo;ve learned a lot about CI/CD and noticed I hadn\u0026rsquo;t automated the build and publish process of the container image! üò¨ Time to fix that! üë∑\nWhat is CI/CD? Before we get started, let\u0026rsquo;s take a quick look at what CI/CD is. According to ChatGPT:\n Continuous Integration (CI) is the practice of frequently and automatically integrating code changes from multiple developers, leading to early issue detection and smoother teamwork. Continuous Delivery (CD) extends CI by automating the deployment process, allowing for faster and more reliable software releases. The benefits of CI/CD include quicker development cycles, higher software quality, reduced errors, and more frequent and reliable software updates.\n Thanks ChatGPT! ü§ñ\nIn short, CI/CD helps developers to automate the process of building, testing and deploying software. It reduces the time and effort required to complete these processes, and helps ensure software is built and tested consistently and reliably.\nBuild and Publish a Container Image using GitHub Actions Begin by creating the GitHub Actions workflow directory .github/workflows in your project:\nmkdir -pv .github/workflows Next, create a .github/workflows/build.yml file with the following content:\nname: Build on: push: tags: - \u0026#39;v*\u0026#39; env: REGISTRY: ghcr.io # https://docs.github.com/en/actions/learn-github-actions/contexts#github-context IMAGE_NAME: ${{ github.repository }} jobs: build-and-push-image: runs-on: ubuntu-latest # Sets the permissions granted to the `GITHUB_TOKEN` for the actions in this job permissions: contents: read packages: write steps: - name: Checkout repository uses: actions/checkout@v3 - name: Log in to the Container registry uses: docker/login-action@v2 with: registry: ${{ env.REGISTRY }} username: ${{ github.actor }} # https://docs.github.com/en/actions/security-guides/automatic-token-authentication password: ${{ secrets.GITHUB_TOKEN }} - name: Extract metadata (tags, labels) for Docker id: meta uses: docker/metadata-action@v4 with: images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }} - name: Build and push Docker image uses: docker/build-push-action@v4 with: context: . push: true tags: ${{ steps.meta.outputs.tags }} labels: ${{ steps.meta.outputs.labels }} The above GitHub Actions workflow will perform the following steps üìù:\n Checkout the repository. Login to the GitHub Packages (ghcr.io) container registry. Use git to extract repository metadata to be used for the container image tag(s) and labels. Build the container image and push it to the GitHub Packages container registry.  Now, commit and push the build.yml workflow:\ngit add .github/workflows/build.yml git commit -m \u0026#34;chore: build and publish container image\u0026#34; git push The workflow will only trigger when a push event occurs for tags matching the pattern v*. To trigger the workflow, create a new tag for your repository following the semver convention:\n# Once PR is merged git checkout main git pull git tag v1.0.0 git push --tags And that\u0026rsquo;s it! üéâ Once the tag is pushed the workflow will trigger! Here is an example from my speeder project üôÇ\nClosing Thoughts The best part about this workflow is the docker/metadata-action. It uses git metadata from the repository to create the container image\u0026rsquo;s labels according to the Open Container Initiative image-spec ‚ù§Ô∏è\n‚ùØ skopeo inspect docker://ghcr.io/dbrennand/speeder | jq .Labels { \u0026#34;org.opencontainers.image.created\u0026#34;: \u0026#34;2023-09-07T16:55:29.525Z\u0026#34;, \u0026#34;org.opencontainers.image.description\u0026#34;: \u0026#34;Python script to monitor your internet speed! üöÄ Periodically run librespeed/speedtest-cli and send results to InfluxDB.\u0026#34;, \u0026#34;org.opencontainers.image.licenses\u0026#34;: \u0026#34;MIT\u0026#34;, \u0026#34;org.opencontainers.image.revision\u0026#34;: \u0026#34;1e9dc09c6a95554ecb74f736029ec09c3ba6910c\u0026#34;, \u0026#34;org.opencontainers.image.source\u0026#34;: \u0026#34;https://github.com/dbrennand/speeder\u0026#34;, \u0026#34;org.opencontainers.image.title\u0026#34;: \u0026#34;speeder\u0026#34;, \u0026#34;org.opencontainers.image.url\u0026#34;: \u0026#34;https://github.com/dbrennand/speeder\u0026#34;, \u0026#34;org.opencontainers.image.version\u0026#34;: \u0026#34;v1.1.0\u0026#34; } Overall, this GitHub Actions workflow helps save a lot of time and effort. Furthermore, it helps make sure the container image is built and published consistently and reliably every time! üöÄ\nThat\u0026rsquo;s all folks! Thanks for reading! üëã\nReferenced Documentation  https://docs.github.com/en/actions/publishing-packages/publishing-docker-images#publishing-images-to-github-packages https://github.com/docker/login-action https://github.com/docker/metadata-action https://github.com/docker/build-push-action  ","permalink":"https://danielbrennand.com/blog/build-and-publish-container-image-gha/","summary":"And we\u0026rsquo;re back! üëã\nIn this blog post we will be looking at building and publishing a container image to GitHub Packages using GitHub Actions!\nI recently revisited an old project of mine; now called speeder. It\u0026rsquo;s a Python script to monitor your internet speed and send the results to InfluxDB. The results can then be visualised in Grafana. I originally created this script during the Coronavirus lockdowns whilst internet usage was high and essential for work.","title":"Build and Publish Container Images using GitHub Actions"},{"content":"Time for another blog post! üöÄ\nIn this blog post, I will be discussing how to test Ansible content with Molecule üß™\nWhat is Molecule?  Molecule aids in the development and testing of Ansible content: collections, playbooks and roles.\ngithub.com/ansible-community/molecule1 Why do we need to test Ansible content? Testing is an integral part of the software development lifecycle. It helps identify and prevent bugs from reaching production and in some cases, helps identify performance issues. When creating Ansible content we need to ensure that it works as expected and that we are not introducing any undesired behaviour. This is where Molecule comes in.\nMolecule Terminology üìö Molecule has several terms that are used throughout the documentation. Let\u0026rsquo;s go over them now.\nInstances \u0026amp; Drivers üöó Molecule instances are what your Ansible content is executed against. Instances are created using a driver. Molecule has several drivers for handling the creation and destruction of instances. The drivers are currently located in ansible-community/molecule-plugins repository.\nFor example, the default Docker driver can be used to create a container instance and the Vagrant driver can create a virtual machine instance.\nScenarios üìñ Molecule scenarios can be thought of as a test suite. Each scenario contains its own instances and configuration. For example, a scenario could be used to test an Ansible role against different distributions or test a specific configuration of a role.\nThere should always be a default scenario which is used to test Ansible content with its default configuration.\nMolecule Directory Structure üìÅ A basic Molecule directory structure is:\nmolecule ‚îî‚îÄ‚îÄ default ‚îú‚îÄ‚îÄ prepare.yml ‚îú‚îÄ‚îÄ converge.yml ‚îú‚îÄ‚îÄ verify.yml ‚îî‚îÄ‚îÄ molecule.yml Within the molecule directory is the default scenario directory which contains the prepare.yml, converge.yml and verify.yml playbooks, as well as the molecule.yml configuration file.\nPrepare Playbook ‚ñ∂Ô∏è The prepare.yml playbook defines the preparation tasks to run before the converge.yml playbook. For example, this could be used configure the instance.\nConverge Playbook ‚ñ∂Ô∏è The converge.yml playbook defines the Ansible content to be tested. This can be a playbook, role or collection.\nVerify Playbook ‚ñ∂Ô∏è The verify.yml playbook is executed after the converge.yml playbook. This playbook contains verification tasks to check the Ansible content has been applied correctly.\nConfiguration File molecule.yml üìù The molecule.yml file contains configuration for each Molecule component2:\n   Component Description     Dependency Manager Molecule uses Ansible Galaxy as the default manager for resolving role and collection dependencies.   Driver Instances \u0026amp; Drivers.   Platforms (Instances) Defines instances to be created by the driver for the scenario.   Provisioner Molecule uses Ansible as the provisioner. The provisioner manages the life cycle of instances by communicating with the driver.   Scenario Molecule\u0026rsquo;s default scenario configuration can be overridden for full control over each sequence.   Verifier Molecule uses Ansible as the verifier. The verifier uses the verify.yml playbook to check the state of the instance.    The example below shows a basic configuration using the Docker driver to create a Debian container instance.\n--- dependency: name: galaxy driver: name: docker platforms: - name: instance image: geerlingguy/docker-debian10-ansible:latest pre_build_image: true provisioner: name: ansible verifier: name: ansible Molecule Commands üìú Molecule has several commands for performing different actions. The most common commands are:\n   Molecule Command Description     create Creates the instance defined in the molecule.yml file.   destroy Destroys the instance defined in the molecule.yml file.   login Spawns a shell inside the instance. Useful for troubleshooting.   converge Performs the converge sequence which includes creating the instance and executing the prepare.yml and converge.yml playbooks.   test Performs a full test scenario. This includes the converge and idempotency sequences, executing the verify.yml playbook and destroying the instance.    Now that we have covered the basics, let\u0026rsquo;s test an Ansible playbook with Molecule! üß™\nDemo üì∫ I\u0026rsquo;ve created a demo repository which contains a simple playbook to install nginx on a Debian container instance. You\u0026rsquo;ll need to have Docker and Python installed to use the repository.\nBegin by cloning the repository:\ngit clone https://github.com/dbrennand/molecule-demo.git \u0026amp;\u0026amp; cd molecule-demo Inspect the files and notice the default molecule scenario with the playbooks and molecule.yml configuration file. The converge.yml playbook calls the main playbook.yml two directories above.\nNext, install Molecule and the Docker driver. I recommend using a virtual environment:\nmkdir -pv ~/.virtualenvs python3 -m venv ~/.virtualenvs/molecule-demo source ~/.virtualenvs/molecule-demo/bin/activate pip install -r requirements.txt Now we can run the molecule test command to perform a full test scenario:\nmolecule test The output should look similar to the following:\nINFO Running default \u0026gt; create PLAY [Create] ****************************************************************** TASK [Create molecule instance(s)] ********************************************* changed: [localhost] =\u0026gt; (item=instance) ... INFO Running default \u0026gt; prepare PLAY [Prepare] ***************************************************************** TASK [Gathering Facts] ********************************************************* ok: [instance] TASK [Update apt cache] ******************************************************** changed: [instance] ... INFO Running default \u0026gt; converge PLAY [Converge] **************************************************************** TASK [Gathering Facts] ********************************************************* ok: [instance] PLAY [Playbook | Install nginx] ************************************************ TASK [Gathering Facts] ********************************************************* ok: [instance] TASK [Install nginx] *********************************************************** changed: [instance] PLAY RECAP ********************************************************************* instance : ok=3 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 INFO Running default \u0026gt; idempotence PLAY [Converge] **************************************************************** TASK [Gathering Facts] ********************************************************* ok: [instance] PLAY [Playbook | Install nginx] ************************************************ TASK [Gathering Facts] ********************************************************* ok: [instance] TASK [Install nginx] *********************************************************** ok: [instance] PLAY RECAP ********************************************************************* instance : ok=3 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 INFO Idempotence completed successfully. INFO Running default \u0026gt; side_effect WARNING Skipping, side effect playbook not configured. INFO Running default \u0026gt; verify INFO Running Ansible Verifier PLAY [Verify] ****************************************************************** TASK [Gathering Facts] ********************************************************* ok: [instance] TASK [Gather package list] ***************************************************** ok: [instance] TASK [Verify nginx is installed] *********************************************** ok: [instance] =\u0026gt; { \u0026#34;changed\u0026#34;: false, \u0026#34;msg\u0026#34;: \u0026#34;All assertions passed\u0026#34; } PLAY RECAP ********************************************************************* instance : ok=3 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 INFO Verifier completed successfully. INFO Running default \u0026gt; cleanup WARNING Skipping, cleanup playbook not configured. INFO Running default \u0026gt; destroy PLAY [Destroy] ***************************************************************** TASK [Wait for instance(s) deletion to complete] ******************************* FAILED - RETRYING: [localhost]: Wait for instance(s) deletion to complete (300 retries left). changed: [localhost] =\u0026gt; (item=instance) PLAY RECAP ********************************************************************* localhost : ok=3 changed=2 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 Tada! You\u0026rsquo;ve just tested an Ansible playbook with Molecule! ‚ú®üéâ\nWe now have peace of mind that our playbook works as expected! üôÇ\nConclusion üìù In this blog post, we covered the basics of using Molecule and how to test Ansible content. I recommend checking out the Molecule documentation for more information.\nI hope you found this post useful and if you have any questions or feedback, feel free to reach out to me on Twitter or via email.\nUntil next time, happy testing! üß™ üëã\nReferences   https://github.com/ansible-community/molecule\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n https://ansible.readthedocs.io/projects/molecule/getting-started/#the-scenario-layout\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://danielbrennand.com/blog/testing-ansible-content/","summary":"Time for another blog post! üöÄ\nIn this blog post, I will be discussing how to test Ansible content with Molecule üß™\nWhat is Molecule?  Molecule aids in the development and testing of Ansible content: collections, playbooks and roles.\ngithub.com/ansible-community/molecule1 Why do we need to test Ansible content? Testing is an integral part of the software development lifecycle. It helps identify and prevent bugs from reaching production and in some cases, helps identify performance issues.","title":"Testing Ansible Content with Molecule"},{"content":"Hi there! üëã\nIn my previous blog post I covered how to get started with Ansible. In this blog post I\u0026rsquo;ll be continuing to write about Ansible and covering how to manage secrets in Ansible playbooks.\nReady\u0026hellip; set\u0026hellip; go! üöÄ\nManaging Secrets in Ansible üîë As with most automation, we need to use credentials to authenticate to our servers and other applications. Examples of secrets include usernames and passwords, API keys, SSH keys, etc. When using these types of secrets in playbooks, we need to store them securely but also still allow Ansible to access them when needed.\nIntroducing Ansible Vault üîê Ansible Vault is a command line tool that is part of the ansible-core package and allows us to encrypt secrets in our playbooks, inventory and vars files. It can encrypt an entire file or a specific variable within a file. Furthermore, Ansible Vault uses the AES256 encryption algorithm to encrypt files and variables using a password provided by the user.\nUsing Ansible Vault Let\u0026rsquo;s take a look at how we can use Ansible Vault to encrypt a file.\nEncrypting a File üîí To encrypt a file use the ansible-vault encrypt command, providing the path for the file to be encrypted. When prompted enter a password to encrypt the file:\nansible-vault encrypt vars/main.yml New Vault password: Confirm New Vault password: Encryption successful Now we can see the file is encrypted:\ncat vars/main.yml $ANSIBLE_VAULT;1.1;AES256 64383033313638636464333937393463663432616532353763646137356664376533326261373265 3466643962633863613061646235333137623266666238650a356163653039616634653632383933 30316438663233316365383637643135633736303435666236356130356565363263353262306564 3931613035373031360a396431626666343564626264346636633266343766316535346266346537 62393334393030386365376237393565343637663766336261313862363936343161 Editing or Viewing an Encrypted File To edit or view the contents of an encrypted file, use the ansible-vault edit or ansible-vault view command respectively:\nansible-vault edit vars/main.yml Vault password: ansible-vault view vars/main.yml Vault password: Decrypting a File üîì To decrypt the file, use the ansible-vault decrypt command and as before, provide the path to the file and the password:\nansible-vault decrypt vars/main.yml Vault password: Next, let\u0026rsquo;s see how we can use Ansible Vault to encrypt a variable.\nEncrypting a Variable To encrypt a variable, use the ansible-vault encrypt_string command providing the name of the variable and the value to encrypt. When prompted, enter a password to encrypt the variable:\nansible-vault encrypt_string -n my_secret_var \u0026#34;\u0026lt;Secret to encrypt here\u0026gt;\u0026#34; New Vault password: Confirm New Vault password: Encryption successful my_secret_var: !vault | $ANSIBLE_VAULT;1.1;AES256 63396563373936313066626338313132343139613636353538633938303135666562326139373534 3962633831613839623663353938393130343036333936340a383830366436323261646137383036 36373738346433366333666632653966323630323237666466326538336535323330663030303762 3436336162363433310a396134663062323431363263393930356237366563306238363463383066 6633 You can now copy and paste the output into your playbook vars section, vars file or inventory.\nGreat! So we now know how to encrypt a file and a variable, but how about a real world example! üåé\nReal World Example I sometimes use Hetzner Cloud to spin up virtual servers for testing. In this short playbook which I use to create the server, there is a variable called hetzner_api_key which contains my API key. I don\u0026rsquo;t want to store this API key in plain text in my playbook! üò± üôÖ‚Äç‚ôÇÔ∏è\n# playbook.yml --- - name: Provision Hetzner VPS hosts: all gather_facts: false vars: hetzner_api_key: \u0026#34;my-secret-api-key\u0026#34; tasks: - name: Create a basic server hetzner.hcloud.hcloud_server: api_token: \u0026#34;{{ hetzner_api_key }}\u0026#34; location: fsn1 name: my-server server_type: cx11 image: ubuntu-18.04 state: present Using Ansible Vault we can encrypt this variable in the playbook:\nansible-vault encrypt_string -n hetzner_api_key \u0026#34;my-secret-api-key\u0026#34; New Vault password: Confirm New Vault password: Encryption successful hetzner_api_key: !vault | $ANSIBLE_VAULT;1.1;AES256 30646134626337346138323439636637366233633261373339666233653735616364623533616539 3133376230313265326631343439383238653539373034650a633632613037653337626237653730 38636136613162386530363530393537323132303538653265633635326236336561633234306562 3061613964343037320a323261363731613130343932393439626133616163346663623933313562 35313765626338653133336436393332636635656361363730363033626335643261 With the variable safely encrypted, place the variable into the playbook:\n# playbook.yml --- - name: Provision Hetzner VPS hosts: all gather_facts: false vars: hetzner_api_key: !vault | $ANSIBLE_VAULT;1.1;AES256 30646134626337346138323439636637366233633261373339666233653735616364623533616539 3133376230313265326631343439383238653539373034650a633632613037653337626237653730 38636136613162386530363530393537323132303538653265633635326236336561633234306562 3061613964343037320a323261363731613130343932393439626133616163346663623933313562 35313765626338653133336436393332636635656361363730363033626335643261 tasks: - name: Create a basic server hetzner.hcloud.hcloud_server: api_token: \u0026#34;{{ hetzner_api_key }}\u0026#34; location: fsn1 name: my-server server_type: cx11 image: ubuntu-18.04 state: present Finally, when running the playbook use the --ask-vault-pass option and enter the password used to encrypt the variable:\nansible-playbook -c local -i localhost, --ask-vault-pass playbook.yml All done! üéâ\nPrompting for the Vault Password Non-Interactively The above example was interactively prompting for the vault password. So how do we do this non-interactively? Say we want to run the playbook in a CI/CD pipeline. We can\u0026rsquo;t prompt for the password in this type of scenario ü§î\nTo solve this, we can use a vault password file.\nFirst, add the password to a file called .vault_password and lock down the permissions::\n# Note the space at the beginning of the command! # This is needed to prevent the password from being stored in the shell history. echo \u0026#34;my-encryption-password\u0026#34; \u0026gt; .vault_password chmod 600 .vault_password Now, we run the playbook with the --vault-password-file option instead:\nansible-playbook -c local -i localhost, --vault-password-file .vault_password playbook.yml Conclusion In this post we have covered how to use Ansible Vault for safely storing secrets for use in our playbooks. We\u0026rsquo;ve covered encrypting files and variables and how to use a vault password file to prompt for the vault password non-interactively.\nI hope you found this post useful and if you have any questions or feedback, feel free to reach out to me on Twitter or via email.\nUntil next time, happy automating! üöÄ üëã\n","permalink":"https://danielbrennand.com/blog/managing-secrets-in-ansible/","summary":"Hi there! üëã\nIn my previous blog post I covered how to get started with Ansible. In this blog post I\u0026rsquo;ll be continuing to write about Ansible and covering how to manage secrets in Ansible playbooks.\nReady\u0026hellip; set\u0026hellip; go! üöÄ\nManaging Secrets in Ansible üîë As with most automation, we need to use credentials to authenticate to our servers and other applications. Examples of secrets include usernames and passwords, API keys, SSH keys, etc.","title":"Managing Secrets in Ansible"},{"content":"Hey there! üëã\nIn this blog post I\u0026rsquo;ll be writing about Ansible. I\u0026rsquo;ve been using it for a little while now and I\u0026rsquo;d like to share my experience with it and share some knowledge for others starting out! üôÇ\nLet\u0026rsquo;s begin! üöÄ\nWhat is Ansible? Ansible is an open-source automation tool which can be used to automate the management and configuration of servers and other devices. Written in Python, Ansible is agentless and connects over SSH. Some common use-cases for Ansible include (but are not limited to):\n Orchestrating large scale application deployments. Prevent configuration drift and enforce consistent configurations. Provisioning and deprovisioning tasks. Audit server configuration and compliance.  What makes Ansible great is its simplicity and ease of use. By adopting a human friendly language such as YAML, it lowers the barrier to entry for new users and makes it easy to read and understand.\nAnsible Terminology üìö Before we get started, lets go over some of the terminology used in Ansible.\nControl Node The control node is the machine that Ansible is installed and run from. In most cases this will be your local machine however, in some cases, it may be another machine. For example, an organisation may use a Rundeck server as the control node or use Ansible Automation Platform which has the Ansible Automation Controller as the control node.\nSee the Ansible control node requirements documentation for further information.\nInventory üìã The inventory file contains a list of servers that Ansible will manage. Inventory can be in INI or YAML format. Furthermore, it can be static or dynamically created via a script or inventory plugin (more on this later). For example, dynamic inventory could be created from a CMDB or cloud provider API.\nStatic INI Inventory # inventory.ini [webservers] # Inventory group name web1.example.com web2.example.com [databases] db1.example.com Static YAML Inventory # inventory.yaml --- webservers: # Inventory group name hosts: web1.example.com: web2.example.com: databases: hosts: db1.example.com: See the Ansible inventory documentation for further information.\nPlaybooks ‚ñ∂Ô∏è Playbooks define the tasks (plays) that Ansible will run against the servers in the inventory. Playbooks are written in YAML and executed using the ansible-playbook command.\nA simple playbook could look like this:\n--- # playbook.yaml - name: Install Nginx on webservers hosts: webservers tasks: - name: Install Nginx ansible.builtin.package: name: nginx state: present Which you can run like this:\nansible-playbook -i inventory.yaml playbook.yaml See the Ansible playbook documentation for further information.\nModules and Plugins üîå Modules are the building blocks for doing anything in Ansible. Each task in a playbook uses a module and there are many modules available for use right out of the box. Modules can perform a wide range of tasks such as installing packages, managing users, configuring services and more. The example playbook above uses the built-in package module to install Nginx on the webservers.\nFurthermore, most modules provide idempotency which means it will check whether the desired state is already present before making changes. This ensures that the state of the server is kept consistent and speeds up our playbook runs! üöÄ\nPlugins allow us to extend Ansible\u0026rsquo;s core functionality. There are many types of plugins in Ansible. Some examples include:\n Inventory plugins to create dynamic inventory. Callback plugins which change the output format. Become plugins for changing the privilege escalation method.  The main difference between a module and a plugin is that modules are executed on the remote server whereas plugins are executed on the control node itself.\nSee the following documentation pages for more information on modules and plugins:\n https://docs.ansible.com/ansible/latest/module_plugin_guide/index.html https://docs.ansible.com/ansible/latest/module_plugin_guide/modules_intro.html https://docs.ansible.com/ansible/latest/plugins/module.html https://docs.ansible.com/ansible/latest/plugins/plugins.html  Roles Roles contain tasks, files, variables, templates and more for performing repeatable actions. The main benefit of roles is that they can be shared and reused easily across playbooks. You may have to regularly install and configure Nginx on servers; but the configuration differs slightly each time. This would be the perfect use-case for a role.\nRoles can be installed using the ansible-galaxy command. For example, to install the geerlingguy.nginx role you would run the following command:\nansible-galaxy role install geerlingguy.nginx Then, you can use the role in your playbook like this:\n# playbook.yaml - name: Install and configure Nginx on webservers hosts: webservers roles: - geerlingguy.nginx This would use the role defaults but nevertheless, it\u0026rsquo;s far simpler and maintainable right?! No copying tasks from a playbook in one project to another! üôÖ‚Äç‚ôÇÔ∏è We need to stay sane after all ü§™\nCollections üì¶ Collections are the newest distribution format for Ansible content. They can be used to package and distribute playbooks, roles, modules and plugins. Collections can also be installed using the ansible-galaxy command. For example, to install the community.docker collection you would run the following command:\nansible-galaxy collection install community.docker Installing Ansible ‚öôÔ∏è My advice would be to install Ansible in a Python virtual environment. This is because more often than not, the version of Ansible provided by your package manager will be sorely outdated. Furthermore, this way the installation of Ansible won\u0026rsquo;t interfere with your system\u0026rsquo;s Python installation and avoiding the pains of dependency hell.\nTo install Ansible in a virtual environment you\u0026rsquo;ll need Python 3 installed (the newer the better) and the venv module.\nBegin by creating a directory to hold your virtual environment and initialise it:\nmkdir -pv ~/.venv/ansible \u0026amp;\u0026amp; python -m venv ~/.venv/ansible Next, activate the virtual environment and install Ansible:\nsource ~/.venv/ansible/bin/activate pip install ansible Now you\u0026rsquo;re ready to go: üéâ\nansible --version ansible [core 2.14.3] config file = None configured module search path = [\u0026#39;/Users/user/.ansible/plugins/modules\u0026#39;, \u0026#39;/usr/share/ansible/plugins/modules\u0026#39;] ansible python module location = /Users/user/.venv/ansible/lib/python3.11/site-packages/ansible ansible collection location = /Users/user/.ansible/collections:/usr/share/ansible/collections executable location = /Users/user/.venv/ansible/bin/ansible python version = 3.11.1 (main, Feb 8 2023, 19:50:54) [Clang 13.1.6 (clang-1316.0.21.2.5)] (/Users/user/.venv/ansible/bin/python) jinja version = 3.1.2 libyaml = True Ad-hoc Commands Ad-hoc commands are a perfect way for running one off commands quickly and easily against one or more servers. For example, if we wanted to check the storage space of block devices on our servers we could run:\nansible webservers -i inventory.yml -m ansible.builtin.command -a \u0026#34;df -hT\u0026#34; Yay! ‚ú® No more sshing into each server and running the command manually! ü§Æ üí™\nWhat am I using Ansible for? Whilst configuring the devices in my Homelab, I quickly realised that I was deploying Caddy as a container and setting up Autorestic (my personal backup tool of choice) over and over again. I decided to create a Caddy Docker and a Autorestic role and use them in my playbooks. This way I can easily deploy them to any new device I add to my Homelab! üòÑ\nI\u0026rsquo;ve also been using Ansible in my Homelab to deploy my Raspberry Pi K3s cluster and configure my Intel NUC media server. These devices have several containerised services running on them such as Pi-hole, Jellyfin, the *arrs and more. It\u0026rsquo;s still a work in progress and I plan on releasing the playbooks publicly once I\u0026rsquo;ve finalised them; so keep an eye out for that! üëÄ\nConclusion We\u0026rsquo;ve only just scratched the surface of Ansible in this blog post. There are many more features and capabilities that Ansible has to offer. I\u0026rsquo;m sure I\u0026rsquo;ll be writing more blog posts about Ansible in the future. If you have any questions or suggestions on what I should write next, feel free to reach out to me on Twitter or via email.\nFinally, if your thirst for knowledge isn\u0026rsquo;t quenched, I highly recommend checking out the following resources:\n Jeff Geerling\u0026rsquo;s Ansible 101 YouTube series. Jeff Geerling\u0026rsquo;s Ansible for DevOps book. Ansible documentation.  That\u0026rsquo;s all folks! üëã\n","permalink":"https://danielbrennand.com/blog/getting-started-ansible/","summary":"Hey there! üëã\nIn this blog post I\u0026rsquo;ll be writing about Ansible. I\u0026rsquo;ve been using it for a little while now and I\u0026rsquo;d like to share my experience with it and share some knowledge for others starting out! üôÇ\nLet\u0026rsquo;s begin! üöÄ\nWhat is Ansible? Ansible is an open-source automation tool which can be used to automate the management and configuration of servers and other devices. Written in Python, Ansible is agentless and connects over SSH.","title":"Getting started with Ansible"},{"content":"Recently I\u0026rsquo;ve been working on several PowerShell scripts that require credentials to access REST APIs. In this blog post, I will showcase two approaches for storing credentials securely for use in PowerShell scripts.\nEncrypted Password File üîí The encrypted password file leverages the Windows Data Protection API (DPAPI) to encrypt the password as a System.Security.SecureString:\n$Credentials = Get-Credential $Credentials.Password System.Security.SecureString $Credentials.Password | ConvertFrom-SecureString 01000000d08c9ddf0115d1118c7a00c04fc297eb01000000f5ab85d7ee9da048ae4ae797ee7eaf0a000000000200000000001066000000010000200000008c4a03d2f0731e0e7661d695fda8b441eaff31e75724931f31374a0c8292b636000000000e800000000200002000000028da885828bd627480178382ce9a1b477819e7703546ce41819d37f4e63d33ba20000000ab2c4401635ec24db9f20071e18dea0b79ce16ba38b5503ec9937b7fbc849dcf40000000155053a793c210998ef7317b0161e7344c2174b904b527c0cf24e7bbf2243b99e936df3ab67bc9e285a1be33aed37c7604fb07f5d0c44ceb7d6334ca30b0a610 By default DPAPI uses the current user context to generate an encryption key. This encryption key is then used to encrypt the PSCredential.Password property as a System.Security.SecureString (as shown above). It is possible to provide your own encryption key, but I won\u0026rsquo;t be covering that in this post. If you want to read more on this, check out Travis Gan\u0026rsquo;s blog 1.\nIt\u0026rsquo;s also worth noting there are several caveats to this approach:\n You must encrypt the password file as the user which will be accessing it. DPAPI is specific to the device which you encrypt the password file on. You cannot decrypt the password file on another system with the same user.  Generating and using the Encrypted Password File To store the password securely in a file, we can use the Export-Clixml cmdlet which will store the System.Management.Automation.PSCredential object in XML format:\n$Credentials = Get-Credential $Credentials | Export-Clixml -Path \u0026#34;$(pwd)\\EncryptedCreds.xml\u0026#34; Once created, the EncryptedCreds.xml file will contain the XML representation of the PSCredential object. As shown below, the Password property is stored as a SecureString:\n\u0026lt;Objs Version=\u0026#34;1.1.0.1\u0026#34; xmlns=\u0026#34;http://schemas.microsoft.com/powershell/2004/04\u0026#34;\u0026gt; \u0026lt;Obj RefId=\u0026#34;0\u0026#34;\u0026gt; \u0026lt;TN RefId=\u0026#34;0\u0026#34;\u0026gt; \u0026lt;T\u0026gt;System.Management.Automation.PSCredential\u0026lt;/T\u0026gt; \u0026lt;T\u0026gt;System.Object\u0026lt;/T\u0026gt; \u0026lt;/TN\u0026gt; \u0026lt;ToString\u0026gt;System.Management.Automation.PSCredential\u0026lt;/ToString\u0026gt; \u0026lt;Props\u0026gt; \u0026lt;S N=\u0026#34;UserName\u0026#34;\u0026gt;User\u0026lt;/S\u0026gt; \u0026lt;SS N=\u0026#34;Password\u0026#34;\u0026gt;01000000d08c9ddf0115d1118c7a00c04fc297eb01000000f5ab85d7ee9da048ae4ae797ee7eaf0a000000000200000000001066000000010000200000008c4a03d2f0731e0e7661d695fda8b441eaff31e75724931f31374a0c8292b636000000000e800000000200002000000028da885828bd627480178382ce9a1b477819e7703546ce41819d37f4e63d33ba20000000ab2c4401635ec24db9f20071e18dea0b79ce16ba38b5503ec9937b7fbc849dcf40000000155053a793c210998ef7317b0161e7344c2174b904b527c0cf24e7bbf2243b99e936df3ab67bc9e285a1be33aed37c7604fb07f5d0c44ceb7d6334ca30b0a610\u0026lt;/SS\u0026gt; \u0026lt;/Props\u0026gt; \u0026lt;/Obj\u0026gt; \u0026lt;/Objs\u0026gt; We can then re-create the PSCredential object for use in PowerShell scripts using the Import-Clixml cmdlet:\n$Credentials = Import-Clixml -Path \u0026#34;$(pwd)\\EncryptedCreds.xml\u0026#34; $Credentials UserName Password -------- -------- User System.Security.SecureString $Credentials.GetType() IsPublic IsSerial Name BaseType -------- -------- ---- -------- True True PSCredential System.Object Write-Output -InputObject \u0026#34;Username: \u0026#34;\u0026#34;$($Credential.UserName)\u0026#34;\u0026#34; - Password: \u0026#34;\u0026#34;$($Credential.GetNetworkCredential().Password)\u0026#34;\u0026#34;\u0026#34; Username: \u0026#34;User\u0026#34; - Password: \u0026#34;Password123!\u0026#34; Tada! ‚ú®\nThis approach can be good for one off scripts but, can quickly become a maintainance burden when you\u0026rsquo;re doing this for many scripts. For this type of use-case, the next approach is probably more suitable.\nSecretManagement \u0026amp; SecretStore Modules üîê The SecretManagement and SecretStore modules are a great choice for storing credentials securely. To begin, install the modules:\nInstall-Module -Name \u0026#34;Microsoft.PowerShell.SecretManagement\u0026#34;, \u0026#34;Microsoft.PowerShell.SecretStore\u0026#34; -Verbose Creating a SecretVault Next, we need to create a SecretVault to store secrets using the Register-SecretVault cmdlet üîê:\nRegister-SecretVault -Name \u0026#34;MySecretVault\u0026#34; -ModuleName \u0026#34;Microsoft.PowerShell.SecretStore\u0026#34; -DefaultVault -Description \u0026#34;Secret vault storing my secrets.\u0026#34; -PassThru Name ModuleName IsDefaultVault ---- ---------- -------------- MySecretVault Microsoft.PowerShell.SecretStore True Storing Secrets Nice! üëç Now lets store a PSCredential in the SecretVault with the Set-Secret cmdlet. When creating the first secret you will be prompted for a password which will be used to encrypt the SecretVault:\n ‚Ñπ You can store a SecureString, HashTable, String or Byte[] in a secret as well!\n $RestAPICreds = Get-Credential Set-Secret -Name \u0026#34;REST API Creds\u0026#34; -Vault \u0026#34;MySecretVault\u0026#34; -Secret $RestAPICreds Creating a new MySecretVault vault. A password is required by the current store configuration. Enter password: ******** Enter password again for verification: ******** # Bonus! Store metadata alongside the secret itself! $Metadata = @{ SecretExpiration = ([DateTime]::New(2022, 12, 22)) } Set-Secret -Name \u0026#34;REST API Creds\u0026#34; -Vault \u0026#34;MySecretVault\u0026#34; -Secret $RestAPICreds -Metadata $Metadata Retrieving Secrets Now that the secret is stored securely, we can retrieve it in another script using the Get-Secret cmdlet:\n$RestAPICredentialsSecret = Get-Secret -Name \u0026#34;REST API Credentials\u0026#34; -Vault \u0026#34;MySecretVault\u0026#34; $RestAPICredentialsSecret UserName Password -------- -------- test System.Security.SecureString # Bonus! Get information about a secret too! # https://learn.microsoft.com/en-us/powershell/module/microsoft.powershell.secretmanagement/get-secretinfo?view=ps-modules Get-SecretInfo -Name \u0026#34;REST API Credentials\u0026#34; -Vault \u0026#34;MySecretVault\u0026#34; Name Type VaultName ---- ---- --------- REST API Credentials PSCredential MySecretVault # As it\u0026#39;s a PSCredential object, we can use the GetNetworkCredential method to finally get the password! $RestAPICredentialsSecret.GetNetworkCredential().Password Awesome! üòé We\u0026rsquo;ve stored the first secret securely! ‚ú®üöÄ\nHowever, if you use the Get-Secret cmdlet in a new session, or after a certain amount of time has elapsed, you may notice that you\u0026rsquo;re prompted for the password to decrypt the SecretVault again! üò±\nGet-Secret -Name \u0026#34;REST API Credentials\u0026#34; -Vault \u0026#34;MySecretVault\u0026#34; Vault MySecretVault requires a password. Enter password: Obviously this is an issue if we want our scripts to work without any interaction!\nRetrieving Secrets Non-Interactively To fix this we can leverage the first approach and store our SecretVault decryption password in an encrypted password file and retrieve it later! So\u0026hellip; lets do it! üòÑ\n$SecretVaultPassword = Get-Credential $SecretVaultPassword | Export-Clixml -Path \u0026#34;$(pwd)\\SecretVaultPassword.xml\u0026#34; Get-Content -Path \u0026#34;$(pwd)\\SecretVaultPassword.xml\u0026#34; ... \u0026lt;Objs Version=\u0026#34;1.1.0.1\u0026#34; xmlns=\u0026#34;http://schemas.microsoft.com/powershell/2004/04\u0026#34;\u0026gt; \u0026lt;Props\u0026gt; ... \u0026lt;SS N=\u0026#34;Password\u0026#34;\u0026gt;700061007300730077006f0072006400\u0026lt;/SS\u0026gt; \u0026lt;/Props\u0026gt; \u0026lt;/Obj\u0026gt; Once done, we can use the Unlock-SecretStore cmdlet in our scripts to unlock the SecretVault non-interactively:\n$SecretVaultPassword = (Import-Clixml -Path \u0026#34;$(pwd)\\SecretVaultPassword.xml\u0026#34;).Password Unlock-SecretStore -Password $SecretVaultPassword Automation FTW! ‚öô üí™\nModifying SecretVault Configuration Finally, you may also want to modify the configuration of a SecretVault. You can do this using the SecretStore cmdlets Get-SecretStoreConfiguration and Set-SecretStoreConfiguration.\nLet\u0026rsquo;s check the SecretVault configuration with the Get-SecretStoreConfiguration cmdlet:\nGet-SecretStoreConfiguration Scope Authentication PasswordTimeout Interaction ----- -------------- --------------- ----------- CurrentUser Password 900 Prompt We can see that the SecretVault configuration is set to prompt for a password, and the timeout before being prompted for the password again is 900 seconds (15 minutes) by default.\nThe timeout can be changed using the Set-SecretStoreConfiguration cmdlet:\nSet-SecretStoreConfiguration -PasswordTimeout 1800 -Confirm:$false # Check the configuration again Get-SecretStoreConfiguration Scope Authentication PasswordTimeout Interaction ----- -------------- --------------- ----------- CurrentUser Password 1800 Prompt Conclusion Finished! üòÑ Hopefully you learned something new and found this post helpful! üôÇ There is a lot that I didn\u0026rsquo;t cover when it comes to the SecretManagement module. For example, extensions which create integrations with third-party secret management products like Azure KeyVault, KeePass, HashiCorp Vault and more!\nUntil the next one! üëã\nReferences   https://www.travisgan.com/2015/06/powershell-password-encryption.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://danielbrennand.com/blog/powershell-storing-credentials/","summary":"Recently I\u0026rsquo;ve been working on several PowerShell scripts that require credentials to access REST APIs. In this blog post, I will showcase two approaches for storing credentials securely for use in PowerShell scripts.\nEncrypted Password File üîí The encrypted password file leverages the Windows Data Protection API (DPAPI) to encrypt the password as a System.Security.SecureString:\n$Credentials = Get-Credential $Credentials.Password System.Security.SecureString $Credentials.Password | ConvertFrom-SecureString 01000000d08c9ddf0115d1118c7a00c04fc297eb01000000f5ab85d7ee9da048ae4ae797ee7eaf0a000000000200000000001066000000010000200000008c4a03d2f0731e0e7661d695fda8b441eaff31e75724931f31374a0c8292b636000000000e800000000200002000000028da885828bd627480178382ce9a1b477819e7703546ce41819d37f4e63d33ba20000000ab2c4401635ec24db9f20071e18dea0b79ce16ba38b5503ec9937b7fbc849dcf40000000155053a793c210998ef7317b0161e7344c2174b904b527c0cf24e7bbf2243b99e936df3ab67bc9e285a1be33aed37c7604fb07f5d0c44ceb7d6334ca30b0a610 By default DPAPI uses the current user context to generate an encryption key.","title":"PowerShell: Storing Credentials Securely"},{"content":"Hi there! üëã\nRecently I discovered Git Hooks. Git Hooks provide a way of running custom scripts when a certain git action occurs. In this post, I want to share a pre-commit Git Hook I\u0026rsquo;ve written to lint PowerShell code using the PSScriptAnalyzer module.\nWhat is a Linter? üïµÔ∏è‚Äç‚ôÇÔ∏è A linter analyses code to identify common errors, bugs and stylistic issues. Their aim is to improve code quality. Linters perform static analysis meaning they check code without executing it. Some well known linters for other languages include ESLint and Pylint.\nWhat is PSScriptAnalyzer? PSScriptAnalyzer is a linter for PowerShell modules and scripts. It runs a set of rules against PowerShell code. These rules are based on best practices identified by the PowerShell team at Microsoft and the community. The full set of rules can be found here.\nWhat is a pre-commit Git Hook? A pre-commit Git Hook executes when running git commit. The contents of the pre-commit file located at .git/hooks/pre-commit is executed and if the script has an exit code of 1 (job failed), then the commit is aborted. Pre-commit hooks provide an excellent use case for linting code changes before pushing to a remote repository.\nLinting PowerShell code using the pre-commit Git Hook üîé To use this pre-commit Git Hook you must have PowerShell 7, Git and the PSScriptAnalyzer module installed.\nInstall PSScriptAnalyzer:\nInstall-Module -Name \u0026#34;PSScriptAnalyzer\u0026#34; -Verbose Pre-commit Git Hook #!/usr/bin/env pwsh Import-Module -Name \u0026#34;PSScriptAnalyzer\u0026#34; $DiffNames = git --no-pager diff --name-only --staged --line-prefix=\u0026#34;$(git rev-parse --show-toplevel)/\u0026#34; $Results = @() foreach ($DiffName in $DiffNames) { Write-Output -InputObject \u0026#34;Analysing \u0026#34;\u0026#34;$($DiffName)\u0026#34;\u0026#34;\u0026#34; $Output = Invoke-ScriptAnalyzer -Path $DiffName $Results += $Output } if ($Results.Count -gt 0) { Write-Warning -Message \u0026#34;PSScriptAnalyzer identified one or more files with linting errors. Commit aborted. Fix them before committing or use \u0026#39;git commit --no-verify\u0026#39; to bypass this check.\u0026#34; foreach ($Result in $Results) { Write-Error -Message \u0026#34;$($Result.ScriptName)- Line $($Result.Line)- $($Result.Message)\u0026#34; } exit 1 } Installation Linux   Create .git/hooks/pre-commit:\ntouch .git/hooks/pre-commit   Paste the PowerShell snippet above into .git/hooks/pre-commit\n  Make .git/hooks/pre-commit executable:\nchmod +x .git/hooks/pre-commit   Windows   Create .git/hooks/pre-commit.ps1 and .git/hooks/pre-commit:\nNew-Item -Path \u0026#34;.git\\hooks\\pre-commit.ps1\u0026#34;, \u0026#34;.git\\hooks\\pre-commit\u0026#34; -ItemType \u0026#34;File\u0026#34;   Paste the PowerShell snippet above into .git/hooks/pre-commit.ps1\n  Paste the snippet below into .git/hooks/pre-commit:\n#!/bin/sh pwsh -File \u0026#34;$(git rev-parse --show-toplevel)\\.git\\hooks\\pre-commit.ps1\u0026#34;   Usage Now lets see if it\u0026rsquo;s working! üòÑ\n  Create a directory and initialise a new repository:\nmkdir ~/git-hook-pwsh cd ~/git-hook-pwsh git init   Follow the installation steps above depending on your environment.\n  Create a file called git-hook-pwsh.ps1 which uses Write-Host:\necho \u0026#39;Write-Host -Object \u0026#34;Oh no! Not the dreaded Write-Host!\u0026#34;\u0026#39; \u0026gt; git-hook-pwsh.ps1   Stage git-hook-pwsh.ps1 and attempt to commit it:\ngit add git-hook-pwsh.ps1; git commit -m \u0026#34;Is my pre-commit git hook working?\u0026#34;   If all goes well you should see an output similar to below in your terminal:\n‚ûú git-hook-pwsh git:(master) ‚úó git commit -m \u0026#34;Is my pre-commit git hook working?\u0026#34; Analysing \u0026#34;/home/dab/git-hook-pwsh/git-hook-pwsh.ps1\u0026#34; WARNING: PSScriptAnalyzer identified one or more files with linting errors. Commit aborted. Fix them before committing or use \u0026#39;git commit --no-verify\u0026#39; to bypass this check. Write-Error: git-hook-pwsh.ps1 - Line 1 - File \u0026#39;git-hook-pwsh.ps1\u0026#39; uses Write-Host. Avoid using Write-Host because it might not work in all hosts, does not work when there is no host, and (prior to PS 5.0) cannot be suppressed, captured, or redirected. Instead, use Write-Output, Write-Verbose, or Write-Information. Awesome! It worked! ‚≠ê\nI\u0026rsquo;ve also posted this pre-commit Git Hook as a GitHub gist so others can find it! üòÉ\nEnjoy! ‚ú® Until next time! üòÑ\n","permalink":"https://danielbrennand.com/blog/git-hook-powershell/","summary":"Hi there! üëã\nRecently I discovered Git Hooks. Git Hooks provide a way of running custom scripts when a certain git action occurs. In this post, I want to share a pre-commit Git Hook I\u0026rsquo;ve written to lint PowerShell code using the PSScriptAnalyzer module.\nWhat is a Linter? üïµÔ∏è‚Äç‚ôÇÔ∏è A linter analyses code to identify common errors, bugs and stylistic issues. Their aim is to improve code quality. Linters perform static analysis meaning they check code without executing it.","title":"Using Git Hooks to lint PowerShell"},{"content":"Hi there! üëã\nIn this blog post, I will show you how to download a streamed video from a website using youtube-dl.\nBackground Recently I needed to download a streamed video from a website for archival purposes. Full disclaimer, the service had already been paid for and I wanted to keep the video which was set to expire after a specific date.\nInvestigation üîç At first there was no obvious way to download the video. Investigating the HTTP traffic in the browser, I could see that the video was being streamed with each part being progressively downloaded.\nEach part of the video was split into Transport Stream (.ts) files, each lasting around 25 seconds. After some quick googling I came across this stack overflow post. The answers said to find the playlist file (.m3u8) associated with the Transport Stream.\nFinding the Transport Stream Playlist File URL üîó To find the Transport Stream playlist file URL, follow the steps below:\n  Open your browser of choice.\n  Press CTRL + SHIFT + I to bring up developer tools and select Network.\n  Go to the web page containing the streamed video and use the filter to search the HTTP requests for playlist.m3u8.\n  In my example, the Transport Stream playlist file URL was similar to: https://x.cloudfront.net/x_vods/_definst_/xvods/x-vod/vb/11111_2021-11-22_12355F4.mp4/playlist.m3u8\nDownloading a Streamed Video using youtube-dl ‚¨áÔ∏è To download the streamed video I used mikenye\u0026rsquo;s youtube-dl container image.\nVerifying the Transport Stream Playlist File URL üîó To verify that you have the correct playlist file URL, run youtube-dl with the -F option to list all available formats:\ndocker run \\  --rm -i \\  -e PGID=$(id -g) \\  -e PUID=$(id -u) \\  -v \u0026#34;$(pwd)\u0026#34;:/workdir:rw \\  mikenye/youtube-dl -F \\  https://x.cloudfront.net/x_vods/_definst_/xvods/x-vod/vb/11111_2021-11-22_12355F4.mp4/playlist.m3u8 This should output something similar to:\n[generic] playlist: Requesting header [generic] playlist: Downloading m3u8 information [info] Available formats for playlist: format code extension resolution note 556 mp4 640x360 556k , avc1.42c01e, mp4a.40.2 Awesome! üòé Now you know you have the correct URL!\nDownloading the Streamed Video ‚¨áÔ∏è To download the streamed video, run the following command providing the format code from the output above:\ndocker run \\  --rm -i \\  -e PGID=$(id -g) \\  -e PUID=$(id -u) \\  -v \u0026#34;$(pwd)\u0026#34;:/workdir:rw \\  mikenye/youtube-dl --format 556 \\  https://x.cloudfront.net/x_vods/_definst_/xvods/x-vod/vb/11111_2021-11-22_12355F4.mp4/playlist.m3u8 You should see youtube-dl identify each .ts file in the playlist, read its contents and combine them into a single .mp4 file.\n[generic] playlist: Requesting header [generic] playlist: Downloading m3u8 information [download] Destination: playlist-playlist.mp4 ... [https @ 0x561d074eadc0] Opening \u0026#39;https://x.cloudfront.net/x_vods/_definst_/xvods/x-vod/vb/11111_2021-11-22_12355F4.mp4/media_w123456789_2.ts\u0026#39; for reading ... [https @ 0x561d074eadc0] Opening \u0026#39;https://x.cloudfront.net/x_vods/_definst_/xvods/x-vod/vb/11111_2021-11-22_12355F4.mp4/media_w123456789_308.ts\u0026#39; for reading ... frame=77069 fps=2670 q=-1.0 Lsize=84894kB time=00:51:22.79 bitrate=225.6kbits/s speed=107x video:35021kB audio:48855kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 1.213379% [ffmpeg] Downloaded 86931123 bytes [download] 100% of 82.90MiB in 00:30 Absolute magic! ‚ú®\nI hope someone finds this blog post useful and it saves them a bit of time üôÇ\n","permalink":"https://danielbrennand.com/blog/download-streamed-video/","summary":"Hi there! üëã\nIn this blog post, I will show you how to download a streamed video from a website using youtube-dl.\nBackground Recently I needed to download a streamed video from a website for archival purposes. Full disclaimer, the service had already been paid for and I wanted to keep the video which was set to expire after a specific date.\nInvestigation üîç At first there was no obvious way to download the video.","title":"Use youtube-dl to download a streamed video"},{"content":"Recently at work, a customer raised a ticket about programmatically interacting with UKCloud\u0026rsquo;s Cloud Storage service.\n UKCloud\u0026rsquo;s Cloud Storage service is an object storage solution based on Dell EMC Elastic Cloud Storage (ECS). Access is via a RESTful API, which also provides support for Amazon\u0026rsquo;s S3 API.\nUKCloud: Getting Started Guide for Cloud Storage 1 This ticket was interesting as the customer was using the @aws-sdk/client-s3 JavaScript package to upload images to the service. Prior to this ticket, I hadn\u0026rsquo;t used this package before or have much experience with S3 object storage and programmatically interacting with it. In this blog post, I want to share my learnings and provide a couple of examples in Node.js for interacting with UKCloud\u0026rsquo;s Cloud Storage service using this package.\n [NOTE]\nIt is worth noting that the code examples in this post should work with any S3 object storage provider, just modify the endpoint and region!\n S3 Terminology Before we get started, it\u0026rsquo;s important to understand certain terminology around Amazon\u0026rsquo;s S3 API.\nObjects  An object is a file and any metadata that describes that file. Objects consist of object data and metadata. The metadata is a set of name-value pairs that describe the object. These pairs include some default metadata, such as the date last modified \u0026hellip; You can also specify custom metadata at the time that the object is stored.\nAmazon Web Services: What is Amazon S3? 2Amazon Web Services: Uploading, downloading, and working with objects in Amazon S3 3 Object Keys üîë An object key (or key name) is a unique identifier of an object.\n Amazon Web Services: Creating object key names 4 An example of an object key could be Testing/Requirements.pdf or Accounting/Payslips.xls. Note that the object keys are prefixed with a directory.\nBuckets  A bucket is a container for objects\u0026hellip; Every object is contained in a bucket.\nAmazon Web Services: What is Amazon S3? 2 A bucket has a flat structure with no actual concept of directories however, you can prefix object keys with (sub)directories to create a directory structure üìÅ\nNow that\u0026rsquo;s out of the way, let\u0026rsquo;s begin interacting with the ECS S3 API using the @aws-sdk/client-s3 for JavaScript üòé\nPrerequisites Before starting, you will need the following:\n   Prerequisite Optional     Docker installed on your system. No   The S3 API endpoint for the object storage provider of your choice. In the examples I will be using UKCloud\u0026rsquo;s Cloud Storage endpoint: https://cas.cor00005.ukcloud.com. No   The object storage provider\u0026rsquo;s region name. In the examples I will be using: cor00005. No   A bucket created with an object storage provider of your choice. Yes - You can create one via the API however, for the examples I already had a bucket.   The object storage provider\u0026rsquo;s equivalent of an AccessKeyId and SecretAccessKey. No    Node.js Examples üë®‚Äçüíª The following packages are used in these examples:\n  @aws-sdk/client-s3 - Documentation.\n  @aws-sdk/s3-request-presigner - Documentation.\n  fs\n  Begin by creating two files: examples.mjs and dockerfile. Then, paste the snippet below into the dockerfile:\nFROM node:16-alpine RUN mkdir -p /usr/src/app \u0026amp;\u0026amp; chown -R node:node /usr/src/app WORKDIR /usr/src/app RUN npm install @aws-sdk/client-s3@3.43.0 @aws-sdk/s3-request-presigner@3.44.0 COPY --chown=node:node examples.mjs examples.mjs USER node CMD [\u0026quot;node\u0026quot;, \u0026quot;examples.mjs\u0026quot;]  [NOTE] Place the Node.js examples below into examples.mjs.\n Initialise the S3Client Start by initialising the S3Client by importing the required package:\nimport { S3Client } from \u0026#39;@aws-sdk/client-s3\u0026#39;; // Retrieve constants from environment variables const S3_ENDPOINT = process.env.S3_ENDPOINT; const REGION = process.env.REGION; const BUCKET_NAME = process.env.BUCKET_NAME; // Initialise client const client = new S3Client({ endpoint: S3_ENDPOINT, region: REGION, }); Sending Commands with the S3Client Now, let\u0026rsquo;s use the S3Client to create an object in a bucket. For this type of action use the PutObjectCommand:\nCreate an Object import { S3Client, PutObjectCommand } from \u0026#39;@aws-sdk/client-s3\u0026#39;; // ...  // Create an object (testObject.txt) prefixed with a directory (test-dir/) in a bucket try { await client.send( new PutObjectCommand({ Bucket: BUCKET_NAME, Key: \u0026#39;test-dir/testObject.txt\u0026#39;, Body: \u0026#39;Uploaded test object.\u0026#39; }) ); console.log(`Successfully created object \u0026#39;test-dir/testObject.txt\u0026#39; in the bucket \u0026#39;${BUCKET_NAME}\u0026#39;`); } catch (err) { console.error(`An error occurred creating object \u0026#39;test-dir/testObject.txt\u0026#39; in the bucket \u0026#39;${BUCKET_NAME}\u0026#39;: ${err}`); }; Let\u0026rsquo;s see if it works! üòÑ Build and run the container image:\ndocker build -t aws-sdk-s3:nodejs . # The aws-sdk looks for the environment variables # AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY automatically docker run --rm --name aws-sdk-s3 \\  -e S3_ENDPOINT=\u0026#34;https://cas.cor00005.ukcloud.com\u0026#34; \\  -e REGION=\u0026#34;cor00005\u0026#34; \\  -e BUCKET_NAME=\u0026#34;db-bucket\u0026#34; \\  -e AWS_ACCESS_KEY_ID=\u0026#34;...\u0026#34; \\  -e AWS_SECRET_ACCESS_KEY=\u0026#34;...\u0026#34; \\  aws-sdk-s3:nodejs # Output Successfully created object \u0026#39;test-dir/testObject.txt\u0026#39; in the bucket \u0026#39;db-bucket\u0026#39; üò± It worked! Congrats! You\u0026rsquo;ve just created your first object in a bucket! üòé\nList and Get Objects Another action is to list objects in a bucket and get them. Here is an example I wrote for that:\nimport { S3Client, PutObjectCommand, ListObjectsCommand, GetObjectCommand } from \u0026#39;@aws-sdk/client-s3\u0026#39;; // ...  // List of all objects (max 1000) in a bucket try { var objects = await client.send( new ListObjectsCommand({ Bucket: BUCKET_NAME }) ); } catch (err) { console.error(`An error occurred listing objects for the bucket \u0026#39;${BUCKET_NAME}\u0026#39;: ${err}`); }; // Iterate over objects in the bucket and perform GetObjectCommand try { for (const [index, object] of objects.Contents.entries()) { console.log(`${index}- Getting object: ${object.Key}`); const resp = await client.send( new GetObjectCommand({ Bucket: BUCKET_NAME, Key: object.Key }) ); console.log(`${index}- Object: ${object.Key}, ETag: ${resp.ETag}`); }; } catch (err) { console.error(`An error occurred getting objects for the bucket \u0026#39;${BUCKET_NAME}\u0026#39;: ${err}`); }; Upload and Download Objects Uploading and downloading objects is done often, so below are examples I wrote for this:\n// ... import fs from \u0026#39;fs\u0026#39;; // ...  // Create a file to be upload fs.writeFileSync(\u0026#39;/usr/src/app/uploadfile.txt\u0026#39;, \u0026#39;Upload me!\u0026#39;, (_) =\u0026gt; { console.log(\u0026#34;Successfully wrote file \u0026#39;/usr/src/app/uploadfile.txt\u0026#39;\u0026#34;); }); // Upload an object (file) to a bucket try { // Load the contents of \u0026#39;uploadfile.txt\u0026#39;  fs.readFile(\u0026#39;/usr/src/app/uploadfile.txt\u0026#39;, \u0026#39;utf8\u0026#39;, async function (_, data) { await client.send( new PutObjectCommand({ Bucket: BUCKET_NAME, Key: \u0026#39;uploadfile.txt\u0026#39;, Body: data }) ) console.log(`Successfully uploaded object \u0026#39;/usr/src/app/uploadfile.txt\u0026#39; to the bucket \u0026#39;${BUCKET_NAME}\u0026#39;`); }); } catch (err) { console.error(`An error occurred uploading object \u0026#39;/usr/src/app/uploadfile.txt\u0026#39; to the bucket \u0026#39;${BUCKET_NAME}\u0026#39;: ${err}`); }; // Download an object (file) from a bucket try { const downloadFile = await client.send( new GetObjectCommand({ Bucket: BUCKET_NAME, Key: \u0026#39;test-dir/testObject.txt\u0026#39; }) ); // Create a write stream  const writeStream = fs.createWriteStream(\u0026#39;/usr/src/app/testObject.txt\u0026#39;); // Pipe the object\u0026#39;s body to the write stream  downloadFile.Body.pipe(writeStream); console.log(`Successfully downloaded object \u0026#39;testObject.txt\u0026#39; from the bucket \u0026#39;${BUCKET_NAME}\u0026#39;`); } catch (err) { console.error(`An error occurred downloading object \u0026#39;test-dir/testObject.txt\u0026#39; from the bucket \u0026#39;${BUCKET_NAME}\u0026#39;: ${err}`); }; Generate a Public URL for an Object The final example I want to share is generating a public URL for an object so it can be downloaded. The example below generates a URL valid for one hour:\n// ... import { getSignedUrl } from \u0026#34;@aws-sdk/s3-request-presigner\u0026#34;; /// ...  // Get a public URL for an object in a bucket // Valid for 1 hour try { const command = new GetObjectCommand({ Bucket: BUCKET_NAME, Key: \u0026#39;uploadfile.txt\u0026#39; }); // Get public URL for the object  const publicUrl = await getSignedUrl(client, command, { expiresIn: 3600 }); console.log(`Successfully generated public URL for object \u0026#39;test-dir/testObject.txt\u0026#39; in the bucket \u0026#39;${BUCKET_NAME}\u0026#39;: ${publicUrl}`); } catch (err) { console.error(`An error occurred generating public URL for object \u0026#39;test-dir/testObject.txt\u0026#39; in the bucket \u0026#39;${BUCKET_NAME}\u0026#39;: ${err}`); }; The examples above and others that I wrote can be found on GitHub here.\nI hope you learned something from this blog post! Until next time! üòÑ üëã\nReferences   https://docs.ukcloud.com/articles/cloud-storage/cs-gs.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n https://docs.aws.amazon.com/AmazonS3/latest/userguide/uploading-downloading-objects.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-keys.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://danielbrennand.com/blog/aws-sdk-s3/","summary":"Recently at work, a customer raised a ticket about programmatically interacting with UKCloud\u0026rsquo;s Cloud Storage service.\n UKCloud\u0026rsquo;s Cloud Storage service is an object storage solution based on Dell EMC Elastic Cloud Storage (ECS). Access is via a RESTful API, which also provides support for Amazon\u0026rsquo;s S3 API.\nUKCloud: Getting Started Guide for Cloud Storage 1 This ticket was interesting as the customer was using the @aws-sdk/client-s3 JavaScript package to upload images to the service.","title":"Interacting with the ECS S3 API using the aws-sdk/client-s3 package"},{"content":"Hey there! üëã\nIn this blog post, I will be showing you how to use Cloudflare as a Dynamic DNS (DDNS) provider.\nWhat is DDNS?  Dynamic DNS (DDNS) is a service that keeps the Domain Name System (DNS) updated with a web property‚Äôs correct IP address.\nCloudflare glossary: Dynamic DNS 1 Essentially, DDNS allows you to automatically update your domain\u0026rsquo;s DNS records when a change is detected to your home\u0026rsquo;s public IP address.\nDDNS Use Case Many Internet Service Providers (ISPs) do not provide a static IP address with their regular consumer plans. This often causes issues for those of us that enjoy self-hosting applications from home.\nSay you\u0026rsquo;re self-hosting some applications from home and have DNS records pointing to your home\u0026rsquo;s public IP address. What if your router suddenly rebooted due to a software or hardware issue?\nYour ISP may assign you with a new public IP address causing your applications to be inaccessible üò∞ This is an example of where DDNS could help.\nWith DDNS, you can ensure your DNS records are automatically kept up to date when your home\u0026rsquo;s public IP address changes üëç\nOther DDNS providers Cloudflare is not the only option for DDNS. There are many others including:\n  DuckDNS\n  NoIP\n  FreeDNS\n  However, for this blog post, we are obviously focusing on using Cloudflare üòÑ\nUsing Cloudflare as a DDNS provider We will be using joshuaavalon\u0026rsquo;s docker-cloudflare container to use Cloudflare as a DDNS provider.\nThere are many Cloudflare DDNS containers out there. The reason I use this one is because:\n  It\u0026rsquo;s multi-architecture meaning the container image can run on amd64, ARM/v6, ARM/v7 and ARM64 based devices.\n ARM FTW! ü•ß    It has a minimal configuration file supporting YAML, JSON or JavaScript. Configuration via environment variables is also supported but is considered \u0026ldquo;legacy\u0026rdquo;.\n  It provides advanced configuration options such as using an IPv(4|6) lookup service of your choice, and Webhooks to notify when DNS record updates run, succeed or fail.\n  Prerequisites   A Cloudflare account and domain added to Cloudflare.\n  Docker installed on the device which can reach the Cloudflare API: https://api.cloudflare.com/client/v4/\n  A Cloudflare API token. Please follow the instructions here.\n   Make sure you generate an API Token and not a Global API Key!\n Using the docker-cloudflare container The minimal configuration file required for the docker-cloudflare container is the following:\n# config.yaml auth: # Provide your API token here! scopedToken: QPExdfoNLwndJPDbt4nK9-yF1z_srC8D0m6-Gv_h domains: # Remember to change this to your domain! - name: ddns.yourdomain.com # The type of record that is created type: A # Determines the proxy status for the record proxied: true # If the record does not exist, create it create: true # zoneId could also be yourdomain.com if the Cloudflare API token is granted #zone:read permissions zoneId: JBFRZWzhTKtRFWgu3X7f4YLX The container will update your DNS record with your public IP address every 5 minutes.\nOnce you have your configuration file, use the following command to start the docker-cloudflare container:\ndocker run --name cloudflare-ddns -d -v ./config.yaml:/app/config.yaml joshava/cloudflare-ddns\nOnce started, you should see something similar to the following when running docker logs cloudflare-ddns:\n[cont-init.d] executing container initialization scripts... [cont-init.d] 10-adduser: executing... usermod: no changes Initializing container User uid: 1001 User gid: 1001 [cont-init.d] 10-adduser: exited 0. [cont-init.d] 11-cron: executing... Setting crontab to */5 * * * * [cont-init.d] 11-cron: exited 0. [cont-init.d] done. [services.d] starting services [services.d] done. 2021-09-16T18:02:57.903Z [info] Cloudflare DDNS start 2021-09-16T18:03:00.026Z [info] Skipped updating. 2021-09-16T18:03:00.027Z [info] Updated ddns.yourdomain.com with \u0026lt;your public IP\u0026gt; 2021-09-16T18:03:00.028Z [info] Cloudflare DDNS end Congratulations, you\u0026rsquo;re now using Cloudflare as a DDNS provider! üòÑ ‚≠ê\nBonus - Using a Discord Webhook for updates A few months ago I looked into using Discord webhooks to receive updates for when my DNS records were updated by the container.\nI submitted a PR because I wanted to send a specific message along with the webhook payload. After discussion with the author, a webhook formatter was added which I tested using a Discord webhook.\nFirstly, you need to create a Discord webhook for a channel on your server. Once created, provide the webhook URL in the config below.\nTo use the formatter, you need to use a JavaScript configuration file similar to the following:\n// config.js const formatter = (status, data) =\u0026gt; { if (status === \u0026#34;run\u0026#34;) { return { content: \u0026#34;Updating DNS record.\u0026#34; }; } else { return { content: JSON.stringify(data) }; } }; const config = { auth: { scopedToken: \u0026#34;QPExdfoNLwndJPDbt4nK9-yF1z_srC8D0m6-Gv_h\u0026#34; }, domains: [ { // Remember to change to your domain!  name: \u0026#34;ddns.yourdomain.com\u0026#34;, type: \u0026#34;A\u0026#34;, proxied: true, create: true, // Remember to change the zone ID!  zoneId: \u0026#34;JBFRZWzhTKtRFWgu3X7f4YLX\u0026#34;, webhook: { // Make sure you edit the webhook URL below to your own!  run: \u0026#34;https://discord.com/api/webhooks/111111233445566678/py-D6zAc4IolXBoA7gslLAJc0WKO3KPU1eOxSNzX6qlkCBsqIP8EGILj-ALraivIbs6n\u0026#34;, success: \u0026#34;https://discord.com/api/webhooks/111111233445566678/py-D6zAc4IolXBoA7gslLAJc0WKO3KPU1eOxSNzX6qlkCBsqIP8EGILj-ALraivIbs6n\u0026#34;, failure: \u0026#34;https://discord.com/api/webhooks/111111233445566678/py-D6zAc4IolXBoA7gslLAJc0WKO3KPU1eOxSNzX6qlkCBsqIP8EGILj-ALraivIbs6n\u0026#34;, formatter } } ] }; module.exports = config; As the configuration file has changed from YAML to JavaScript, the docker run command used before is slightly different: docker run --name cloudflare-ddns -d -v ./config.js:/app/config.js joshava/cloudflare-ddns\nOnce running, you should see messages appearing in the Discord channel when a DNS record update occurs!\nI hope you found this blog post useful! Until next time! üòÑ\nReferences   https://www.cloudflare.com/learning/dns/glossary/dynamic-dns/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://danielbrennand.com/blog/cloudflare-ddns/","summary":"Hey there! üëã\nIn this blog post, I will be showing you how to use Cloudflare as a Dynamic DNS (DDNS) provider.\nWhat is DDNS?  Dynamic DNS (DDNS) is a service that keeps the Domain Name System (DNS) updated with a web property‚Äôs correct IP address.\nCloudflare glossary: Dynamic DNS 1 Essentially, DDNS allows you to automatically update your domain\u0026rsquo;s DNS records when a change is detected to your home\u0026rsquo;s public IP address.","title":"Using Cloudflare as a Dynamic DNS (DDNS) provider"},{"content":"Welcome üëã\nIn this blog post, I will be showing you how to create a blog for free using GitHub Pages and Hugo.\nI will walk you through the process of creating the GitHub repository (where your blog will live), creating your Hugo site, adding a theme to your blog üé®, creating your first blog post and automating the publishing process!\nWhat is GitHub Pages and Hugo? GitHub Pages allows you to create a website which is hosted directly from a repository on GitHub.\nHugo is a fast and highly customisable static site generator.\nPrerequisites You\u0026rsquo;re going to need a couple of things before you start creating your blog:\n  A GitHub account.\n  Git.\n Follow the instructions for installing Git, setting up your username, commit email address.    Hugo.\n  I recommend installing the extended version of Hugo as some themes require it.\n  Follow the instructions for Windows or Linux.\n   Most likely your operating system will be 64-bit architecture. So you would download: hugo_extended_{version}_Windows-64bit.zip\n  If you have choco installed, run the following command to install Hugo: choco install hugo-extended -y\n   Step 1 - Creating and cloning the GitHub repository Create a new public GitHub repository named username.github.io. Where username is your GitHub username. For example, if your GitHub username was bumblebee, then you would enter bumblebee.github.io.\nEnter the following command in a terminal to clone the repository to your machine (providing your GitHub username instead of username). Enter your GitHub credentials when prompted: git clone https://github.com/username/username.github.io.git\nNow, change directory and change the default branch to source using the following command:  cd username.github.io; git branch -M source\nFor the rest of this post, substitute username for your GitHub username.\nStep 2 - Initalising your Hugo site Run the following command to initalise your site: hugo new site . -f yml --force\nYou should see the following output:\nCongratulations! Your new Hugo site is created in /path/to/your/hugo/site/username.github.io.\nStep 3 - Adding and configuring a site theme üé® Next, go to https://themes.gohugo.io/ and browse the list of themes that are available. There are a lot\u0026hellip; üòÑ\nMany themes provide a demo so you can see for yourself whether you like a theme. Furthermore, many themes have their own configuration so make sure you read the documentation.\nFor this blog post, I\u0026rsquo;m going to use the Tania theme.\nFollowing the installation instructions, run the command: git submodule add https://github.com/WingLim/hugo-tania themes/hugo-tania to install the theme.\nIn the site\u0026rsquo;s root directory (username.github.io), open the config.yml file and paste the Tania theme\u0026rsquo;s configuration. Edit it to your liking and save the file.\n NOTE: Make sure you edit baseurl: \u0026quot;https://example.com\u0026quot; to baseurl: \u0026quot;https://username.github.io\u0026quot; - Remembering to substitute username for your GitHub username!\n Additional configuration for the Tania theme can be found here.\nThe Tania theme also requires an articles.md file to be created if you want to have an archive page of blog posts as shown on the demo site.\nCreate the articles.md file inside the site\u0026rsquo;s content directory (username.github.io/content), paste the following block into it and save the file:\n--- title: Articles # Edit subtitle and date! subtitle: Posts, tutorials, snippets, musings, and everything else. date: 2020-11-26 type: section layout: \u0026quot;archives\u0026quot; --- Step 4 - Creating your first blog post ‚úèÔ∏è Now it\u0026rsquo;s time to create your first blog post!\nRun the following command to create the blog post file (changing your-blog-post to the name of the post): hugo new post/your-blog-post.md\nYou should see the following output verifying that a markdown file was created for the post: username.github.io/content/post/your-blog-post.md created.\nNext, open the file, write your post\u0026rsquo;s content and save the file.\nTo see your site rendered run the command: hugo server -D and paste the following URL into your browser: http://localhost:1313/\n NOTE: Once you have finished writing your post, set draft: true at the top of the file to draft: false\n Step 5 - Automating the publishing process ‚öôÔ∏è To automate the publishing process of your blog, create a gh-pages.yml file located at: username.github.io/.github/workflows/gh-pages.yml, paste the following block into the file and save the file:\nname: Github Pages on: push: branches: - source jobs: deploy: runs-on: ubuntu-20.04 concurrency: group: ${{ github.workflow }}-${{ github.ref }} steps: - name: Checkout uses: actions/checkout@v2 with: submodules: true # Fetch Hugo themes (true OR recursive) fetch-depth: 0 # Fetch all history for .GitInfo and .Lastmod - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: \u0026#39;0.85.0\u0026#39; extended: true - name: Build run: hugo --minify - name: Deploy uses: peaceiris/actions-gh-pages@v3 if: ${{ github.ref == \u0026#39;refs/heads/source\u0026#39; }} with: github_token: ${{ secrets.GITHUB_TOKEN }} publish_branch: main publish_dir: ./public Step 6 - Push the site\u0026rsquo;s content to the GitHub repository Finally, you need to push all the site\u0026rsquo;s content to the GitHub repository. To do this, run the following commands from the site\u0026rsquo;s root directory (username.github.io):\n  Stage all of the site\u0026rsquo;s content ready to commit: git add -A\n  Create a commit message to go along with the site\u0026rsquo;s content: git commit -m \u0026quot;Publishing my first blog post.\u0026quot;\n  Push the commit to the source branch: git push -u origin source\n  Step 7 - Configuring GitHub Pages for your GitHub repository Go to https://github.com/username/username.github.io/settings/pages and perform the following steps:\n  Set the source branch to main.\n  Set the source folder to / (root).\n  Press Save.\n  Enable Enforce HTTPS so your blog is served over HTTPS.\n  If all goes well the automated workflow will trigger and your site will be published at: https://username.github.io üéâ\nEnjoy! üòÑ\n","permalink":"https://danielbrennand.com/blog/blog-github-pages-hugo/","summary":"Welcome üëã\nIn this blog post, I will be showing you how to create a blog for free using GitHub Pages and Hugo.\nI will walk you through the process of creating the GitHub repository (where your blog will live), creating your Hugo site, adding a theme to your blog üé®, creating your first blog post and automating the publishing process!\nWhat is GitHub Pages and Hugo? GitHub Pages allows you to create a website which is hosted directly from a repository on GitHub.","title":"Create a blog for free using GitHub Pages and Hugo"},{"content":"Hi! üëã\nI had some free time recently so I decided to develop something which was relevant to current worldwide situation. The ongoing Coronavirus (COVID-19) pandemic.\nThis is going to be a short blog post about the application, its aims and the process of deploying it to Azure App Service.\nAims I had three aims that I wanted to achieve:\n  I wanted to develop an application which was simple.\n  I wanted to show data about how the COVID-19 pandemic is developing in the UK.\n  I wanted to show relevant news related to COVID-19 in the UK.\n  UK-COVID-19-Stats Python application The application is open source on Github and developed in Python. Check it out here! üòÑ\nThe application uses a number of libraries including:\n  Flask: I have had some previous experience with this web application framework for Python. It is well documented and this project is relatively small, so it made sense to pick Flask for this application.\n  UK-COVID-19: This library is maintained by the folks over at Public Health England (PHE) and allows my application to retrieve UK based data about the COVID-19 outbreak. More on this below.\n  Loguru: This is personal preference but I like how this library makes Python logging hassle free and is very easy to integrate into my application.\n  Feedparser: This library allows my application to parse BBC\u0026rsquo;s health news RSS feed and show COVID-19 related articles in my application. This can be seen on this line in app.py.\n  Application datasource I began looking for a potential datasource for my application to retrieve UK based data for the COVID-19 outbreak.\nI came across the UK COVID-19 dashboard which provides an application programming interface (API) and Python software development kit (SDK) (mentioned above) developed by PHE. The API is free (Thanks PHE! ‚ù§Ô∏è) and provides loads of data including (but not limited to):\n  New cases.\n  Hospital cases.\n  Admissions.\n  Substantial testing data (including data for each pillar).\n  Deaths.\n  Obviously, it was a perfect choice for the project üòÑ\nFor more information on the API, you can look at the documentation yourself here.\nRunning locally You can run the application locally and test it out for yourself if you have docker installed.\nFirst, build the image from the dockerfile in the repository and then run the image. You can access the app at http://localhost:5000.\nMore detailed instructions on how to do this can be found here.\nDeploying to Azure App Service I wanted to host the application somewhere for free. Luckily for me, Microsoft\u0026rsquo;s Azure App Service has a free tier instance (SKU: F1) which allowed me to host my application free of charge! ‚ù§Ô∏è üòÑ\nI deployed the application onto Azure App Service by using the Azure command-line interface (CLI). It was super easy and involved the following steps:\n  Install the Azure CLI.\n I\u0026rsquo;m using Windows so I used chocolatey to install it using the following command: choco install azure-cli -y    Log in to my Azure account using the command: az login.\n  From my project repository, run the following command: az webapp up --sku F1 --name \u0026lt;my-app-name\u0026gt;.\n  Thats it! The Azure CLI did all of the heavy lifting creating a resource group for my application in Azure, creating an App Service plan and the app service object, zipping up my application and deploying it! üòÑ\nYou can access a live version of the application by heading to https://uk-covid-19-stats.azurewebsites.net.\nFor more information on the steps above, see the following Microsoft documentation.\nOverall, this was a fun little project which I believe met the aims I set out to achieve. I also ended up learning how to deploy a Python application to Azure App Service üòÉ\nThanks for reading and I hope you enjoyed reading this short blog post. Until next time! üëã\n","permalink":"https://danielbrennand.com/blog/covid-19-app/","summary":"Hi! üëã\nI had some free time recently so I decided to develop something which was relevant to current worldwide situation. The ongoing Coronavirus (COVID-19) pandemic.\nThis is going to be a short blog post about the application, its aims and the process of deploying it to Azure App Service.\nAims I had three aims that I wanted to achieve:\n  I wanted to develop an application which was simple.","title":"UK COVID-19 stats application"},{"content":"Caddy version 2.0.0 released on May 4th 2020. Caddy brands itself as \u0026ldquo;The Ultimate Server\u0026rdquo; with functionality including a web server, reverse proxy, automatic certificate renewal, automatic HTTPS and more!\nI have to say, it\u0026rsquo;s pretty awesome! üëç\nThis blog post will show you how to use the Caddy v2 reverse proxy feature with a domain managed with Cloudflare. So, lets jump in! üòÑ\nCaddyfile Caddy uses a Caddyfile for it\u0026rsquo;s configuration.\nTwo main Caddyfile concepts to understand are blocks and directives.\nBlocks A Caddyfile block contains configuration for a site. Blocks are declared using curly braces:\nexample.com { ... } One unique block worth mentioning is the global options block. This block must be defined at the top of the Caddyfile and allows you to modify options which apply globally to Caddy. Two usage examples for this block are altering the acme_ca (ACME CA\u0026rsquo;s directory) to the Let\u0026rsquo;s Encrypt staging endpoint or the email option which is used when creating the ACME account.\nTo use the global options block along with a site block, your Caddyfile would look similar to the following:\n{ email example@example.com# This is a valid comment in a Caddyfile :-) # This acme_ca option tells Caddy to use the Let\u0026#39;s Encrypt staging endpoint # Remove when transitioning to a production environment  acme_ca https://acme-staging-v02.api.letsencrypt.org/directory }# Site block below example.com { ... } Directives and Subdirectives Caddyfile directives customise how a site is served by Caddy and must be declared within a site block. A subdirective provides additional configuration for a directive. See the Caddy documentation for a full list of directives and their respective subdirectives.\nThe directive we are interested in this blog post is the reverse_proxy directive.\nAdding to the previous example, usage of the reverse_proxy directive is as follows:\n{ email example@example.com# This acme_ca option tells Caddy to use the Let\u0026#39;s Encrypt staging endpoint # Remove when transitioning to a production environment  acme_ca https://acme-staging-v02.api.letsencrypt.org/directory }# Site block below example.com {# Using the reverse_proxy directive within a site block  reverse_proxy example:80 } Using the Caddy v2 Docker container with a Cloudflare managed domain Right, enough of the boring theory! Onto an actual example.\nI have created a Github repository which uses docker-compose to deploy the Caddy v2 container (including the Cloudflare module) and freshrss as an example application.\nWe will be using the DNS-01 challenge type to request a certificate for the subdomain freshrss under your Cloudflare managed apex domain.\nPrerequisites   A domain managed with Cloudflare\n  SSL/TLS encryption mode set the Full (strict)\n This ensures that clients don\u0026rsquo;t encounter infinite redirects, they connect via HTTPS and ensures the Let\u0026rsquo;s Encrypt certificate on the server is valid.    A server with ports 80 and 443 accessible to the internet\n NOTE: The DNS-01 challenge type doesn\u0026rsquo;t require any open ports however, in order to access any applications from the internet, we need these ports exposed.    A server with Docker, docker-compose and git installed\n  An A record configured in Cloudflare with the following values:\n  Type: A\n  Name: freshrss\n  Content: The IPv4 address of your server\n  TTL: Auto\n  Proxy Status: Proxied\n    A Cloudflare API token (NOT an API key!) with the following permissions:\n  Zone / Zone / Read\n  Zone / DNS / Edit\n    Deployment   Clone the caddy-cloudflare-docker-compose repository and change directory: git clone https://github.com/dbrennand/caddy-cloudflare-docker-compose.git; cd caddy-cloudflare-docker-compose.\n  Make a copy of the ExampleCaddyfile called Caddyfile: cp ExampleCaddyfile Caddyfile.\n  Modify the following lines in the Caddyfile:\n  email example@example.com: Alter example@example.com to your email address.\n  subdomain.example.com: Modify this to be your domain. For example: freshrss.yourdomain.com.\n    NOTE: I have provided some snippets and comments in the ExampleCaddyfile which I hope you will find useful.\nModify the CLOUDFLARE_API_TOKEN environment variable in the docker-compose.yaml file with your Cloudflare API token:  environment: CLOUDFLARE_API_TOKEN: \u0026#34;Insert your Cloudflare API token here.\u0026#34; Modify the PUID, PGID and TZ environment variables in the docker-compose.yaml file (if required).\n  Start the Caddy and freshrss containers using: docker-compose up -d.\n  View the Caddy container logs using: docker logs caddy. If everything is configured correctly, you will see something similar to the following output in the Caddy container logs:\n[INFO] [freshrss.yourdomain.com] acme: use dns-01 solver [INFO] [freshrss.yourdomain.com] acme: Preparing to solve DNS-01 [INFO] [freshrss.yourdomain.com] acme: Trying to solve DNS-01 [INFO] [freshrss.yourdomain.com] acme: Checking DNS record propagation using [127.0.0.11:53] [INFO] Wait for propagation [timeout: 1m0s, interval: 2s] [INFO] [freshrss.yourdomain.com] acme: Waiting for DNS record propagation. [INFO] [freshrss.yourdomain.com] The server validated our request [INFO] [freshrss.yourdomain.com] acme: Cleaning DNS-01 challenge [INFO] [freshrss.yourdomain.com] acme: Validations succeeded; requesting certificates [INFO] [freshrss.yourdomain.com] Server responded with a certificate. [INFO][freshrss.yourdomain.com] Certificate obtained successfully [INFO][freshrss.yourdomain.com] Obtain: Releasing lock ‚≠ê Congratulations! You have successfully configured Caddy to obtain a valid certificate from Let\u0026rsquo;s Encrypt for the subdomain freshrss under your Cloudflare apex domain üòÑ\nNavigate to freshrss.yourdomain.com in your browser and you will see the setup wizard for freshrss!\nIn my next blog post, I will cover how you can use Cloudflare as a Dynamic DNS (DDNS) provider.\n","permalink":"https://danielbrennand.com/blog/caddy-cloudflare/","summary":"Caddy version 2.0.0 released on May 4th 2020. Caddy brands itself as \u0026ldquo;The Ultimate Server\u0026rdquo; with functionality including a web server, reverse proxy, automatic certificate renewal, automatic HTTPS and more!\nI have to say, it\u0026rsquo;s pretty awesome! üëç\nThis blog post will show you how to use the Caddy v2 reverse proxy feature with a domain managed with Cloudflare. So, lets jump in! üòÑ\nCaddyfile Caddy uses a Caddyfile for it\u0026rsquo;s configuration.","title":"Using Caddy v2's reverse proxy feature with a domain managed with Cloudflare!"}]